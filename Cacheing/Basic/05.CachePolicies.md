## 📘 Cache Eviction Policies

> When a cache is full, it needs to evict (remove) items to make space for new ones. **Eviction policies** decide **which items to evict**.

---

### 🔹 1. **LRU (Least Recently Used)**

**Evicts the item that hasn’t been accessed for the longest time.**

The **Least Recently Used (LRU)** cache is a popular caching strategy where the cache automatically removes the **least recently accessed item** when it runs out of space.

To make LRU efficient, it must support:

* ✅ **Fast access (O(1))**
* ✅ **Fast eviction of the least recently used item**

---

### 🔧 Core Idea of LRU Cache Implementation

To implement an efficient LRU cache, we typically use **two main data structures**:

1. **Hash Map** – for O(1) access to cache entries by key.
2. **Doubly Linked List** – to track the order of usage.

---

### 🧠 How It Works

* When a new item is added or accessed, it’s moved to the **front** of the list (most recently used).
* When the cache exceeds capacity, the item at the **back** of the list (least recently used) is **evicted**.
* The **hash map** keeps track of nodes in the list for O(1) lookup.

---

### 📦 Visualization

Let’s say you have a cache capacity of 3:

```
[Most Recent]  A <-> B <-> C  [Least Recent]
```

* Accessing `B` moves it to the front:

```
[Most Recent]  B <-> A <-> C
```

* Adding a new item `D` causes eviction of `C`:

```
[Most Recent]  D <-> B <-> A
```

---

### 🛠️ Basic Operations

| Operation         | Time Complexity | Description                  |
| ----------------- | --------------- | ---------------------------- |
| `get(key)`        | O(1)            | Return value & move to front |
| `put(key, value)` | O(1)            | Add/move key & evict if full |

---

### ✅ Summary

> An **LRU cache** combines a **hash map + doubly linked list** to provide fast access and maintain the usage order.
> It's ideal when **recent data is more likely to be reused**, such as in memory caching, session storage, or page replacements.


---

## 🔹 LFU Cache – How It Works Internally

The **Least Frequently Used (LFU)** cache removes the data that is **used least often** when the cache is full.

Unlike LRU (Least Recently Used), which tracks *recency*, LFU focuses on *frequency* of access.

---

### 🧠 Core Concept

Each item in the cache maintains a **counter** of how many times it has been accessed.
When eviction is needed, the item with the **lowest access count** (i.e., least frequently used) is removed.

---

### 🔧 Key Data Structures

A typical LFU cache uses:

1. **Hash Map** – for quick key-to-data mapping.
2. **Frequency Map** – groups keys by frequency (e.g., frequency → list of keys).
3. **Priority Queue or Linked List** – to quickly find the least frequently used item.

---

### 🛠️ How LFU Works

* Every time a key is accessed, its **frequency counter increases**.
* If the cache is full:

  * The **key with the lowest frequency** is evicted.
  * If multiple keys share the same frequency, one of them (often the oldest) is removed.

---

### 📦 Example

Suppose cache capacity = 3

| Key | Frequency |
| --- | --------- |
| A   | 3         |
| B   | 2         |
| C   | 1         |

If a new item `D` is added:

* `C` has the **lowest frequency** (1), so it gets **evicted**.
* New state: A(3), B(2), D(1)

---

### 🧪 Common Use Case

LFU is ideal when the system must **prioritize frequently accessed data** over recently accessed data — for example, in AI model cache layers or content delivery systems with long-lived hot content.

---

### ⚠️ Complexity

LFU is **more complex to implement** than LRU because:

* It must **track frequency counts**
* It may require **additional structures** (like doubly linked lists per frequency)

However, some modern variants (like **Approximate LFU**) trade accuracy for performance.

---

### ✅ Summary

> **LFU evicts items that are used the least over time**.
> It’s great for systems where frequent access signals long-term importance.

---

## 🔹 FIFO Cache – How It Works Internally

The **First In, First Out (FIFO)** cache eviction policy is the simplest cache replacement strategy.

It removes the **oldest item** in the cache—i.e., the data that was **added earliest**—when space is needed for new entries.

---

### 🧠 Core Concept

FIFO treats the cache like a queue:

* Items enter the cache at the **back** (tail of the queue).
* Items are evicted from the **front** (head of the queue).

---

### 🔧 Data Structures Used

* **Queue (or Linked List)** to maintain the insertion order.
* **Hash Map** for O(1) key lookup (optional, depending on implementation).

---

### 🛠️ How FIFO Works

* When a new item is added and the cache is full:

  * The **oldest inserted item** is removed first.
* The system does **not consider access frequency or recency**.
* Simple and predictable eviction behavior.

---

### 📦 Example

Cache capacity = 3

```text
Cache State (oldest → newest): [A, B, C]
Insert D:
→ Evict A (first inserted)
→ Cache after insertion: [B, C, D]
```

---

### ✅ When to Use FIFO

* Simple caching scenarios where **insertion order is sufficient**.
* Systems where **predictability** is more important than sophisticated eviction.
* Useful for **streaming data buffers** or **logging systems**.

---

### ⚠️ Limitations

* May evict frequently or recently accessed data prematurely.
* Less efficient than LRU or LFU for typical caching workloads.

---

### 📌 Summary

> FIFO eviction removes the **oldest cached item first**, regardless of how often or recently it was accessed.
> It’s simple and fast but may not always yield the best cache hit rates.

---

## 📌 Summary Table:

| Policy   | Evicts                | Best for               | Downside                 |
| -------- | --------------------- | ---------------------- | ------------------------ |
| **LRU**  | Least recently used   | Time-sensitive data    | May evict frequent items |
| **LFU**  | Least frequently used | Stable access patterns | Costly to track usage    |
| **FIFO** | Oldest inserted       | Simplicity             | Ignores access behavior  |

---

> **Choosing the right eviction policy** depends on your application's data usage pattern:
> Are users re-accessing recent data (LRU)? Or often-used items (LFU)? Or do you just want simplicity (FIFO)?
