# **Redis + Sockets**

Let’s treat “Redis + sockets” like building a radio-backed control tower for your real-time system. You have many WebSocket servers (gateways) spread across nodes and regions, users connect to any of them through a load balancer, and yet messages must flow instantly to the right people no matter which box they landed on. Redis is the airwave: sometimes a live broadcast (Pub/Sub), sometimes a recorded flight log you can replay (Streams), and often a shared clipboard for presence, routing, throttling, and idempotency. What follows is a deep, developer-to-developer guide—not toy snippets, but the mental models and patterns you actually ship.

---

# The three Redis roles in a socket architecture

In production you rarely use Redis for only one thing. A serious real-time stack leans on Redis in three complementary ways.

**First, live fan-out with Pub/Sub.** Every socket gateway both publishes and subscribes. When Gateway A receives a message for Room R, it publishes to `chat:room:{R}`; all gateways subscribed to that channel receive it and emit to their local sockets in that room. Pub/Sub is “radio”: ultra-low latency, no persistence, drop-on-the-floor if no one is listening. It’s perfect for typing indicators, presence pings, in-game state, and messages to already-connected users.

**Second, durable delivery with Streams.** Some events must survive disconnects: chat history, order-status updates, or anything you owe the user even if they were offline. Think of a Stream per room or per user inbox: `stream:room:{R}` or `stream:user:{U}`. Producers `XADD`, consumers read with `XREAD` or consumer groups (`XREADGROUP`), and you get IDs, ordering, and backpressure control. Streams let a reconnecting client “catch up from last seen ID,” achieving at-least-once delivery with idempotent client logic.

**Third, fast coordination via keys and small data structures.** Presence is a `SETEX user:{U}:presence online`, a roster is a `SET room:{R}:members`, per-connection routing is a `HSET conn:{socketId} userId … room …`, rate limits are counters with expiries, and dedupe is short-lived “seen” keys. You’ll also keep shard assignments and sticky routing metadata here.

Each role solves a different pain: Pub/Sub makes a cluster behave like one giant socket server; Streams make state survivable; keys make it governable.

---

# The end-to-end flow, as it actually runs

A client opens a WebSocket to any gateway behind your LB. During the handshake the gateway authenticates the user (JWT/session), loads a tiny profile and permissions from Redis (or your auth service and then caches them), and joins server-local “rooms.” Locally, a room is just a set of socket IDs. Globally, a room is a Redis concept: there’s a Pub/Sub channel (`chat:room:{R}`) and optionally a Stream (`stream:room:{R}`) for history.

When the client emits a message to Room R, the gateway does three things in order:

1. **Persist if necessary.** If the message must survive, `XADD stream:room:{R} * payload…` and capture the resulting Stream ID. That ID becomes the canonical dedupe key and ordering marker across the cluster.

2. **Broadcast live.** `PUBLISH chat:room:{R} {json}` containing the stream ID and payload. Every gateway subscribed to `chat:room:{R}` receives it and emits to its local sockets in R.

3. **Track delivery and presence.** The gateway updates last-seen markers like `HSET user:{U}:room:{R} last_id <stream-id>` and bumps a heartbeat key `SETEX user:{U}:presence online 60`.

If a client reconnects after a network blip, it sends its last received Stream ID (`sinceId`). The gateway queries Redis Streams: `XREAD COUNT 200 STREAMS stream:room:{R} {sinceId}` (or uses consumer groups per-user for strict tracking) and replays missed messages before resuming live Pub/Sub.

Nothing about this requires gateways to talk to each other directly. Redis is the rendezvous, the bus, and the short-term brain.

---

# Pub/Sub: the radio you scale with

Pub/Sub is brutally simple: no acks, no history, just fire and immediate fan-out. That’s precisely why it scales so well for hot paths. Every gateway process maintains a small set of subscriptions it cares about: the rooms that currently have at least one local member. When the first local user joins Room R, the gateway subscribes to `chat:room:{R}`; when the last one leaves, it unsubscribes. This keeps subscription counts proportional to live interest rather than total rooms in your system.

Because Pub/Sub isn’t durable, every payload you publish for important messages should carry a **streamId** from the durable path. That way gateways can dedupe locally and clients can request a replay if they missed anything.

Two production footguns to handle up front:

* **Resubscribe on disconnect.** Your Redis client must auto-reconnect and resubscribe. Treat this like TCP reconnect logic: exponential backoff, jitter, health metrics.
* **Ordering is best-effort.** Across multiple publishers, order is not guaranteed. If you care about order, use the Stream’s ID for client-side ordering. Pub/Sub informs you “something new happened,” the Stream tells you the authoritative sequence.

---

# Streams: the replayable log

Streams give you an append-only log with IDs like `1713366400000-0`. For a chat room, a **single stream per room** is straightforward. For DM use-cases, a **stream per user** avoids head-of-line blocking and simplifies read markers. The producer path is:

* `XADD stream:room:{R} * type=message msg=<json> from=<user>`

Consumers have two patterns:

* **Client-pull replay:** Gateways read on behalf of reconnecting clients with plain `XREAD` and send the backlog to that socket. Store last delivered ID per user/room in Redis (`HSET user:{U}:room:{R} last_id …`) so any gateway can service the replay.

* **Consumer groups for background workers:** Delivery receipts, moderation, indexing, or push notifications read from a consumer group (`XGROUP CREATE …`). Gateways don’t need groups; workers do, because they want at-least-once with a pending entries list (PEL) and explicit `XACK`.

You’ll make one more pragmatic choice: **compaction.** Chats can grow indefinitely. Use `XTRIM stream:room:{R} MAXLEN ~ 50000` to retain “enough” history per room while capping memory growth. For compliance-grade retention, mirror to a persistent store (PostgreSQL/S3) asynchronously.

---

# Presence, rosters, and read receipts

Presence is ephemeral and cheap. On socket open, `SETEX user:{U}:presence online 60`. Gateways refresh it every \~30 seconds from a heartbeat timer. A background job scans for expired keys to mark users offline. Room rosters are a `SET room:{R}:members` updated when local sockets join/leave; gateways use a short `PEXPIRE` on the set entries tied to a per-connection heartbeat so zombie entries disappear.

Read receipts need a monotonically increasing marker. Use the Stream ID you already have: when a client displays message `1713366400000-7`, the gateway writes `HSET user:{U}:room:{R} read_id 1713366400000-7`. The UI can compute unread by comparing `XINFO STREAM last-entry` against `read_id`, and your notification service uses the same markers to decide whether to send a push.

---

# Routing and sharding the work

If you have millions of rooms, you can’t have every gateway subscribe to every channel; you subscribe **on demand**. A gateway keeps an LRU of active room subscriptions keyed by `room:{R}` and unsubscribes when idle. For extremely hot rooms, you may shard the broadcast itself: publish to `chat:room:{R}:part:{k}` where `k` is a small number (say 0..3) and have each gateway subscribe only to a subset based on a hash of its instance ID. Locally, you still emit to all sockets; the point is reducing the Pub/Sub fanout pressure on Redis.

For key placement in Redis Cluster, adopt **hash tags** so related keys live on the same slot: `stream:{room:{R}}`, `chat:{room:{R}}`, `room:{R}:members`. Everything between `{}` is the hash basis. That avoids cross-slot pitfalls and reduces latency under load.

---

# Backpressure and overload

Sockets are write-heavy bursts. Gateways must protect themselves from slow clients. Each connection needs a bounded outbound queue; if it overflows, drop the least important events (typing, presence) first, then disconnect if it keeps misbehaving. On the Redis side, Streams give you **pull** semantics to pace consumption; Pub/Sub is **push**, so you throttle at the edge (don’t publish every keystroke, coalesce to 5–10/s, collapse duplicate notifications).

Your notification and indexing workers should be in consumer groups with small `MAXLEN` trims on the streams feeding them, and they should checkpoint progress with `XACK` only after their side effects commit.

---

# Delivery semantics and idempotency you can trust

Exactly-once is a myth here; you implement **at-least-once** plus idempotency:

* Every message has a **stable ID** (`streamId`), carried in Pub/Sub and persisted in the stream.
* Gateways dedupe with a short-lived key: `SETNX seen:{socketId}:{streamId} 1 EX 60`. If it already exists, they skip emitting.
* Downstream services (notifications, search) also dedupe with `SETNX` or a Redis Bloom filter if volume is extreme.

With this, duplicates don’t hurt you, and loss is mitigated by replay on reconnect.

---

# Rate limiting, throttling, and abuse controls

Redis is your throttle. Token bucket in Lua or simple counters with TTLs are enough:

* Per user emit rate: `INCR rl:emit:{U}:{minute}; EXPIRE …` and drop/slow if beyond threshold.
* Per room fan-out cap: track `INCR room:{R}:msg:{second}` to detect floods and temporarily switch to “digest mode” (coalesce frequent updates into batched deltas).
* Per API key/global limits for bots or integrations follow the same pattern.

Do this at the gateway to avoid tipping messages into Pub/Sub that you’ll only discard later.

---

# Security: don’t let the client steer Redis

Clients never know channel names or stream keys. They ask to join a logical room; the gateway verifies authorization against your auth service or its Redis cache, then subscribes internally. Publish paths validate the sender’s rights every time; don’t trust a “roomId” sent from the client. Encrypt your Redis traffic (TLS), keep Redis on a private network, and enable ACLs so only gateway/worker roles can touch the keys they need.

---

# Observability and operability

You will want three dashboards from day one:

1. **Gateway health:** connection count, send queue depths, dropped messages, reconnect rates, publish and subscribe errors, and latency from “message received” to “message emitted.”

2. **Redis health:** memory used, fragmentation, evictions, Pub/Sub channel counts and messages/sec, Stream lengths and trims, consumer lag (`XPENDING`), and replication/cluster health.

3. **User experience:** end-to-end delivery percentiles (server-timestamp to client render), replay counts on reconnect, and per-room message rates.

For Streams, alert on growing pending entries (consumers died) and on streams not being trimmed. For Pub/Sub, alert on sudden spikes in subscriptions (thundering herds) and on reconnect storms from gateways.

---

# A practical Go blueprint (gateway core)

Below is a condensed skeleton using Gorilla WebSocket and go-redis/v9. It shows the three critical paths: subscribe/unsubscribe per room, publish with persistence, and replay on reconnect. This is not copy-paste complete, but it’s the spine you can flesh out.

```go
// go.mod: github.com/redis/go-redis/v9, github.com/gorilla/websocket
type Gateway struct {
    rdb      *redis.Client
    sub      *redis.PubSub
    rooms    map[string]int           // local join counts per room
    roomSubs map[string]context.CancelFunc // cancel Pub/Sub readers
}

// Called when first local user joins a room on this gateway:
func (g *Gateway) ensureSubscribed(ctx context.Context, room string) {
    if _, ok := g.roomSubs[room]; ok { return }
    channel := fmt.Sprintf("chat:room:{%s}", room)

    // Separate reader goroutine so we don't block.
    cctx, cancel := context.WithCancel(ctx)
    g.roomSubs[room] = cancel

    go func() {
        ps := g.rdb.Subscribe(cctx, channel)
        ch := ps.Channel()
        for {
            select {
            case <-cctx.Done():
                _ = ps.Close()
                return
            case msg, ok := <-ch:
                if !ok { return }
                // msg.Payload should include streamId + payload JSON
                g.emitToLocalSockets(room, []byte(msg.Payload))
            }
        }
    }()
}

// Called when last local user leaves the room:
func (g *Gateway) maybeUnsubscribe(room string) {
    if cancel, ok := g.roomSubs[room]; ok {
        cancel()
        delete(g.roomSubs, room)
    }
}

type ChatMessage struct {
    Room     string          `json:"room"`
    From     string          `json:"from"`
    Payload  json.RawMessage `json:"payload"`
    StreamID string          `json:"streamId"`
    Ts       int64           `json:"ts"`
}

// When a client sends a message to a room:
func (g *Gateway) handleInbound(ctx context.Context, m ChatMessage) error {
    // 1) Persist (optional but recommended for anything important)
    streamKey := fmt.Sprintf("stream:room:{%s}", m.Room)
    args := &redis.XAddArgs{
        Stream: streamKey,
        Values: map[string]any{
            "from": m.From,
            "ts":   time.Now().UnixMilli(),
            "msg":  m.Payload, // store JSON blob
        },
        Approx: true,
        MaxLen: 50000, // trim capacity-style
    }
    id, err := g.rdb.XAdd(ctx, args).Result()
    if err != nil { return err }
    m.StreamID = id
    m.Ts = time.Now().UnixMilli()

    // 2) Publish live
    bus := fmt.Sprintf("chat:room:{%s}", m.Room)
    b, _ := json.Marshal(m)
    if err := g.rdb.Publish(ctx, bus, b).Err(); err != nil {
        return err
    }

    // 3) Optionally bump last activity / presence
    _ = g.rdb.Set(ctx, fmt.Sprintf("user:%s:presence", m.From), "online", 60*time.Second).Err()
    return nil
}

// On reconnect, client provides sinceId:
func (g *Gateway) replay(ctx context.Context, room, sinceId string, emit func([]byte)) error {
    streamKey := fmt.Sprintf("stream:room:{%s}", room)
    id := sinceId
    for {
        // Read a chunk after id
        res, err := g.rdb.XRead(ctx, &redis.XReadArgs{
            Streams: []string{streamKey, id},
            Count:   200,
            Block:   0,
        }).Result()
        if err != nil && err != redis.Nil { return err }
        if len(res) == 0 || len(res[0].Messages) == 0 { break }

        for _, xmsg := range res[0].Messages {
            out := ChatMessage{
                Room:     room,
                From:     xmsg.Values["from"].(string),
                StreamID: xmsg.ID,
                Ts:       xmsg.Values["ts"].(int64),
            }
            if raw, ok := xmsg.Values["msg"].(string); ok {
                out.Payload = json.RawMessage(raw)
            }
            b, _ := json.Marshal(out)
            emit(b)
            id = xmsg.ID
        }
        // If fewer than Count returned, we're caught up
        if len(res[0].Messages) < 200 { break }
    }
    return nil
}
```

This spine leaves room for the essential production details: auth checks before `handleInbound`, per-connection outbound queues with drop policies, dedupe via `SETNX seen:{socket}:{streamId}`, and metrics around publish/emit latency.

---

# Where this plugs into your microservices

Your socket gateways are stateless front doors. Other services—Orders, Inventory, Notifications—never talk to sockets directly. They emit domain events to your core bus (Kafka/NATS/Postgres logical replication—whatever your backbone is). A “realtime-bridge” service consumes those domain events, projects them into user/room–scoped Streams in Redis, and publishes Pub/Sub nudges to wake the gateways. That separation keeps your business eventing decoupled from your delivery substrate and lets you swap Redis out or scale it independently.

For example, when Order Service marks an order “shipped,” it emits `order.shipped`. The bridge writes:

* `XADD stream:user:{buyerId} type=order_shipped orderId=...`
* `PUBLISH chat:user:{buyerId} {"type":"order_shipped","streamId":"…","..."}`
* Gateways subscribed to `chat:user:{buyerId}` fan-out to any connected sessions; if the buyer is offline, the Stream makes sure they can catch up later and Notifications Service can read the same stream to send a push/email if `read_id` lags.

---

# Failure modes you’ll be glad you planned for

Redis restarts? Gateways reconnect, resubscribe, and life goes on; no durable messages were lost because the durable pieces were in Streams (AOF or RDB configured). A single gateway dies? Users reconnect through the load balancer; their last seen IDs resynchronize history. A region partitions? Users in that region keep chatting with each other if they share a regional Redis; global rooms stall until connectivity returns—this is where you decide whether to run a regional Redis Cluster with local rooms only, or a globally replicated tier for cross-region rooms.

Memory pressure? You trim streams, cap message sizes, and avoid caching huge blobs. Evictions are dangerous if you rely on keys being present; for critical coordination keys, set them to short TTLs you can recreate, and never make Redis a source of truth for anything you cannot recompute.

---

# Putting it together

The pattern that endures is simple: **Pub/Sub for right now, Streams for whenever, keys for everything else.** Gateways subscribe only to what they need, publish everything important with a Stream ID, replay on reconnect, and protect themselves with queues and rate limits. Your domain services never shout into Pub/Sub directly; a translation layer projects domain events into user- or room-centric channels and streams so delivery is tailored and privacy boundaries remain intact.



# copy-pasteable Node.js gateway implementation using plain WebSockets and Redis

* WebSocket gateway (plain `ws`)
* Redis Pub/Sub for live fan-out
* Redis Streams (`XADD` / `XREAD`) for durable history and replay
* Per-room on-demand subscriptions (subscribe when first local client joins, unsubscribe when last leaves)
* Deduplication using short-lived `SETNX` keys with TTL
* Basic per-user rate limiting (sliding-window counter)
* Outbound queues and simple drop policy for slow clients
* Reconnect/replay flow for clients that provide `lastSeenId`

This is a practical, production-minded skeleton — not a one-liner demo. It’s commented and ready to adapt for your auth, metrics, and persistence needs.

---

# Files

1. `package.json`
2. `gateway.js` (main server)
3. `sample_client.html` (simple test client to demonstrate reconnect/replay)

---

### package.json

```json
{
  "name": "redis-ws-gateway",
  "version": "1.0.0",
  "description": "WebSocket gateway with Redis Pub/Sub + Streams for realtime messaging (plain WebSocket)",
  "main": "gateway.js",
  "scripts": {
    "start": "node gateway.js"
  },
  "dependencies": {
    "ioredis": "^5.3.2",
    "ws": "^8.13.0",
    "uuid": "^9.0.0"
  }
}
```

---

### gateway.js

```js
/**
 * gateway.js
 * Node.js WebSocket gateway that integrates with Redis (ioredis).
 *
 * Features:
 *  - Plain WebSocket server (ws)
 *  - Redis Pub/Sub for live fan-out
 *  - Redis Streams for durable history and replay
 *  - Per-room subscriptions (subscribe on first local member)
 *  - Deduplication (SETNX seen:{streamId})
 *  - Basic rate limit (per user)
 *  - Outbound queue per socket with drop policy
 *
 * Environment variables:
 *  - REDIS_URL (optional, default: redis://127.0.0.1:6379)
 *  - PORT (optional, default: 8080)
 *
 * Note: Replace the `authenticate` function with your real auth logic.
 */

const WebSocket = require('ws');
const IORedis = require('ioredis');
const { v4: uuidv4 } = require('uuid');

const REDIS_URL = process.env.REDIS_URL || 'redis://127.0.0.1:6379';
const PORT = Number(process.env.PORT || 8080);

// Redis clients
const redis = new IORedis(REDIS_URL);        // general client (streams, keys, counters)
const pub = new IORedis(REDIS_URL);          // publisher (pub/sub publish)
const sub = new IORedis(REDIS_URL);          // subscriber (pub/sub subscribe)

// Local in-memory state
// Maintain which rooms have local subscriptions and the subscribe worker handles.
const localRoomMembers = new Map(); // room -> Set of socketIds (local)
const roomSubWorkers = new Map();   // room -> boolean (subscribed)
const wsById = new Map();           // socketId -> ws wrapper

// Config
const STREAM_MAXLEN = 10000;        // max stream length per room (approximate)
const OUTBOUND_QUEUE_LIMIT = 1000;  // per-socket outbound queue size
const DEDUPE_TTL_SEC = 60;          // seen:{streamId} ttl to dedupe emits
const RATE_LIMIT_MAX_PER_MIN = 120; // messages per minute per user (change as needed)

// WebSocket server
const wss = new WebSocket.Server({ port: PORT }, () => {
  console.log(`WebSocket gateway listening on ws://localhost:${PORT}`);
});

// Basic authenticate: replace with real JWT/session validation
async function authenticate(token) {
  // token is whatever the client sends; here we simulate a simple mapping
  // In production: verify JWT, query auth service, or check Redis-cached session
  if (!token || token.trim() === '') return null;
  // for demo, token === userId
  return { userId: token };
}

// Rate limiter (simple per-minute counter using INCR + EXPIRE)
async function isRateLimited(userId) {
  if (!userId) return true;
  const key = `rl:emit:${userId}:${Math.floor(Date.now() / 60000)}`; // per-minute window
  const val = await redis.incr(key);
  if (val === 1) {
    // first increment in this window, set TTL of ~70s
    await redis.expire(key, 70);
  }
  return val > RATE_LIMIT_MAX_PER_MIN;
}

// Helper: stream key and channel key conventions (use hash tags to collocate in cluster)
function streamKey(room) { return `stream:room:{${room}}`; }
function channelKey(room) { return `chat:room:{${room}}`; }
function seenKey(streamId) { return `seen:${streamId}`; }

// Subscribe/unsubscribe logic per room
async function ensureRoomSubscribed(room) {
  if (roomSubWorkers.has(room)) return; // already subscribed locally
  // subscribe via Redis Pub/Sub: sub.subscribe(channel, listener)
  const channel = channelKey(room);
  roomSubWorkers.set(room, true);
  await sub.subscribe(channel, (message) => {
    // message is a stringified JSON with at least { streamId, payload }
    handlePubSubMessage(room, message).catch(err => {
      console.error('Error handling pubsub message', err);
    });
  });
  console.log('Subscribed locally to', channel);
}

async function maybeUnsubscribeRoom(room) {
  const members = localRoomMembers.get(room);
  if (members && members.size > 0) return; // still have local members
  if (!roomSubWorkers.has(room)) return;
  const channel = channelKey(room);
  // ioredis unsubscribe: sub.unsubscribe(channel)
  await sub.unsubscribe(channel);
  roomSubWorkers.delete(room);
  console.log('Unsubscribed locally from', channel);
}

// Handle messages received from Redis Pub/Sub and emit to local sockets
async function handlePubSubMessage(room, message) {
  // parse and dedupe
  let msg;
  try { msg = JSON.parse(message); } catch(e) {
    console.warn('Invalid pubsub message', e);
    return;
  }
  const sId = msg.streamId || msg.streamID || msg.id || msg.id; // support variants
  if (sId) {
    const dedup = await redis.setnx(seenKey(sId), '1');
    if (dedup === 0) {
      // already seen recently
      return;
    }
    await redis.expire(seenKey(sId), DEDUPE_TTL_SEC);
  }
  // emit to all local sockets in room
  const members = localRoomMembers.get(room);
  if (!members) return;
  const payload = JSON.stringify({ type: 'message', data: msg });
  for (const sockId of members) {
    const wrapper = wsById.get(sockId);
    if (!wrapper) continue;
    wrapper.enqueueOutbound(payload);
  }
}

// Wrapper around WebSocket to handle outbound queue and metadata
class SocketWrapper {
  constructor(ws, userId) {
    this.ws = ws;
    this.userId = userId;
    this.id = uuidv4();
    this.rooms = new Set();
    this.outboundQueue = [];
    this.sending = false;
    this.closed = false;

    // bind events
    ws.on('close', () => this.onClose());
    ws.on('error', (err) => {
      console.warn('Socket error', err);
      this.onClose();
    });
    ws.on('pong', () => this.onPong());
    // heartbeat ping/pong handled by server loop if desired
  }

  enqueueOutbound(msg) {
    if (this.closed) return;
    if (this.outboundQueue.length >= OUTBOUND_QUEUE_LIMIT) {
      // drop oldest non-critical message; simple policy
      this.outboundQueue.shift();
      // optionally record a drop metric here
    }
    this.outboundQueue.push(msg);
    this.flushOutbound();
  }

  async flushOutbound() {
    if (this.sending) return;
    this.sending = true;
    while (this.outboundQueue.length > 0 && this.ws.readyState === WebSocket.OPEN) {
      const msg = this.outboundQueue.shift();
      try {
        // send is async but ws.send has callback form; we wrap into a promise
        await new Promise((res, rej) => {
          this.ws.send(msg, (err) => {
            if (err) return rej(err);
            res();
          });
        });
      } catch (err) {
        console.warn('Error sending to socket', err);
        // push message back for retry if desired, but to avoid stuck we drop it
        // optionally increment a metric for failures
        break;
      }
    }
    this.sending = false;
  }

  joinRoom(room) {
    if (this.rooms.has(room)) return;
    this.rooms.add(room);
    if (!localRoomMembers.has(room)) localRoomMembers.set(room, new Set());
    localRoomMembers.get(room).add(this.id);
    ensureRoomSubscribed(room).catch(err => console.error(err));
  }

  leaveRoom(room) {
    if (!this.rooms.has(room)) return;
    this.rooms.delete(room);
    const set = localRoomMembers.get(room);
    if (set) {
      set.delete(this.id);
      if (set.size === 0) localRoomMembers.delete(room);
    }
    maybeUnsubscribeRoom(room).catch(err => console.error(err));
  }

  async onClose() {
    if (this.closed) return;
    this.closed = true;
    // remove from all rooms
    for (const room of Array.from(this.rooms)) {
      this.leaveRoom(room);
    }
    wsById.delete(this.id);
  }

  onPong() {
    // you can use this to keep-alive and update presence TTL
  }
}

// Persist message to stream and publish to channel
async function persistAndPublish(room, fromUser, payload) {
  // payload: any JSON serializable data
  const sKey = streamKey(room);
  // XADD stream:room {maxlen ~ STREAM_MAXLEN} field1 value1 ...
  // ioredis xadd signature: xadd(key, id, field1, value1, ...)
  // We'll store two fields: from, msg (stringified)
  const id = await redis.xadd(sKey, 'MAXLEN', '~', STREAM_MAXLEN, '*',
    'from', fromUser, 'msg', JSON.stringify(payload), 'ts', Date.now().toString());
  const channel = channelKey(room);
  const pubpayload = JSON.stringify({ streamId: id, from: fromUser, payload, ts: Date.now() });
  await pub.publish(channel, pubpayload);
  return id;
}

// Replay backlog for a user in a room from lastSeenId (exclusive)
async function replaySince(room, sinceId, emitFn) {
  const sKey = streamKey(room);
  let lastId = sinceId || '0-0';

  while (true) {
    // XREAD COUNT 200 STREAMS streamKey lastId
    // ioredis: xread('COUNT', 200, 'STREAMS', sKey, lastId)
    const res = await redis.xread('COUNT', 200, 'STREAMS', sKey, lastId);
    if (!res || res.length === 0) break;
    const [stream, messages] = res[0]; // res[0][0] = stream key, res[0][1] = messages array
    if (!messages || messages.length === 0) break;
    for (const msg of messages) {
      // msg = [id, [field1, value1, field2, value2, ...]]
      const id = msg[0];
      const kv = msg[1];
      // parse into map
      const obj = {};
      for (let i = 0; i < kv.length; i += 2) {
        obj[kv[i]] = kv[i + 1];
      }
      let parsedMsg = null;
      try { parsedMsg = JSON.parse(obj.msg); } catch (e) { parsedMsg = obj.msg; }
      await emitFn(JSON.stringify({ type: 'replay', data: { streamId: id, from: obj.from, payload: parsedMsg, ts: obj.ts } }));
      lastId = id;
    }
    if (messages.length < 200) break; // caught up
  }
}

// When the gateway receives a domain event to push to a room (e.g., chat message)
async function handleClientMessage(sockWrapper, msgObj) {
  // msgObj must include: { type, room, payload, lastSeenId? }
  // For 'send' action: type = 'send', room, payload
  if (msgObj.type === 'send') {
    const room = msgObj.room;
    const payload = msgObj.payload;
    const userId = sockWrapper.userId;

    // rate limit check
    if (await isRateLimited(userId)) {
      // send a rate limit error
      sockWrapper.enqueueOutbound(JSON.stringify({ type: 'error', error: 'rate_limited' }));
      return;
    }

    // optional: authorization check to ensure user can send to this room
    // TODO: add real checks

    // persist and publish
    const streamId = await persistAndPublish(room, userId, payload);
    // locally the pubsub handler will pick it up and forward it to local sockets.
    // Optionally, also emit to the sender immediately (optimistic UI)
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'sent', streamId, room, ts: Date.now() }));
  } else if (msgObj.type === 'join') {
    const room = msgObj.room;
    sockWrapper.joinRoom(room);
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'joined', room }));
  } else if (msgObj.type === 'leave') {
    const room = msgObj.room;
    sockWrapper.leaveRoom(room);
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'left', room }));
  } else if (msgObj.type === 'replay') {
    // client asks to replay since lastSeenId for a room
    const room = msgObj.room;
    const sinceId = msgObj.sinceId || '0-0';
    await replaySince(room, sinceId, async (payload) => {
      sockWrapper.enqueueOutbound(payload);
    });
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'replay_done', room }));
  } else {
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'error', error: 'unknown_type' }));
  }
}

// WebSocket connection handling and handshake
wss.on('connection', async (ws, req) => {
  // For simplicity, the client provides a token query param: ws://host:port/?token=USERID
  const url = new URL(req.url ?? '', `http://${req.headers.host}`);
  const token = url.searchParams.get('token');
  const auth = await authenticate(token);
  if (!auth) {
    ws.send(JSON.stringify({ type: 'error', error: 'unauthenticated' }));
    ws.close();
    return;
  }

  const wrapper = new SocketWrapper(ws, auth.userId);
  wsById.set(wrapper.id, wrapper);

  ws.on('message', async (raw) => {
    let obj = null;
    try { obj = JSON.parse(raw.toString()); }
    catch (e) {
      wrapper.enqueueOutbound(JSON.stringify({ type: 'error', error: 'invalid_json' }));
      return;
    }

    // Example incoming messages:
    // { type: 'join', room: 'room-1' }
    // { type: 'send', room: 'room-1', payload: { text: 'hello' } }
    // { type: 'replay', room: 'room-1', sinceId: '162...' }

    try {
      await handleClientMessage(wrapper, obj);
    } catch (err) {
      console.error('handleClientMessage error', err);
      wrapper.enqueueOutbound(JSON.stringify({ type: 'error', error: 'server_error' }));
    }
  });

  // optional: ping/pong keepalive - server pings, client must pong
  const pingInterval = setInterval(() => {
    if (ws.readyState === WebSocket.OPEN) {
      try { ws.ping(); }
      catch (e) { /* ignore */ }
    } else {
      clearInterval(pingInterval);
    }
  }, 30000);

  // on connect, you may want to send presence or send a welcome
  wrapper.enqueueOutbound(JSON.stringify({ type: 'welcome', socketId: wrapper.id, userId: wrapper.userId }));
});
```

---

### sample\_client.html

```html
<!doctype html>
<html>
<head><meta charset="utf-8"><title>WS Client</title></head>
<body>
  <pre id="log"></pre>
  <script>
    // Replace token with a userId for demo. Example: user 'alice'
    const token = prompt("Enter token (userId)", "alice") || "alice";
    const ws = new WebSocket(`ws://localhost:8080/?token=${token}`);

    const log = (txt) => {
      document.getElementById('log').textContent += txt + '\n';
    };

    ws.addEventListener('open', () => {
      log('OPEN');
      // join a room
      ws.send(JSON.stringify({ type: 'join', room: 'room-1' }));
      // send a message after join
      setTimeout(() => {
        ws.send(JSON.stringify({ type: 'send', room: 'room-1', payload: { text: 'hello from ' + token } }));
      }, 500);
    });

    ws.addEventListener('message', (ev) => {
      log('RECV: ' + ev.data);
    });

    ws.addEventListener('close', () => log('CLOSED'));
    ws.addEventListener('error', (e) => log('ERROR: ' + e.message));
  </script>
</body>
</html>
```

---

# How it works — narrative (concise)

When a WebSocket client connects, it authenticates (replace the demo `authenticate` with real JWT/session verification). The gateway wraps each socket in a `SocketWrapper` that manages a bounded outbound queue to avoid blocking the event loop when a client is slow.

When a client joins a room, the wrapper updates local in-memory `localRoomMembers` and ensures the gateway subscribes to the room’s Pub/Sub `chat:room:{room}` channel if this is the first local member. Subscribing is done on demand so each gateway only listens to channels with local interest.

When a client sends a message, the gateway first checks rate limits (a simple per-minute INCR counter). If allowed, the gateway appends the message to the room’s Redis Stream (`XADD stream:room:{room} ...`) and then publishes a lightweight notification to the Pub/Sub channel (`PUBLISH chat:room:{room} {streamId,...}`). Each gateway subscribed to that channel receives the published message and emits to its local sockets for that room. Emitting uses an outbound queue to avoid blocking — if a client is slow, messages are dropped once the queue limit is reached.

Deduplication is handled via short-lived keys `seen:{streamId}` using `SETNX` logic, so if multiple pubs or replays would otherwise cause duplicates, duplicates are filtered for a short window.

If a client disconnects and later reconnects, it can ask for a replay from its last seen Stream ID. The gateway runs `XREAD` on the appropriate stream(s) and replays missed messages. Because every published payload includes the Stream ID, the replayed messages remain authoritative and ordered.

This design separates responsibilities: Streams are the durable log (history & replay), Pub/Sub is the low-latency fan-out (live), and small keys are used for presence, rate-limiting, and dedupe.

---

# Production notes & next steps

1. **Auth**: Replace the simple token-to-user mapping with JWT validation or session lookup and a Redis-cached profile. Validate permissions on join/send.
2. **Scaling Redis**: Use Redis Cluster with key hash tags `{room}` so the stream and channel and seen keys for the same room land on the same slot. Consider dedicated regional Redis for low-latency regional rooms.
3. **Persistence & archival**: Trim streams with `XTRIM` (we used `MAXLEN ~`). For long-term retention, run background jobs to copy stream data to a DB or object store.
4. **Consumer groups**: For notification workers, moderation, or analytics, use `XGROUP` with consumer groups to process streams independently from gateways.
5. **Monitoring**: Expose metrics: publish/subscribe latency, Redis memory and eviction metrics, stream lengths, consumer lag (`XPENDING`), per-socket queue depth, dropped messages.
6. **Security**: Never expose raw channel names to clients. Keep Redis in a private network, enable TLS, and use ACLs.
7. **Backpressure & QoS**: Improve message prioritization (typing vs chat) and implement QoS (e.g., guaranteed delivery only for important messages).
8. **Exactly-once**: This architecture is at-least-once with dedupe; if you need stronger guarantees, implement idempotency at business level (messages with server-assigned ids, idempotent handlers).
9. **Throttling with Lua**: For more precise rate limits (token bucket), implement the logic in a Lua script to avoid race conditions.

---

# Quick run instructions

1. `npm install`
2. Start Redis locally (`redis-server`) or set `REDIS_URL` env var to your Redis instance.
3. `npm start` to run `gateway.js`
4. Open multiple `sample_client.html` in your browser (serve it from a file or a simple static server). Use different tokens (user IDs) and observe broadcasts across multiple browser clients.

---


# Notification Service that acts as a Redis Stream consumer group. The idea is:

1. Every time a message is appended to a room stream (`XADD stream:room:{room}`), it’s durable.
2. The consumer group reads messages reliably, even if a gateway or worker crashes.
3. The service can send notifications (push/email) to users who are offline.

This separates real-time delivery (gateway) from **asynchronous offline handling** (Notification Service).

We’ll do a practical Node.js example using `ioredis`.

---

### notification\_worker.js

```js
/**
 * Notification Service (Consumer Group)
 *
 * Reads messages from Redis Streams via a consumer group.
 * Sends notifications (simulated) to offline users.
 *
 * Assumptions:
 *  - Redis Streams already exist for rooms
 *  - Messages are appended with fields: from, msg, ts
 *  - User online status is tracked (simple demo with Redis SET 'online:userId')
 *
 * Usage:
 *   node notification_worker.js
 */

const IORedis = require('ioredis');

const REDIS_URL = process.env.REDIS_URL || 'redis://127.0.0.1:6379';
const redis = new IORedis(REDIS_URL);

// Example configuration
const ROOM_LIST = ['room-1', 'room-2']; // rooms to watch
const CONSUMER_NAME = `notif-worker-${process.pid}`;

// Helper functions
function streamKey(room) {
  return `stream:room:{${room}}`;
}

// simulate checking if a user is online
async function isUserOnline(userId) {
  const val = await redis.get(`online:${userId}`);
  return val === '1';
}

// simulate sending push/email
async function sendOfflineNotification(userId, message) {
  console.log(`Sending offline notification to ${userId}:`, message);
  // Replace this with real push/email logic
  await new Promise(res => setTimeout(res, 10)); // simulate async delay
}

// Create consumer group if not exists
async function ensureConsumerGroup(room) {
  const key = streamKey(room);
  try {
    await redis.xgroup('CREATE', key, 'notif_group', '0', 'MKSTREAM');
    console.log(`Created consumer group for ${key}`);
  } catch (err) {
    if (!/BUSYGROUP/.test(err.message)) {
      console.error('Error creating consumer group', err);
    }
  }
}

// Consume loop for one room
async function consumeRoom(room) {
  const key = streamKey(room);
  while (true) {
    try {
      // XREADGROUP BLOCK 5000 COUNT 10 GROUP notif_group CONSUMER_NAME streamKey >
      const res = await redis.xreadgroup(
        'GROUP', 'notif_group', CONSUMER_NAME,
        'BLOCK', 5000,
        'COUNT', 10,
        'STREAMS', key, '>'
      );
      if (!res) continue; // timeout, loop again

      const [stream, messages] = res[0];
      for (const msg of messages) {
        const id = msg[0];
        const kv = msg[1];
        const obj = {};
        for (let i = 0; i < kv.length; i += 2) obj[kv[i]] = kv[i + 1];

        // parse payload
        let payload = null;
        try { payload = JSON.parse(obj.msg); } catch(e) { payload = obj.msg; }

        // Determine recipients (for simplicity, assume all room users)
        // In production: track room members, exclude sender
        const recipients = await redis.smembers(`room_users:${room}`);
        for (const userId of recipients) {
          if (userId === obj.from) continue; // skip sender
          const online = await isUserOnline(userId);
          if (!online) {
            await sendOfflineNotification(userId, payload);
          }
        }

        // Acknowledge message
        await redis.xack(key, 'notif_group', id);
      }
    } catch (err) {
      console.error('Error consuming stream', err);
      await new Promise(res => setTimeout(res, 1000));
    }
  }
}

// Bootstrap
async function start() {
  // ensure consumer groups exist
  for (const room of ROOM_LIST) await ensureConsumerGroup(room);

  // start consuming all rooms concurrently
  for (const room of ROOM_LIST) {
    consumeRoom(room);
  }

  console.log('Notification worker started for rooms:', ROOM_LIST);
}

start().catch(console.error);
```

---

### How it works

1. **Consumer group**:

   * `notif_group` is created per room stream.
   * Multiple worker processes can share a consumer group for horizontal scaling.

2. **Reliable processing**:

   * `XREADGROUP` with `BLOCK` reads new messages.
   * `XACK` acknowledges processed messages.
   * If a worker crashes, pending messages remain in the stream (`XPENDING`) and can be claimed by another worker.

3. **Offline detection**:

   * `online:userId` key determines if a user is connected.
   * In production, your gateway should set `online:userId = 1` on connect and expire on disconnect/timeout.

4. **Notification sending**:

   * If user is offline, simulate push/email.
   * This could call Firebase, APNs, or an email service.

5. **Scaling**:

   * Add more worker processes with the same `notif_group` and `XREADGROUP`.
   * Redis distributes stream messages across consumers.

---

### Integrating with your gateway

* Gateway sets `online:userId = 1` with a TTL (e.g., 60s) and refreshes on heartbeat.
* Gateway adds/removes users from `room_users:{room}` set when users join/leave rooms.
* When a message is `XADD`-ed to `stream:room:{room}`, the worker consumes it asynchronously for offline notifications.
* You can create multiple notification workers for scaling or for different notification channels (email, push, SMS).

---

Perfect! Let’s build an **end-to-end system** that combines the WebSocket Gateway with Redis Streams, Pub/Sub, and a Notification Worker for offline users. I’ll provide a **single cohesive Node.js setup** with the following features:

1. **WebSocket Gateway**:

   * Handles client connections, authentication, rooms, and messages.
   * Updates online presence (`online:userId`) and room memberships (`room_users:{room}`).
   * Pushes messages to Redis Streams (`XADD`) and Pub/Sub (`PUBLISH`).
   * Supports replay for reconnecting clients (`XREAD`).

2. **Notification Worker** (integrated in the same server for simplicity, could be separate process in production):

   * Consumes room streams using Redis consumer groups (`XREADGROUP`).
   * Checks if users are online; if offline, simulates sending push/email.

3. **Deduplication**, **rate-limiting**, and **replay** remain.

---

### Complete Combined Gateway + Notification Worker

```js
/**
 * full_gateway_with_notifications.js
 * 
 * Features:
 * - WebSocket gateway (ws)
 * - Redis Streams + Pub/Sub
 * - Deduplication
 * - Rate limiting
 * - Replay for reconnects
 * - Online presence tracking
 * - Room membership tracking
 * - Notification worker for offline users
 */

const WebSocket = require('ws');
const IORedis = require('ioredis');
const { v4: uuidv4 } = require('uuid');

const REDIS_URL = process.env.REDIS_URL || 'redis://127.0.0.1:6379';
const PORT = Number(process.env.PORT || 8080);

const redis = new IORedis(REDIS_URL);
const pub = new IORedis(REDIS_URL);
const sub = new IORedis(REDIS_URL);

// Local state
const localRoomMembers = new Map(); // room -> Set(socketId)
const roomSubWorkers = new Map(); // room -> subscribed?
const wsById = new Map(); // socketId -> SocketWrapper

// Config
const STREAM_MAXLEN = 10000;
const OUTBOUND_QUEUE_LIMIT = 1000;
const DEDUPE_TTL_SEC = 60;
const RATE_LIMIT_MAX_PER_MIN = 120;

// Helper keys
const streamKey = room => `stream:room:{${room}}`;
const channelKey = room => `chat:room:{${room}}`;
const seenKey = streamId => `seen:${streamId}`;
const onlineKey = userId => `online:${userId}`;
const roomUsersKey = room => `room_users:${room}`;
const NOTIF_CONSUMER_GROUP = 'notif_group';

// Authenticate (demo: token === userId)
async function authenticate(token) {
  if (!token) return null;
  return { userId: token };
}

// Rate limiter
async function isRateLimited(userId) {
  const key = `rl:emit:${userId}:${Math.floor(Date.now() / 60000)}`;
  const val = await redis.incr(key);
  if (val === 1) await redis.expire(key, 70);
  return val > RATE_LIMIT_MAX_PER_MIN;
}

// Ensure Redis Pub/Sub subscription
async function ensureRoomSubscribed(room) {
  if (roomSubWorkers.has(room)) return;
  const channel = channelKey(room);
  roomSubWorkers.set(room, true);
  await sub.subscribe(channel, message => {
    handlePubSubMessage(room, message).catch(console.error);
  });
  console.log('Subscribed locally to', channel);
}

// Maybe unsubscribe if no local members
async function maybeUnsubscribeRoom(room) {
  const members = localRoomMembers.get(room);
  if (members && members.size > 0) return;
  if (!roomSubWorkers.has(room)) return;
  const channel = channelKey(room);
  await sub.unsubscribe(channel);
  roomSubWorkers.delete(room);
  console.log('Unsubscribed locally from', channel);
}

// Handle Pub/Sub messages (from other gateway instances)
async function handlePubSubMessage(room, message) {
  let msg;
  try { msg = JSON.parse(message); } catch(e) { return; }
  const sId = msg.streamId;
  if (sId) {
    const dedup = await redis.setnx(seenKey(sId), '1');
    if (dedup === 0) return; // already seen
    await redis.expire(seenKey(sId), DEDUPE_TTL_SEC);
  }
  const members = localRoomMembers.get(room);
  if (!members) return;
  const payload = JSON.stringify({ type: 'message', data: msg });
  for (const sockId of members) {
    const wrapper = wsById.get(sockId);
    if (!wrapper) continue;
    wrapper.enqueueOutbound(payload);
  }
}

// WebSocket wrapper
class SocketWrapper {
  constructor(ws, userId) {
    this.ws = ws;
    this.userId = userId;
    this.id = uuidv4();
    this.rooms = new Set();
    this.outboundQueue = [];
    this.sending = false;
    this.closed = false;

    ws.on('close', () => this.onClose());
    ws.on('error', err => this.onClose());
    ws.on('pong', () => {}); // optional heartbeat
  }

  enqueueOutbound(msg) {
    if (this.closed) return;
    if (this.outboundQueue.length >= OUTBOUND_QUEUE_LIMIT) this.outboundQueue.shift();
    this.outboundQueue.push(msg);
    this.flushOutbound();
  }

  async flushOutbound() {
    if (this.sending) return;
    this.sending = true;
    while (this.outboundQueue.length > 0 && this.ws.readyState === WebSocket.OPEN) {
      const msg = this.outboundQueue.shift();
      try {
        await new Promise((res, rej) => this.ws.send(msg, err => err ? rej(err) : res()));
      } catch (err) { break; }
    }
    this.sending = false;
  }

  joinRoom(room) {
    if (this.rooms.has(room)) return;
    this.rooms.add(room);
    if (!localRoomMembers.has(room)) localRoomMembers.set(room, new Set());
    localRoomMembers.get(room).add(this.id);
    ensureRoomSubscribed(room);
    redis.sadd(roomUsersKey(room), this.userId);
  }

  leaveRoom(room) {
    if (!this.rooms.has(room)) return;
    this.rooms.delete(room);
    const set = localRoomMembers.get(room);
    if (set) {
      set.delete(this.id);
      if (set.size === 0) localRoomMembers.delete(room);
    }
    maybeUnsubscribeRoom(room);
    redis.srem(roomUsersKey(room), this.userId);
  }

  async onClose() {
    if (this.closed) return;
    this.closed = true;
    for (const room of Array.from(this.rooms)) this.leaveRoom(room);
    wsById.delete(this.id);
    await redis.del(onlineKey(this.userId));
  }
}

// Persist + publish message
async function persistAndPublish(room, fromUser, payload) {
  const sKey = streamKey(room);
  const id = await redis.xadd(sKey, 'MAXLEN', '~', STREAM_MAXLEN, '*',
    'from', fromUser, 'msg', JSON.stringify(payload), 'ts', Date.now().toString());
  const channel = channelKey(room);
  const pubpayload = JSON.stringify({ streamId: id, from: fromUser, payload, ts: Date.now() });
  await pub.publish(channel, pubpayload);
  return id;
}

// Replay stream
async function replaySince(room, sinceId, emitFn) {
  const sKey = streamKey(room);
  let lastId = sinceId || '0-0';
  while (true) {
    const res = await redis.xread('COUNT', 200, 'STREAMS', sKey, lastId);
    if (!res || res.length === 0) break;
    const [stream, messages] = res[0];
    for (const msg of messages) {
      const id = msg[0];
      const kv = msg[1];
      const obj = {};
      for (let i = 0; i < kv.length; i += 2) obj[kv[i]] = kv[i + 1];
      let parsedMsg = null;
      try { parsedMsg = JSON.parse(obj.msg); } catch(e) { parsedMsg = obj.msg; }
      await emitFn(JSON.stringify({ type: 'replay', data: { streamId: id, from: obj.from, payload: parsedMsg, ts: obj.ts } }));
      lastId = id;
    }
    if (messages.length < 200) break;
  }
}

// Handle client message
async function handleClientMessage(sockWrapper, msgObj) {
  if (msgObj.type === 'send') {
    const room = msgObj.room;
    const payload = msgObj.payload;
    const userId = sockWrapper.userId;
    if (await isRateLimited(userId)) {
      sockWrapper.enqueueOutbound(JSON.stringify({ type: 'error', error: 'rate_limited' }));
      return;
    }
    const streamId = await persistAndPublish(room, userId, payload);
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'sent', streamId, room, ts: Date.now() }));
  } else if (msgObj.type === 'join') {
    const room = msgObj.room;
    sockWrapper.joinRoom(room);
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'joined', room }));
  } else if (msgObj.type === 'leave') {
    const room = msgObj.room;
    sockWrapper.leaveRoom(room);
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'left', room }));
  } else if (msgObj.type === 'replay') {
    const room = msgObj.room;
    const sinceId = msgObj.sinceId || '0-0';
    await replaySince(room, sinceId, async payload => sockWrapper.enqueueOutbound(payload));
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'replay_done', room }));
  } else {
    sockWrapper.enqueueOutbound(JSON.stringify({ type: 'error', error: 'unknown_type' }));
  }
}

// WebSocket server
const wss = new WebSocket.Server({ port: PORT }, () => console.log(`WS gateway listening on ${PORT}`));

wss.on('connection', async (ws, req) => {
  const url = new URL(req.url ?? '', `http://${req.headers.host}`);
  const token = url.searchParams.get('token');
  const auth = await authenticate(token);
  if (!auth) {
    ws.send(JSON.stringify({ type: 'error', error: 'unauthenticated' }));
    ws.close();
    return;
  }

  const wrapper = new SocketWrapper(ws, auth.userId);
  wsById.set(wrapper.id, wrapper);

  // mark online with 60s TTL (heartbeat to refresh)
  await redis.set(onlineKey(auth.userId), '1', 'EX', 60);

  ws.on('message', async raw => {
    let obj;
    try { obj = JSON.parse(raw.toString()); } catch(e) { wrapper.enqueueOutbound(JSON.stringify({ type: 'error', error: 'invalid_json' })); return; }
    try { await handleClientMessage(wrapper, obj); } catch(err) { wrapper.enqueueOutbound(JSON.stringify({ type: 'error', error: 'server_error' })); console.error(err); }
  });

  // heartbeat ping
  const pingInterval = setInterval(() => { if (ws.readyState === WebSocket.OPEN) ws.ping(); else clearInterval(pingInterval); }, 30000);

  wrapper.enqueueOutbound(JSON.stringify({ type: 'welcome', socketId: wrapper.id, userId: wrapper.userId }));
});

// ------------------ Notification Worker ------------------

// Ensure consumer group exists
async function ensureNotifConsumerGroup(room) {
  const key = streamKey(room);
  try {
    await redis.xgroup('CREATE', key, NOTIF_CONSUMER_GROUP, '0', 'MKSTREAM');
  } catch(err) { if (!/BUSYGROUP/.test(err.message)) console.error(err); }
}

// Check if user is online
async function isUserOnline(userId) {
  const val = await redis.get(onlineKey(userId));
  return val === '1';
}

// Send simulated notification
async function sendOfflineNotification(userId, message) {
  console.log(`Sending offline notification to ${userId}:`, message);
}

// Consume stream for a room
async function consumeRoom(room) {
  const key = streamKey(room);
  while (true) {
    try {
      const res = await redis.xreadgroup('GROUP', NOTIF_CONSUMER_GROUP, `worker-${process.pid}`, 'BLOCK', 5000, 'COUNT', 10, 'STREAMS', key, '>');
      if (!res) continue;
      const [stream, messages] = res[0];
      for (const msg of messages) {
        const id = msg[0];
        const kv = msg[1];
        const obj = {};
        for (let i = 0; i < kv.length; i+=2) obj[kv[i]] = kv[i+1];
        let payload = null; try { payload = JSON.parse(obj.msg); } catch(e) { payload = obj.msg; }
        // get room members
        const recipients = await redis.smembers(roomUsersKey(room));
        for (const userId of recipients) {
          if (userId === obj.from) continue;
          const online = await isUserOnline(userId);
          if (!online) await sendOfflineNotification(userId, payload);
        }
        await redis.xack(key, NOTIF_CONSUMER_GROUP, id);
      }
    } catch(err) { console.error('Notification worker error', err); await new Promise(res => setTimeout(res, 1000)); }
  }
}

// Start notification workers
async function startNotificationWorkers() {
  const rooms = ['room-1','room-2']; // adapt to your rooms
  for (const room of rooms) await ensureNotifConsumerGroup(room);
  for (const room of rooms) consumeRoom(room);
}

startNotificationWorkers().catch(console.error);
```

---

### How it works

1. **Gateway**: Handles WebSocket clients, joins/leaves rooms, persists messages to Redis Stream, publishes via Pub/Sub.
2. **Online Presence**: `online:userId` key refreshed with TTL; offline users are detected by the notification worker.
3. **Room Membership**: `room_users:{room}` set tracks current members for the notification worker.
4. **Notification Worker**: Reads each room stream via consumer group, checks which users are offline, and sends notifications.
5. **Scaling**: Multiple gateways and workers can run in parallel; deduplication prevents double-sends.
6. **Replay**: Clients can reconnect and request replay from a last-seen Stream ID.

---


