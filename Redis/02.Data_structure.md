# Redis Data Structures: Complete Deep Dive

## 1. Strings: The Universal Data Type

### What Are Redis Strings?

Redis Strings are the most fundamental and versatile data type in Redis. Despite the name "String," they are actually binary-safe, meaning they can contain any kind of data—text, numbers, serialized objects, JSON, or even images and files. The term "binary-safe" indicates that Redis doesn't interpret the content; it simply stores the bytes exactly as provided. A Redis string can hold up to 512 megabytes of data.

When we talk about strings in Redis, we're really discussing a key-value pair where both the key and the value are strings. The key acts as a unique identifier, and the value is the data you want to store. This simplicity makes strings the building block for more complex operations and the go-to choice for many caching and storage scenarios.

### Core String Operations and Commands

**Basic Storage and Retrieval:**

The most fundamental operations with strings are SET and GET. The SET command stores a value at a specified key, while GET retrieves it.

```
SET user:profile:1000 "John Doe"
GET user:profile:1000
```

This stores the string "John Doe" with the key "user:profile:1000" and then retrieves it. The key naming convention with colons is a common practice that helps organize your data logically, though Redis doesn't enforce any structure—colons are just part of the key name.

**Setting Multiple Values Simultaneously:**

When you need to store multiple key-value pairs, using MSET is more efficient than multiple SET commands because it reduces network round trips.

```
MSET user:1000:name "Alice" user:1000:email "alice@example.com" user:1000:age "28"
MGET user:1000:name user:1000:email user:1000:age
```

MSET sets all three values atomically in a single operation. MGET retrieves multiple values at once, returning them in the same order as the keys were specified.

**Conditional Setting:**

Sometimes you only want to set a value if the key doesn't already exist, or conversely, only if it does exist.

```
SETNX session:token:abc123 "user_data"
SET session:token:abc123 "new_data" XX
```

SETNX means "SET if Not eXists"—it only sets the value if the key doesn't exist, returning 1 if successful or 0 if the key already existed. The XX option in SET means "only set if the key already exists," useful for updating existing values without accidentally creating new ones.

**String Operations with Automatic Expiration:**

One of Redis's most powerful features is the ability to automatically expire keys after a specified time period.

```
SET cache:product:500 '{"name":"Laptop","price":999}' EX 3600
SETEX cache:product:500 3600 '{"name":"Laptop","price":999}'
```

Both commands achieve the same result—storing the product data for 3600 seconds (1 hour). After this time, Redis automatically deletes the key. The EX parameter sets expiration in seconds, while PX sets it in milliseconds. SETEX is a shorthand command that combines SET with expiration.

**Numeric Operations on Strings:**

Redis can treat string values as numbers and perform atomic increment and decrement operations.

```
SET page:views:homepage 1000
INCR page:views:homepage
INCRBY page:views:homepage 5
DECR page:views:homepage
DECRBY page:views:homepage 3
```

INCR atomically increments the value by 1, resulting in 1001. INCRBY increments by a specified amount (5), resulting in 1006. DECR and DECRBY work similarly for decrementing. The critical word here is "atomic"—even if thousands of operations happen simultaneously, Redis guarantees accurate counting without race conditions.

**Working with Floating-Point Numbers:**

```
SET product:price 29.99
INCRBYFLOAT product:price 5.50
INCRBYFLOAT product:price -2.25
```

INCRBYFLOAT handles decimal numbers, increasing the price to 35.49, then to 33.24. This is essential for applications dealing with prices, ratings, or any decimal values that need precise arithmetic.

**String Manipulation Operations:**

Redis provides commands to work with portions of string values.

```
SET greeting "Hello World"
APPEND greeting " from Redis"
GET greeting
GETRANGE greeting 0 4
SETRANGE greeting 6 "Redis"
STRLEN greeting
```

APPEND adds text to the end of an existing string. GETRANGE retrieves a substring (characters 0 through 4, returning "Hello"). SETRANGE overwrites part of the string starting at position 6. STRLEN returns the length of the string. These operations are useful for manipulating text data without retrieving, modifying, and re-storing the entire value.

**Get and Set Pattern:**

```
GETSET lock:resource:database "server-02"
```

GETSET atomically sets a new value while returning the old value. This is useful for acquiring locks or tracking state changes where you need to know the previous value. In this example, server-02 is acquiring a lock on a database resource and can see which server previously held it.

### Advanced String Patterns

**Distributed Locking:**

A common pattern for coordinating actions across multiple servers uses SET with additional options.

```
SET lock:resource:api:external NX PX 30000
```

This attempts to acquire a lock on an external API resource. The NX option ensures it only succeeds if no lock exists. PX 30000 sets expiration to 30 seconds (30,000 milliseconds). If the server crashes while holding the lock, it automatically releases after 30 seconds, preventing deadlocks. This pattern is fundamental to distributed systems where multiple servers need to coordinate access to shared resources.

**Counter with Expiration:**

Combining numeric operations with expiration creates powerful rate-limiting mechanisms.

```
SET ratelimit:user:1000:20241123:14:30 0 EX 60 NX
INCR ratelimit:user:1000:20241123:14:30
```

This creates a counter that tracks requests from user 1000 during a specific minute. The first command initializes the counter only if it doesn't exist (NX), setting it to expire after 60 seconds. Each request increments the counter. Before processing a request, you check if the counter exceeds your limit (e.g., 100 requests per minute).

### Production Use Cases for Strings

**High-Performance Caching Layer:**

In production environments, database queries are often expensive operations that can take tens or hundreds of milliseconds. Redis strings provide a caching solution that reduces these queries to microseconds.

Consider an e-commerce platform where product details are frequently accessed. Instead of querying the database for every page view:

```
SET product:details:12345 '{"id":12345,"name":"Wireless Mouse","price":29.99,"stock":150}' EX 3600
```

The first request fetches product details from the database, serializes them to JSON, and stores them in Redis with a 1-hour expiration. Subsequent requests retrieve from Redis instantly. When product details change in the database, you either update the Redis cache or delete the key, forcing the next request to refresh from the database. This pattern can reduce database load by 80-90% in high-traffic applications.

**Session Management in Distributed Systems:**

Modern web applications often run across multiple servers behind a load balancer. When a user logs in, their session data must be accessible from any server.

```
SET session:token:9f8a7b6c5d '{"user_id":1000,"role":"admin","login_time":1700000000}' EX 86400
```

The session token serves as the key, and the value contains user information as JSON. The 24-hour expiration (86400 seconds) automatically logs out inactive users. When a request arrives at any server, it extracts the session token, queries Redis, and retrieves the user's information in under a millisecond. This stateless approach allows servers to be added or removed without session loss.

**API Rate Limiting with Sliding Windows:**

Protecting your API from abuse requires tracking request rates per user or API key.

```
SET ratelimit:api:user:5000:1700000400 1 EX 60 NX
INCR ratelimit:api:user:5000:1700000400
```

Each minute gets a unique counter based on the Unix timestamp divided by 60. Before processing an API request, you check the current counter. If it exceeds your limit (say, 100 requests per minute), you reject the request with a 429 status code. The automatic 60-second expiration ensures counters clean themselves up, preventing unbounded memory growth.

**Feature Flags and A/B Testing:**

Controlling which features users see without redeploying code is crucial for modern applications.

```
SET feature:new_checkout:enabled "true" EX 3600
SET ab_test:homepage_layout:user:1000 "variant_b" EX 604800
```

Feature flags stored in Redis can be toggled instantly across all servers. A/B test assignments persist for a week (604800 seconds), ensuring users see consistent experiences. Your application checks these flags before rendering features, enabling controlled rollouts and instant rollbacks if issues arise.

**Configuration Management:**

Applications often need configuration values that can change without restarting servers.

```
MSET config:max_upload_size "10485760" config:api_timeout "30" config:retry_attempts "3"
```

Configuration values stored in Redis can be updated in real-time. Applications periodically refresh their configuration from Redis (perhaps every few seconds), enabling dynamic configuration changes across your entire infrastructure without downtime.

**Distributed Counters and Metrics:**

Tracking real-time metrics across multiple servers requires a centralized counter.

```
INCR metrics:api:requests:total
INCR metrics:api:requests:successful
INCR metrics:api:errors:500
```

Multiple application servers increment these counters simultaneously. Redis's atomic operations ensure accurate counts even under high concurrency. Monitoring systems periodically read these counters to display dashboards, and you can reset counters daily or hourly using expiration or scheduled jobs.

**URL Shortening Services:**

Services like bit.ly use Redis strings to map short codes to full URLs.

```
SET short:a1b2c3 "https://example.com/very/long/url/path" 
INCR short:url:counter
```

The short code maps to the full URL. When users visit the short URL, your service performs a GET, retrieves the full URL, and redirects. The counter generates unique short codes. This pattern handles millions of redirects per second with minimal latency.

---

## 2. Lists: Ordered Sequential Collections

### What Are Redis Lists?

Redis Lists are ordered collections of strings, implemented as linked lists. This means they maintain the insertion order of elements and support efficient addition and removal operations at both ends (head and tail). Unlike arrays, lists don't provide instant access to elements in the middle, but they excel at queue-like operations where you process elements sequentially.

The linked list implementation means that adding an element to either end of a list takes constant time O(1), regardless of the list's size. Whether your list has 10 elements or 10 million elements, pushing or popping from either end is equally fast. However, accessing elements by index in the middle of the list requires traversing from one end, taking O(N) time.

### Core List Operations and Commands

**Adding Elements to Lists:**

Lists support adding elements to either the left (head) or right (tail) end.

```
LPUSH notifications:user:1000 "New message from Alice"
LPUSH notifications:user:1000 "Your order has shipped"
RPUSH notifications:user:1000 "Price drop on saved item"
```

LPUSH adds elements to the left (beginning) of the list. Each new element becomes the first element. RPUSH adds to the right (end) of the list. After these operations, if we view the list from left to right, we'd see: "Your order has shipped", "New message from Alice", "Price drop on saved item". Notice how LPUSH elements appear in reverse order because each new element pushes to the front.

**Retrieving Elements Without Removal:**

```
LRANGE notifications:user:1000 0 -1
LRANGE notifications:user:1000 0 9
LINDEX notifications:user:1000 0
```

LRANGE retrieves a range of elements. The indices are zero-based: 0 is the first element, -1 is the last element, -2 is the second-to-last, and so on. Using `0 -1` retrieves the entire list. Using `0 9` retrieves the first 10 elements. LINDEX gets a single element at a specific index—here, the first element (index 0).

**Removing Elements from Lists:**

```
LPOP queue:emails
RPOP queue:emails
LPOP queue:emails 3
```

LPOP removes and returns the leftmost (first) element. RPOP removes and returns the rightmost (last) element. The third command shows Redis 6.2+ syntax for popping multiple elements at once (here, 3 elements from the left).

**Blocking Pop Operations:**

One of the most powerful features of Redis lists is blocking operations.

```
BLPOP queue:tasks 30
BRPOP queue:priority:high queue:priority:normal 10
```

BLPOP blocks (waits) until an element is available or until timeout (30 seconds here). If the list is empty, the command doesn't return immediately—it waits for another client to push an element. This is perfect for worker processes that need to wait for jobs without constantly polling. BRPOP can watch multiple lists, returning the first element from whichever list gets data first—enabling priority queues where high-priority tasks are processed before normal ones.

**List Length and Existence:**

```
LLEN queue:emails
EXISTS queue:emails
```

LLEN returns the number of elements in the list. This is an O(1) operation since Redis maintains the count internally. EXISTS checks if the key exists, returning 1 for exists or 0 for doesn't exist.

**Trimming Lists:**

```
LPUSH recent:articles "article:500"
LTRIM recent:articles 0 99
```

LTRIM keeps only the specified range of elements, removing all others. This example maintains only the 100 most recent articles (indices 0 through 99). Each time you add a new article with LPUSH, you follow with LTRIM to enforce the size limit. This is an efficient way to maintain fixed-size lists without manual cleanup.

**Inserting Elements at Specific Positions:**

```
LINSERT tasks:todo BEFORE "task:prepare_report" "task:collect_data"
LINSERT tasks:todo AFTER "task:prepare_report" "task:review_report"
```

LINSERT adds elements before or after a specific element in the list. This is useful when you need to maintain specific ordering based on dependencies or priorities. Note that LINSERT requires searching through the list for the pivot element, so it's O(N) operation.

**Removing Specific Elements:**

```
LREM notifications:user:1000 0 "Spam notification"
LREM notifications:user:1000 2 "Duplicate alert"
LREM notifications:user:1000 -1 "Old notification"
```

LREM removes occurrences of an element. The count parameter determines behavior: 0 removes all occurrences, positive numbers remove that many from the left, negative numbers remove that many from the right. This is useful for cleaning up duplicates or removing specific items from queues.

**Setting Values at Specific Indices:**

```
LSET tasks:todo 0 "URGENT: Complete project report"
```

LSET replaces the value at a specific index. Index 0 is the first element. This operation requires the index to exist—you can't use it to expand a list, only to modify existing elements.

### Advanced List Patterns

**Implementing a Capped Collection:**

A pattern for maintaining a rolling collection of recent items:

```
LPUSH recent:logins:user:1000 '{"time":1700000000,"ip":"192.168.1.1"}'
LTRIM recent:logins:user:1000 0 49
```

This maintains the 50 most recent login records. Each new login is pushed to the left, and trimming ensures the list never exceeds 50 elements. This pattern is memory-efficient for maintaining recent history without unbounded growth.

**List as a Stack (LIFO):**

```
LPUSH stack:undo "action1"
LPUSH stack:undo "action2"
LPUSH stack:undo "action3"
LPOP stack:undo
```

Using LPUSH and LPOP on the same end creates a Last-In-First-Out (LIFO) stack. The most recently pushed element is the first one popped. This is useful for implementing undo functionality or parsing operations.

**List as a Queue (FIFO):**

```
RPUSH queue:tasks "task1"
RPUSH queue:tasks "task2"
LPOP queue:tasks
```

Using RPUSH to add and LPOP to remove creates a First-In-First-Out (FIFO) queue. Elements are processed in the order they were added. This is the most common pattern for job queues.

### Production Use Cases for Lists

**Background Job Processing Queue:**

One of the most prevalent uses of Redis lists in production is implementing reliable job queues for background processing.

Consider an image processing service where users upload photos that need resizing, filtering, and thumbnail generation. Handling these operations synchronously would make uploads unbearably slow. Instead:

```
RPUSH queue:images:process '{"image_id":"img_12345","user_id":1000,"operations":["resize","thumbnail"]}'
RPUSH queue:images:process '{"image_id":"img_12346","user_id":1001,"operations":["filter","resize"]}'
```

Your web servers push image processing jobs to the right end of the queue as they arrive. Separate worker processes run continuously:

```
BLPOP queue:images:process 30
```

Workers block-wait for jobs, pulling from the left end. When a job appears, a worker immediately receives it and begins processing. If processing succeeds, the job is done. If it fails, the worker can push the job to a retry queue or dead-letter queue for later analysis. This pattern scales beautifully—you can run dozens of worker processes, and Redis ensures each job is delivered to exactly one worker.

For fault tolerance, you might use a more sophisticated pattern with RPOPLPUSH:

```
RPOPLPUSH queue:images:process queue:images:processing
```

This atomically moves a job from the pending queue to a processing queue. If a worker crashes while processing, another process can inspect the processing queue and re-queue jobs that have been there too long. After successful completion, workers remove jobs from the processing queue.

**Activity Feeds and Timeline Implementation:**

Social media platforms use Redis lists to power user activity feeds efficiently.

```
LPUSH feed:user:1000 '{"type":"post","user":"Alice","content":"Just learned Redis!","time":1700000000}'
LPUSH feed:user:1000 '{"type":"like","user":"Bob","post_id":5000,"time":1700000010}'
LTRIM feed:user:1000 0 499
```

Each action in a user's network (posts, likes, comments, shares) gets pushed to their feed. LTRIM maintains only the 500 most recent activities, preventing unbounded memory growth. When a user opens their app:

```
LRANGE feed:user:1000 0 49
```

This instantly retrieves the 50 most recent activities for display. As the user scrolls, subsequent LRANGE commands fetch older activities. This approach provides blazingly fast feed loading compared to complex database queries joining multiple tables.

For a more sophisticated implementation, you might fan-out activities to all followers' feeds when an action occurs. When Alice posts, your system pushes the post to feed lists for all her followers. This trades write amplification (one post = many writes) for ultra-fast reads, which is often worthwhile since users read feeds far more often than they post.

**Real-Time Messaging and Chat Systems:**

Chat applications require storing recent messages for users joining conversations or reconnecting after network issues.

```
LPUSH chat:room:5000 '{"user":"Charlie","msg":"Anyone here?","time":1700000000}'
LPUSH chat:room:5000 '{"user":"Dana","msg":"Hi Charlie!","time":1700000003}'
LTRIM chat:room:5000 0 999
```

Each chat room has a list maintaining the 1,000 most recent messages. When a user joins a room or scrolls up for history:

```
LRANGE chat:room:5000 0 49
```

They receive the 50 most recent messages. As new messages arrive, they're pushed to the list, and all connected clients are notified (perhaps through Redis Pub/Sub). This pattern provides instant message history without complex database queries.

For private conversations between two users, you might maintain a single list for the conversation:

```
LPUSH conversation:user:1000:user:2000 '{"from":1000,"msg":"Hi there!","time":1700000000}'
```

Both users read from the same list, providing a shared message history.

**Log Aggregation in Distributed Systems:**

In microservice architectures with dozens of services running across hundreds of servers, centralized logging is crucial for debugging.

```
RPUSH logs:application:errors '{"service":"api","server":"web-03","level":"error","msg":"Database timeout","time":1700000000}'
RPUSH logs:application:errors '{"service":"auth","server":"web-07","level":"error","msg":"Redis connection failed","time":1700000005}'
```

Each service pushes log entries to Redis lists. A separate log processing service continuously consumes these entries:

```
BLPOP logs:application:errors 5
```

The processor might write logs to Elasticsearch for searching, trigger alerts for critical errors, or update dashboards. If the processor falls behind, logs accumulate in Redis rather than being lost. You can run multiple processors in parallel to handle high volumes, with each consuming from the same list.

You might maintain separate lists for different log levels (errors, warnings, info) or different services, allowing flexible processing based on urgency and importance.

**Rate Limiting with Sliding Windows (Alternative Approach):**

While sorted sets provide more precise rate limiting, lists offer a simpler alternative for basic rate limiting.

```
LPUSH ratelimit:user:1000 "1700000000"
LTRIM ratelimit:user:1000 0 99
LLEN ratelimit:user:1000
```

Each API request pushes the current timestamp to a list, trimmed to 100 elements. The list length indicates recent request count. Before processing a request, if LLEN returns 100, you reject it. To make this a proper sliding window, you'd need to remove expired timestamps, which is where sorted sets become superior. However, for simple fixed-window rate limiting, this pattern works adequately with minimal complexity.

**Task Scheduling with Priority Queues:**

Complex applications often have tasks with different priority levels requiring guaranteed ordering.

```
LPUSH queue:priority:critical "task:critical_1"
LPUSH queue:priority:high "task:high_1"
LPUSH queue:priority:normal "task:normal_1"
```

Workers use BRPOP with multiple queues:

```
BRPOP queue:priority:critical queue:priority:high queue:priority:normal 30
```

BRPOP checks queues in order, returning the first available job from the highest priority queue. Critical tasks always process before high priority, which always process before normal priority. This ensures important tasks aren't starved by a flood of low-priority work.

**Implementing Leaderboards with Recent Scores:**

For games that care about recent performance rather than all-time scores:

```
LPUSH leaderboard:recent:scores '{"player":"Alice","score":5000,"time":1700000000}'
LTRIM leaderboard:recent:scores 0 9999
```

Maintaining the 10,000 most recent scores provides a rolling leaderboard of recent performance. You can process this list periodically to calculate top players based on recent activity, then store results in sorted sets for fast retrieval.

---

## 3. Sets: Unique Unordered Collections

### What Are Redis Sets?

Redis Sets are unordered collections of unique strings. The fundamental characteristic that defines a set is uniqueness—each element can appear only once within a set. Unlike lists, sets don't maintain any particular order, and you cannot have duplicate values. This makes sets perfect for scenarios where you need to track membership, eliminate duplicates automatically, or perform mathematical set operations.

The internal implementation of Redis sets uses hash tables, which provides O(1) time complexity for add, remove, and membership test operations. This means checking whether an element exists in a set with one million members takes the same amount of time as checking a set with just ten members. This exceptional performance makes sets invaluable for large-scale applications dealing with unique collections.

Sets also support powerful operations borrowed from mathematical set theory: union (combining sets), intersection (finding common elements), and difference (finding elements in one set but not another). These operations enable sophisticated queries and relationship modeling that would be complex and slow in traditional databases.

### Core Set Operations and Commands

**Adding and Removing Elements:**

```
SADD users:online "user:1000"
SADD users:online "user:1001" "user:1002" "user:1003"
SADD users:online "user:1000"
SREM users:online "user:1002"
```

SADD adds one or more members to a set. The first command adds user:1000. The second adds three users at once. The third attempts to add user:1000 again, but since it already exists, the set remains unchanged—SADD returns 0 to indicate no new members were added. SREM removes a member from the set.

**Membership Testing:**

```
SISMEMBER users:online "user:1000"
SISMEMBER users:online "user:9999"
```

SISMEMBER checks if an element exists in the set, returning 1 for true or 0 for false. This operation is incredibly fast regardless of set size. In the first command, it returns 1 because user:1000 is in the set. The second returns 0 because user:9999 is not a member.

**Retrieving Set Members:**

```
SMEMBERS tags:article:100
SCARD tags:article:100
```

SMEMBERS returns all members of a set. Be cautious with this command on large sets—if a set contains millions of members, SMEMBERS will return them all, which could overwhelm your network and memory. SCARD returns the cardinality (count) of members in O(1) time without retrieving them, making it safe for sets of any size.

**Random Element Selection:**

```
SRANDMEMBER lottery:participants
SRANDMEMBER lottery:participants 3
SPOP lottery:participants
SPOP lottery:participants 2
```

SRANDMEMBER returns random members without removing them. The first command returns one random member, useful for random selection. The second returns three random members. SPOP removes and returns random members. The first SPOP removes one member, perfect for lottery-style drawings where each participant can only win once. The second SPOP removes and returns two members.

**Moving Elements Between Sets:**

```
SMOVE users:online:server1 users:offline:server1 "user:1000"
```

SMOVE atomically moves a member from one set to another. This is useful for state transitions. If user:1000 disconnects from server1, you can move them from the online set to the offline set in a single atomic operation.

### Set Operations: Union, Intersection, and Difference

**Union - Combining Sets:**

```
SADD fruits:tropical "mango" "papaya" "pineapple"
SADD fruits:citrus "orange" "lemon" "lime"
SUNION fruits:tropical fruits:citrus
SUNIONSTORE fruits:all fruits:tropical fruits:citrus
```

SUNION returns all unique elements from multiple sets combined. In this example, it returns all six fruits. SUNIONSTORE performs the same operation but stores the result in a new set (fruits:all) rather than just returning it. This is useful when you need to reuse the result multiple times.

**Intersection - Finding Common Elements:**

```
SADD users:purchased:product500 "user:1000" "user:1001" "user:1002"
SADD users:purchased:product501 "user:1001" "user:1002" "user:1003"
SINTER users:purchased:product500 users:purchased:product501
SINTERSTORE users:purchased:both users:purchased:product500 users:purchased:product501
```

SINTER returns only elements that appear in all specified sets. Here, it returns user:1001 and user:1002 because they purchased both products. SINTERSTORE saves this result. This is powerful for finding customers who bought multiple products, users in multiple groups, or any scenario requiring "AND" logic.

**Difference - Finding Unique Elements:**

```
SADD users:trial "user:1000" "user:1001" "user:1002" "user:1003"
SADD users:converted "user:1001" "user:1003"
SDIFF users:trial users:converted
SDIFFSTORE users:still_trial users:trial users:converted
```

SDIFF returns elements in the first set that aren't in subsequent sets. This example finds trial users who haven't converted (user:1000 and user:1002). SDIFFSTORE saves the result. This is perfect for finding gaps, targeting unconverted users, or identifying missing relationships.

**Counting Common Elements:**

```
SINTERCARD 2 users:group:admins users:group:developers
```

SINTERCARD (available in Redis 7.0+) returns the count of elements in the intersection without retrieving them. The first parameter (2) indicates the number of sets to intersect. This is much more efficient than SINTER when you only need the count, especially with large sets.

### Advanced Set Patterns

**Set Scanning for Large Sets:**

```
SSCAN tags:popular:all 0 MATCH "tech:*" COUNT 100
```

SSCAN iterates through set members in chunks, preventing memory overload with huge sets. The cursor starts at 0, and Redis returns a new cursor and up to 100 members matching the pattern. You use the returned cursor in the next SSCAN call until the cursor returns to 0, indicating you've scanned the entire set.

### Production Use Cases for Sets

**Tagging System Implementation:**

E-commerce platforms, content management systems, and social media all use tagging systems extensively. Sets are perfect because tags are unique per item, and you often need to find items with specific tag combinations.

For each tagged item, maintain a set of tags:

```
SADD article:12345:tags "redis" "database" "caching" "performance"
SADD article:12346:tags "redis" "tutorial" "beginner"
SADD article:12347:tags "database" "sql" "optimization"
```

To enable tag-based searching, maintain reverse indexes—sets of articles for each tag:

```
SADD tag:redis:articles "article:12345" "article:12346"
SADD tag:database:articles "article:12345" "article:12347"
SADD tag:caching:articles "article:12345"
```

Finding all articles tagged with "redis":

```
SMEMBERS tag:redis:articles
```

Finding articles tagged with both "redis" AND "database":

```
SINTER tag:redis:articles tag:database:articles
```

This returns article:12345 because it's the only article with both tags. Finding articles with either "redis" OR "caching":

```
SUNION tag:redis:articles tag:caching:articles
```

For complex queries like "articles with 'redis' but NOT 'beginner'":

```
SDIFF tag:redis:articles tag:beginner:articles
```

This pattern scales to millions of articles and thousands of tags with consistent performance. When articles are edited, you update both the article's tag set and the reverse index sets atomically.

**Social Graph Relationships:**

Social networks use sets to model relationships efficiently. Each user has sets for followers, following, and various connection types.

```
SADD user:1000:followers "user:2000" "user:3000" "user:4000"
SADD user:1000:following "user:5000" "user:6000"
SADD user:2000:followers "user:1000" "user:3000"
SADD user:2000:following "user:1000" "user:5000"
```

Finding mutual followers between two users:

```
SINTER user:1000:followers user:2000:followers
```

This returns user:3000, who follows both users. Finding users that user:1000 follows who also follow them back (friends):

```
SINTER user:1000:following user:1000:followers
```

Suggesting new connections—people followed by users you follow but not by you:

```
SUNIONSTORE temp:suggestions user:5000:following user:6000:following
SDIFF temp:suggestions user:1000:following
DEL temp:suggestions
```

This finds everyone that users 5000 and 6000 follow, then removes people user:1000 already follows, generating connection suggestions. The temporary set is deleted after use.

For follower count (a common metric):

```
SCARD user:1000:followers
```

This instantly returns the count without retrieving the actual follower list, perfect for displaying on profiles.

**Real-Time Analytics for Unique Visitors:**

Tracking unique visitors, devices, or IP addresses is a classic set application. Traditional databases struggle with unique counting at scale, but Redis sets handle it effortlessly.

```
SADD analytics:visitors:2024-11-23 "ip:192.168.1.1"
SADD analytics:visitors:2024-11-23 "ip:192.168.1.2"
SADD analytics:visitors:2024-11 -23 "ip:192.168.1.1"
SCARD analytics:visitors:2024-11-23
```

Each visitor's IP (or user ID, or device fingerprint) is added to the day's set. Duplicate visits don't inflate the count because sets automatically maintain uniqueness. SCARD instantly returns the unique visitor count—no complex SQL GROUP BY queries needed.

Comparing visitor patterns across days:

```
SINTER analytics:visitors:2024-11-23 analytics:visitors:2024-11-22
```

This finds returning visitors who came both days. Finding new visitors who didn't visit yesterday:

```
SDIFF analytics:visitors:2024-11-23 analytics:visitors:2024-11-22
```

Finding users who visited last week but haven't returned this week:

```
SUNIONSTORE analytics:visitors:lastweek analytics:visitors:2024-11-17 analytics:visitors:2024-11-18 analytics:visitors:2024-11-19 analytics:visitors:2024-11-20 analytics:visitors:2024-11-21 analytics:visitors:2024-11-22 analytics:visitors:2024-11-23
SUNIONSTORE analytics:visitors:thisweek analytics:visitors:2024-11-24 analytics:visitors:2024-11-25 analytics:visitors:2024-11-26 analytics:visitors:2024-11-27 analytics:visitors:2024-11-28 analytics:visitors:2024-11-29 analytics:visitors:2024-11-30
SDIFF analytics:visitors:lastweek analytics:visitors:thisweek
```

This pattern enables sophisticated cohort analysis and retention metrics with simple set operations. You can maintain daily sets with expiration to automatically clean up old data:

```
SADD analytics:visitors:2024-11-23 "ip:192.168.1.1"
EXPIRE analytics:visitors:2024-11-23 2592000
```

This keeps daily visitor data for 30 days (2,592,000 seconds), after which Redis automatically deletes it.

**Permission and Access Control Systems:**

Authorization systems need fast permission checking for users accessing resources. Sets provide O(1) permission verification.

```
SADD user:1000:permissions "read:articles" "write:comments" "delete:own_comments" "upload:images"
SADD role:editor:permissions "read:articles" "write:articles" "edit:articles" "delete:own_articles"
SADD role:moderator:permissions "read:comments" "delete:any_comments" "ban:users"
```

Checking if a user has a specific permission:

```
SISMEMBER user:1000:permissions "delete:comments"
```

This returns 0 (false) because user:1000 can only delete their own comments, not any comment. For role-based access control (RBAC), combine user permissions with role permissions:

```
SADD user:1000:roles "editor" "moderator"
SUNIONSTORE user:1000:all_permissions user:1000:permissions role:editor:permissions role:moderator:permissions
```

Now checking permissions uses the combined set. This pattern scales to complex permission hierarchies. When user roles change, you simply update their roles set and regenerate the combined permissions.

For resource-level permissions:

```
SADD document:5000:viewers "user:1000" "user:1001"
SADD document:5000:editors "user:1000"
SISMEMBER document:5000:editors "user:1001"
```

This checks if user:1001 can edit document 5000, returning 0 (false—they can only view).

**Online User Tracking:**

Tracking which users are currently online across multiple servers requires a centralized data structure.

```
SADD users:online "user:1000"
EXPIRE users:online 300
SADD users:online "user:1001"
```

Each server adds users to the online set when they connect. However, the challenge is removing users when they disconnect or go idle. One approach uses short expiration with heartbeats—each user's connection sends periodic updates:

```
SADD users:online:active "user:1000"
EXPIRE users:online:active 60
```

The set expires after 60 seconds. If the user's connection sends a heartbeat every 30 seconds, they remain in the set. If they disconnect or their device sleeps, they're automatically removed within 60 seconds.

For server-specific tracking:

```
SADD users:online:server1 "user:1000" "user:1001"
SADD users:online:server2 "user:1002" "user:1003"
SUNIONSTORE users:online:all users:online:server1 users:online:server2 users:online:server3
```

This aggregates online users across all servers. You can find which server a user is connected to:

```
SISMEMBER users:online:server1 "user:1000"
```

**Content Deduplication:**

When processing user-submitted content, preventing duplicates is crucial. Sets provide efficient deduplication.

```
SADD articles:submitted:hashes "5f3a8b2c9d1e4f6a7b8c9d0e1f2a3b4c"
```

Before accepting a new article, you compute its hash (using MD5, SHA256, or a content fingerprinting algorithm) and check:

```
SISMEMBER articles:submitted:hashes "5f3a8b2c9d1e4f6a7b8c9d0e1f2a3b4c"
```

If this returns 1, the content already exists, and you can reject the submission or show the existing article. This pattern works for images, documents, or any content where duplicates should be prevented.

For URL deduplication in web crawlers:

```
SADD crawler:visited:urls "https://example.com/page1"
SISMEMBER crawler:visited:urls "https://example.com/page1"
```

Before crawling a URL, check if it's already been visited, preventing infinite loops and wasted bandwidth.

**Real-Time Recommendation Systems:**

Sets enable fast "users who liked this also liked" recommendations.

```
SADD product:500:liked_by "user:1000" "user:1001" "user:1002"
SADD product:501:liked_by "user:1001" "user:1002" "user:1003"
SADD product:502:liked_by "user:1002" "user:1003" "user:1004"
```

To recommend products to user:1000, find products liked by similar users:

```
SADD user:1000:liked "product:500"
SUNIONSTORE temp:similar_users product:500:liked_by
SREM temp:similar_users "user:1000"
```

Now find what these similar users also liked:

```
SUNIONSTORE temp:recommendations user:1001:liked user:1002:liked
SDIFF temp:recommendations user:1000:liked
```

This finds products liked by users who also liked product:500, excluding products user:1000 already liked. While this simplified example doesn't weight by popularity or similarity score, it demonstrates how sets enable real-time collaborative filtering.

**A/B Testing and Experiment Cohorts:**

Managing experiment cohorts requires tracking which users are in which groups.

```
SADD experiment:new_checkout:variant_a "user:1000" "user:1002" "user:1004"
SADD experiment:new_checkout:variant_b "user:1001" "user:1003" "user:1005"
SADD experiment:new_checkout:control "user:1006" "user:1007" "user:1008"
```

Checking a user's experiment variant:

```
SISMEMBER experiment:new_checkout:variant_a "user:1000"
```

Finding users in multiple experiments:

```
SINTER experiment:new_checkout:variant_a experiment:new_pricing:variant_b
```

This identifies users experiencing both the new checkout and new pricing simultaneously, helping detect interaction effects between experiments.

---

## 4. Sorted Sets (ZSETs): Ordered Collections with Scores

### What Are Redis Sorted Sets?

Redis Sorted Sets, often called ZSETs, are one of Redis's most powerful and unique data structures. They combine the uniqueness of sets with automatic ordering based on scores. Each element in a sorted set is a unique string member associated with a floating-point score. Elements are automatically maintained in order by their scores, from lowest to highest.

The internal implementation uses both a hash table and a skip list. The hash table provides O(1) access for checking membership and retrieving scores, while the skip list maintains sorted order and enables O(log N) operations for adding, updating, and range queries. This dual structure makes sorted sets incredibly versatile—you get the fast lookups of sets combined with the ordered access of sorted lists.

What makes sorted sets particularly special is that scores can be any double-precision floating-point number, and you can have multiple members with the same score. When members have identical scores, they're sorted lexicographically (alphabetically). This flexibility enables diverse use cases from leaderboards to time-series data to priority queues.

### Core Sorted Set Operations and Commands

**Adding Members with Scores:**

```
ZADD leaderboard:game:500 1000 "player:Alice"
ZADD leaderboard:game:500 2500 "player:Bob" 1800 "player:Charlie" 3200 "player:David"
ZADD leaderboard:game:500 NX 1500 "player:Eve"
ZADD leaderboard:game:500 XX 2600 "player:Bob"
```

ZADD adds members with their scores. The first command adds Alice with a score of 1000. The second adds three players at once—note the syntax alternates between score and member. The NX option means "only add if the member doesn't already exist," useful for preventing overwrites. The XX option means "only update if the member exists," preventing accidental additions. The fourth command updates Bob's score from 2500 to 2600.

**Retrieving Members in Order:**

```
ZRANGE leaderboard:game:500 0 -1
ZRANGE leaderboard:game:500 0 -1 WITHSCORES
ZREVRANGE leaderboard:game:500 0 9 WITHSCORES
```

ZRANGE retrieves members in ascending order by score (lowest to highest). Using `0 -1` gets all members. WITHSCORES includes the scores alongside members in the result. ZREVRANGE returns members in descending order (highest to lowest)—perfect for leaderboards where you want top scorers. Using `0 9` gets the top 10 players.

**Querying by Score Range:**

```
ZRANGEBYSCORE leaderboard:game:500 1500 2500
ZRANGEBYSCORE leaderboard:game:500 1500 2500 WITHSCORES LIMIT 0 10
ZRANGEBYSCORE leaderboard:game:500 (1500 2500
ZRANGEBYSCORE leaderboard:game:500 -inf 2000
```

ZRANGEBYSCORE retrieves members with scores in a specific range. The first command gets all members with scores between 1500 and 2500 (inclusive). The second includes scores and limits results to 10 members. The third uses `(1500` to make the lower bound exclusive (scores greater than 1500 but not equal). The fourth uses `-inf` for negative infinity, getting all members with scores up to 2000. You can also use `+inf` for positive infinity.

**Reverse Score Range Queries:**

```
ZREVRANGEBYSCORE leaderboard:game:500 2500 1500 WITHSCORES
```

ZREVRANGEBYSCORE works like ZRANGEBYSCORE but returns results in descending order. Note that the score parameters are reversed (max first, then min) to maintain intuitive syntax.

**Counting Members in Score Ranges:**

```
ZCOUNT leaderboard:game:500 1500 2500
```

ZCOUNT returns the number of members with scores in the specified range without retrieving them. This is O(log N) and much more efficient than ZRANGEBYSCORE when you only need the count.

**Getting Member Scores and Ranks:**

```
ZSCORE leaderboard:game:500 "player:Bob"
ZRANK leaderboard:game:500 "player:Bob"
ZREVRANK leaderboard:game:500 "player:Bob"
```

ZSCORE returns Bob's score (2600). ZRANK returns Bob's rank (position) counting from 0 for the lowest score. ZREVRANK returns Bob's rank counting from 0 for the highest score—more useful for leaderboards where you want to say "You're ranked #5."

**Incrementing Scores:**

```
ZINCRBY leaderboard:game:500 500 "player:Alice"
ZINCRBY leaderboard:game:500 -200 "player:Bob"
```

ZINCRBY atomically increments a member's score. Alice's score increases by 500 (from 1000 to 1500). Bob's score decreases by 200 (using negative increment). Redis automatically repositions members in the sorted set after score changes. If the member doesn't exist, ZINCRBY creates it with the increment as the initial score.

**Removing Members:**

```
ZREM leaderboard:game:500 "player:Alice"
ZREMRANGEBYRANK leaderboard:game:500 0 2
ZREMRANGEBYSCORE leaderboard:game:500 0 1000
```

ZREM removes specific members. ZREMRANGEBYRANK removes members by rank—this example removes the bottom three players (ranks 0, 1, and 2). ZREMRANGEBYSCORE removes members by score range—this removes all players with scores from 0 to 1000.

**Getting Sorted Set Size:**

```
ZCARD leaderboard:game:500
```

ZCARD returns the total number of members in the sorted set. This is O(1) since Redis maintains the count internally.

**Lexicographical Range Queries:**

When members have the same score, sorted sets offer lexicographical (alphabetical) operations.

```
ZADD words 0 "apple" 0 "banana" 0 "cherry" 0 "date" 0 "elderberry"
ZRANGEBYLEX words [b [d
ZRANGEBYLEX words - [c
```

All words have score 0, so they're sorted alphabetically. ZRANGEBYLEX retrieves by lexicographical range. The first command gets words from "b" to "d" (banana, cherry, date). The second uses `-` for "start from the beginning" and gets words up to "c" (apple, banana). The `[` means inclusive, while `(` means exclusive.

### Advanced Sorted Set Operations

**Set Operations on Sorted Sets:**

```
ZADD scores:math 90 "Alice" 85 "Bob" 95 "Charlie"
ZADD scores:english 88 "Alice" 92 "Bob" 78 "Charlie"
ZUNIONSTORE scores:total 2 scores:math scores:english
ZINTERSTORE scores:common 2 scores:math scores:english WEIGHTS 0.5 0.5
```

ZUNIONSTORE combines sorted sets, summing scores for members that appear in multiple sets. Alice's total would be 178 (90 + 88). ZINTERSTORE keeps only members present in all sets. WEIGHTS multiplies scores before combining—here, averaging the scores (multiplying by 0.5). You can also specify AGGREGATE options: SUM (default), MIN, or MAX to control how scores combine.

**Population Ranking:**

```
ZPOPMAX leaderboard:game:500 3
ZPOPMIN leaderboard:game:500 2
```

ZPOPMAX removes and returns the highest-scoring members—here, the top 3 players. ZPOPMIN removes and returns the lowest-scoring members. These are useful for processing top/bottom items in priority queues.

**Blocking Pop Operations:**

```
BZPOPMAX leaderboard:game:500 30
BZPOPMIN queue:priority 10
```

Similar to lists' BLPOP, these commands wait (block) until members are available or timeout expires. Perfect for worker processes waiting for high-priority tasks.

**Scanning Large Sorted Sets:**

```
ZSCAN leaderboard:game:500 0 MATCH "player:A*" COUNT 100
```

ZSCAN iterates through large sorted sets without loading everything into memory, similar to SSCAN for sets.

### Production Use Cases for Sorted Sets

**Gaming Leaderboards and Rankings:**

Leaderboards are the canonical example of sorted set usage because they require exactly what sorted sets provide: unique players with scores, automatically sorted, with fast rank lookups.

For a global game leaderboard:

```
ZADD leaderboard:global 45000 "player:a1b2c3d4"
ZADD leaderboard:global 52000 "player:e5f6g7h8"
ZADD leaderboard:global 38000 "player:i9j0k1l2"
```

When a player completes a level or earns points:

```
ZINCRBY leaderboard:global 5000 "player:a1b2c3d4"
```

This instantly updates their score and repositions them. Getting the top 10 players:

```
ZREVRANGE leaderboard:global 0 9 WITHSCORES
```

This returns the top 10 in milliseconds, even with millions of players. Showing a player their rank:

```
ZREVRANK leaderboard:global "player:a1b2c3d4"
```

If this returns 145, you tell the player "You're ranked #146" (adding 1 because ranks start at 0). Getting players around a specific player (context ranking):

```
ZREVRANK leaderboard:global "player:a1b2c3d4"
```

If the rank is 145, then:

```
ZREVRANGE leaderboard:global 143 147 WITHSCORES
```

This shows 2 players above, the player, and 2 players below—giving competitive context.

For time-limited leaderboards (weekly, monthly):

```
ZADD leaderboard:weekly:202447 45000 "player:a1b2c3d4"
EXPIRE leaderboard:weekly:202447 604800
```

The leaderboard automatically expires after a week. For multi-tier competitions:

```
ZRANGEBYSCORE leaderboard:global 40000 +inf WITHSCORES
```

This gets all players in the "Gold tier" (scores above 40,000). You can segment players into leagues dynamically based on score ranges.

**Priority Queue for Task Scheduling:**

Sorted sets excel at scheduling tasks for future execution, with scores representing Unix timestamps.

```
ZADD tasks:scheduled 1700000000 "task:send_email:user:1000"
ZADD tasks:scheduled 1700000300 "task:generate_report:daily"
ZADD tasks:scheduled 1700000600 "task:cleanup:temp_files"
```

A worker process continuously checks for due tasks:

```
ZRANGEBYSCORE tasks:scheduled -inf 1700000000 LIMIT 0 10
```

This gets up to 10 tasks that are due (score less than or equal to current timestamp). After retrieving tasks:

```
ZREM tasks:scheduled "task:send_email:user:1000"
```

The worker removes and processes the task. For atomic retrieval and removal:

```
ZPOPMIN tasks:scheduled 1
```

This gets the earliest task and removes it in one operation, preventing duplicate processing if multiple workers are running.

For recurring tasks, after processing, you can reschedule:

```
ZADD tasks:scheduled 1700086400 "task:generate_report:daily"
```

This pattern handles delayed jobs, cron-like scheduling, and ensures tasks execute at the right time even with system restarts (assuming Redis persistence is enabled).

**Rate Limiting with Sliding Windows:**

Sorted sets provide more accurate rate limiting than string-based counters by implementing true sliding windows.

```
ZADD ratelimit:user:1000 1700000000 "request:unique-id-1"
ZADD ratelimit:user:1000 1700000010 "request:unique-id-2"
ZADD ratelimit:user:1000 1700000020 "request:unique-id-3"
```

Each request adds an entry with the current timestamp as the score and a unique identifier as the member. To enforce a "100 requests per minute" limit:

```
current_time = 1700000030
window_start = current_time - 60
ZREMRANGEBYSCORE ratelimit:user:1000 -inf window_start
ZCARD ratelimit:user:1000
```

First, remove all requests older than 60 seconds. Then count remaining requests. If the count is under 100, allow the request and add it:

```
ZADD ratelimit:user:1000 current_time "request:unique-id-4"
```

If the count exceeds 100, reject the request. This provides a true sliding window—the window moves with each request, not in fixed minute blocks. For memory efficiency, you can use timestamps as both scores and members:

```
ZADD ratelimit:user:1000 1700000000 "1700000000"
```

This works because sorted sets require unique members, and timestamps are already unique per request (or nearly unique—you might add milliseconds for precision).

To auto-cleanup old data:

```
EXPIRE ratelimit:user:1000 120
```

The entire sorted set expires after 2 minutes of inactivity, automatically cleaning up data for inactive users.

**Time-Series Data Storage:**

Sorted sets naturally handle time-series data with timestamps as scores.

```
ZADD temperature:sensor:101 1700000000 "22.5"
ZADD temperature:sensor:101 1700000060 "22.7"
ZADD temperature:sensor:101 1700000120 "22.3"
ZADD temperature:sensor:101 1700000180 "22.8"
```

Each temperature reading is stored with its timestamp. Querying temperatures in a time range:

```
ZRANGEBYSCORE temperature:sensor:101 1700000000 1700000180 WITHSCORES
```

This retrieves all readings between these timestamps. Getting the most recent reading:

```
ZREVRANGE temperature:sensor:101 0 0 WITHSCORES
```

Calculating averages over time windows:

```
readings = ZRANGEBYSCORE temperature:sensor:101 start_time end_time WITHSCORES
```

Your application retrieves the readings and calculates statistics. For high-frequency data, combine multiple readings:

```
average_temp = calculate_average(readings_in_minute)
ZADD temperature:sensor:101:hourly 1700000000 average_temp
```

Store raw data at one granularity and aggregated data at coarser granularities, balancing detail with storage efficiency.

For retention policies:

```
cutoff_time = current_time - (30 * 24 * 3600)
ZREMRANGEBYSCORE temperature:sensor:101 -inf cutoff_time
```

This removes readings older than 30 days, implementing a rolling retention window.

**Auto-Complete and Search Suggestions:**

Sorted sets power auto-complete by storing completions with popularity scores.

```
ZADD autocomplete:search 1000 "redis"
ZADD autocomplete:search 850 "redis tutorial"
ZADD autocomplete:search 600 "redis commands"
ZADD autocomplete:search 500 "redis cache"
```

When a user types "redis":

```
ZREVRANGEBYLEX autocomplete:search [redis\xff [redis LIMIT 0 5
```

This gets suggestions starting with "redis" ordered by score. As users select suggestions:

```
ZINCRBY autocomplete:search 1 "redis tutorial"
```

The system learns which suggestions are most popular, creating a self-improving auto-complete. For prefix-based auto-complete with lexicographical ordering:

```
ZADD words 0 "apple" 0 "application" 0 "apply" 0 "approval" 0 "apricot"
ZRANGEBYLEX words [app [app\xff
```

This gets all words starting with "app". The `\xff` is a high Unicode character that ensures you get all strings with the prefix.

**Session Management with Activity Tracking:**

Sorted sets track user sessions with last activity times as scores.

```
ZADD sessions:active 1700000000 "session:abc123"
ZADD sessions:active 1700000100 "session:def456"
ZADD sessions:active 1700000200 "session:ghi789"
```

Each user action updates their session's score:

```
ZADD sessions:active 1700000250 "session:abc123"
```

Finding sessions idle for more than 30 minutes:

```
timeout_threshold = current_time - 1800
ZRANGEBYSCORE sessions:active -inf timeout_threshold
```

These sessions can be cleaned up or logged out. Removing expired sessions:

```
ZREMRANGEBYSCORE sessions:active -inf timeout_threshold
```

This pattern provides automatic session timeout detection with efficient query performance even with millions of active sessions.

**Stock Price or Cryptocurrency Price Tracking:**

Financial applications use sorted sets to maintain price histories.

```
ZADD prices:BTC:USD 1700000000 "50000.50"
ZADD prices:BTC:USD 1700000300 "50100.75"
ZADD prices:BTC:USD 1700000600 "49950.25"
```

Querying recent prices:

```
ZREVRANGE prices:BTC:USD 0 99 WITHSCORES
```

This gets the 100 most recent prices. Finding prices in a time window:

```
ZRANGEBYSCORE prices:BTC:USD 1700000000 1700000600 WITHSCORES
```

Detecting price changes above a threshold:

```
last_price = ZREVRANGE prices:BTC:USD 0 0 WITHSCORES
compare_price = ZREVRANGE prices:BTC:USD 1 1 WITHSCORES
```

Your application calculates the percentage change and triggers alerts if significant. For high-frequency trading, you might maintain multiple granularities—seconds, minutes, hours—each in separate sorted sets with appropriate retention policies.

**Geospatial Indexing (Before Redis Geo Commands):**

Before Redis introduced specialized geo commands, sorted sets were used for spatial indexing using geohashes.

```
ZADD locations:nearby 32.715738 "restaurant:1"
ZADD locations:nearby 32.715821 "restaurant:2"
```

The score is a geohash (a numeric encoding of latitude/longitude). Finding nearby locations:

```
ZRANGEBYSCORE locations:nearby 32.715 32.716
```

While Redis now has dedicated GEO commands (built on sorted sets), understanding this pattern shows sorted sets' versatility for custom indexing schemes.

---

## 5. Hashes: Structured Object Storage

### What Are Redis Hashes?

Redis Hashes are maps between string field names and string values, essentially representing objects or records with multiple attributes. Think of a hash as a miniature key-value store nested within a single Redis key. While you could store object attributes as separate Redis keys (like `user:1000:name`, `user:1000:email`, `user:1000:age`), hashes group related data together under one key, providing better organization, memory efficiency, and atomic field-level operations.

The internal implementation of hashes is optimized for memory. When hashes are small (typically under 512 fields or when all field values are under 64 bytes), Redis uses a compact encoding called ziplist that significantly reduces memory overhead. For larger hashes, Redis uses a hash table. This automatic optimization makes hashes extremely memory-efficient for storing millions of objects.

Hashes provide field-level granularity—you can update, retrieve, or delete individual fields without touching the entire object. This is more efficient than storing serialized objects (like JSON strings) where you'd need to deserialize, modify, and re-serialize for every change. Hashes strike a balance between structure and performance.

### Core Hash Operations and Commands

**Setting and Getting Individual Fields:**

```
HSET user:1000 name "Alice Johnson"
HSET user:1000 email "alice@example.com"
HSET user:1000 age "28"
HSET user:1000 city "New York"
HGET user:1000 name
HGET user:1000 email
```

HSET sets a single field in the hash. If the field already exists, it's updated; otherwise, it's created. HGET retrieves a specific field's value. This is much more efficient than storing each attribute as a separate Redis key because it groups related data and reduces Redis's overhead for managing separate keys.

**Setting Multiple Fields at Once:**

```
HMSET user:2000 name "Bob Smith" email "bob@example.com" age "35" city "Los Angeles"
HSET user:3000 name "Charlie Brown" email "charlie@example.com" age "42"
```

HMSET sets multiple fields in a single command, reducing network round trips. Note that in Redis 4.0+, HSET was extended to accept multiple field-value pairs, making HMSET somewhat redundant (though still supported for backward compatibility). The syntax alternates between field name and value.

**Getting Multiple Fields:**

```
HMGET user:1000 name email city
```

HMGET retrieves multiple field values at once, returning them in the same order as the requested fields. This is much more efficient than multiple HGET commands when you need several fields.

**Getting All Fields and Values:**

```
HGETALL user:1000
```

HGETALL retrieves all field-value pairs in the hash. The result alternates between field names and values. Be cautious with this command on large hashes—if a user object has hundreds of fields, HGETALL returns everything, which could be substantial data. For large hashes, use HMGET to retrieve only needed fields or HSCAN to iterate incrementally.

**Checking Field Existence:**

```
HEXISTS user:1000 email
HEXISTS user:1000 phone
```

HEXISTS checks if a field exists in the hash, returning 1 for exists or 0 for doesn't exist. This is useful for conditional logic, like "only send email if the user has an email field."

**Deleting Fields:**

```
HDEL user:1000 age
HDEL user:1000 city zip_code
```

HDEL removes one or more fields from the hash. The first command removes the age field. The second removes both city and zip_code in one operation. Fields that don't exist are silently ignored.

**Getting Field Names and Values Separately:**

```
HKEYS user:1000
HVALS user:1000
```

HKEYS returns all field names in the hash without their values—useful for discovering what attributes an object has. HVALS returns all values without field names—less commonly used but available for specific scenarios.

**Counting Fields:**

```
HLEN user:1000
```

HLEN returns the number of fields in the hash. This is O(1) since Redis maintains the count internally. Useful for checking how many attributes an object has or validating data completeness.

**Numeric Operations on Hash Fields:**

```
HSET product:500 price "29.99"
HSET product:500 stock "100"
HSET product:500 views "0"
HINCRBY product:500 stock -5
HINCRBY product:500 views 1
HINCRBYFLOAT product:500 price 5.50
```

HINCRBY atomically increments an integer field—here, reducing stock by 5 (using negative increment) and incrementing views by 1. HINCRBYFLOAT handles floating-point numbers, increasing the price by $5.50 to $35.49. These atomic operations are crucial for inventory management and metrics where concurrent updates are common. If the field doesn't exist, it's created with the increment value.

**Setting Fields Only If They Don't Exist:**

```
HSETNX user:1000 status "active"
HSETNX user:1000 status "inactive"
```

HSETNX sets a field only if it doesn't already exist. The first command succeeds, setting status to "active". The second fails because status now exists, returning 0. This is useful for setting defaults without overwriting existing values.

**Getting String Length of Field Value:**

```
HSTRLEN user:1000 name
```

HSTRLEN returns the string length of a field's value. For "Alice Johnson", it returns 13. Useful for validation or checking data sizes without retrieving the actual value.

### Advanced Hash Patterns

**Scanning Large Hashes:**

```
HSCAN user:1000 0 MATCH "pref_*" COUNT 100
```

HSCAN iterates through hash fields incrementally, similar to SSCAN and ZSCAN. The cursor starts at 0, and Redis returns a new cursor plus up to 100 fields matching the pattern. You continue with the returned cursor until it returns to 0. This prevents memory overload when dealing with hashes containing thousands of fields.

**Random Field Selection:**

```
HRANDFIELD user:1000 3
HRANDFIELD user:1000 2 WITHVALUES
```

HRANDFIELD (Redis 6.2+) returns random fields from the hash. The first command returns 3 random field names. The second returns 2 random field-value pairs with WITHVALUES. Useful for sampling or random feature selection.

### Production Use Cases for Hashes

**User Profile and Account Data Storage:**

Hashes are the ideal data structure for storing user profiles because they naturally represent objects with multiple attributes and provide efficient field-level access.

```
HSET user:5000 username "alice_johnson" email "alice@example.com" full_name "Alice Johnson" avatar_url "https://cdn.example.com/avatars/5000.jpg" bio "Software engineer passionate about databases" location "New York, NY" website "https://alicejohnson.dev" created_at "1700000000" last_login "1700050000" email_verified "true" newsletter_subscribed "true"
```

This stores a complete user profile in a single hash. Retrieving the profile for display:

```
HGETALL user:5000
```

This returns all profile data in one operation. When a user updates their bio:

```
HSET user:5000 bio "Software engineer and Redis enthusiast"
```

Only the bio field updates—no need to retrieve, deserialize, modify, and re-serialize the entire profile. Updating last login time:

```
HSET user:5000 last_login "1700060000"
```

This atomic update ensures accurate tracking even with concurrent requests. Checking if email is verified before sending:

```
HGET user:5000 email_verified
```

The field-level access means you retrieve only what you need, reducing bandwidth and processing. For privacy features, selectively retrieve public fields:

```
HMGET user:5000 username full_name avatar_url bio location website
```

This excludes private fields like email. For user settings stored as preferences:

```
HSET user:5000 pref_theme "dark" pref_language "en" pref_timezone "America/New_York" pref_notifications_email "true" pref_notifications_push "false"
```

Retrieving all preferences:

```
HSCAN user:5000 0 MATCH "pref_*"
```

This pattern scales to millions of users. Compared to storing each field as a separate Redis key (`user:5000:username`, `user:5000:email`, etc.), hashes reduce memory overhead by 30-40% and simplify management. Compared to storing the entire profile as a JSON string (`SET user:5000 '{"username":"alice_johnson",...}'`), hashes enable granular updates without serialization overhead.

**Shopping Cart Implementation:**

E-commerce shopping carts map perfectly to hashes where field names are product IDs and values are quantities.

```
HSET cart:user:1000 product:500 "2"
HSET cart:user:1000 product:501 "1"
HSET cart:user:1000 product:502 "3"
```

User 1000 has three products in their cart with specified quantities. Adding more of product 500:

```
HINCRBY cart:user:1000 product:500 1
```

The quantity atomically increases to 3. This handles concurrent adds if a user clicks "add to cart" from multiple devices simultaneously. Removing product 501 from the cart:

```
HDEL cart:user:1000 product:501
```

Getting the entire cart for checkout:

```
HGETALL cart:user:1000
```

This returns all products and quantities. Your application then fetches product details and prices from a database or cache to display the cart. Checking if a specific product is in the cart:

```
HEXISTS cart:user:1000 product:500
```

Getting the total number of different products:

```
HLEN cart:user:1000
```

This returns 2 (after removing product 501). Implementing cart expiration to clear abandoned carts:

```
EXPIRE cart:user:1000 604800
```

The cart expires after 7 days of inactivity. Each time the user modifies their cart, refresh the expiration:

```
HSET cart:user:1000 product:503 "1"
EXPIRE cart:user:1000 604800
```

This pattern handles millions of active carts efficiently. For persistent carts that survive logout:

```
PERSIST cart:user:1000
```

This removes the expiration, keeping the cart indefinitely. For guest carts (before login):

```
HSET cart:guest:abc123def456 product:500 "1"
```

When the guest logs in, merge their guest cart with their user cart:

```
guest_cart = HGETALL cart:guest:abc123def456
for product_id, quantity in guest_cart:
    HINCRBY cart:user:1000 product_id quantity
DEL cart:guest:abc123def456
```

**Product Catalog and Inventory Management:**

Hashes efficiently store product information with multiple attributes.

```
HSET product:12345 name "Wireless Bluetooth Headphones" brand "AudioTech" category "Electronics" subcategory "Audio" price "79.99" original_price "99.99" currency "USD" stock_quantity "150" sku "AT-WBH-001" weight_kg "0.25" color "Black" rating "4.5" review_count "287" description "Premium wireless headphones with noise cancellation" is_featured "true" is_available "true"
```

A complete product record in one hash. Displaying product details:

```
HGETALL product:12345
```

This retrieves everything for the product page. Updating stock after a purchase:

```
HINCRBY product:12345 stock_quantity -1
```

This atomically decrements stock, preventing overselling in high-concurrency situations. Checking stock availability before purchase:

```
stock = HGET product:12345 stock_quantity
if int(stock) > 0:
    # Process order
```

Updating product price during a sale:

```
HSET product:12345 price "59.99"
```

Just the price updates—other attributes remain unchanged. For product variations (colors, sizes):

```
HSET product:12345:variant:black_small sku "AT-WBH-001-BLK-S" color "Black" size "Small" stock_quantity "30" price "79.99"
HSET product:12345:variant:black_large sku "AT-WBH-001-BLK-L" color "Black" size "Large" stock_quantity "45" price "79.99"
```

Each variant has its own hash with specific attributes. For warehouse-specific inventory:

```
HSET inventory:warehouse:west product:12345 "75"
HSET inventory:warehouse:east product:12345 "75"
```

This tracks stock across multiple locations. For product metadata that changes frequently:

```
HSET product:12345:metrics views_today "1523" purchases_today "28" cart_adds_today "156" last_purchased_at "1700060000"
HINCRBY product:12345:metrics views_today 1
```

This separates volatile metrics from stable product data, enabling efficient updates without touching the main product hash.

**Server and Application Configuration:**

Hashes provide an excellent way to manage configuration that needs runtime updates without application restarts.

```
HSET config:application max_upload_size_mb "50" session_timeout_seconds "3600" api_rate_limit_per_minute "1000" cache_ttl_seconds "300" maintenance_mode "false" feature_new_ui "true" feature_advanced_search "false" database_connection_pool_size "20" log_level "info" admin_email "admin@example.com"
```

Application servers periodically read configuration:

```
config = HGETALL config:application
```

Updating configuration in real-time:

```
HSET config:application feature_new_ui "false"
```

All servers pick up the change within their next configuration refresh cycle (typically seconds). Enabling maintenance mode:

```
HSET config:application maintenance_mode "true"
```

Servers check this before processing requests:

```
maintenance = HGET config:application maintenance_mode
if maintenance == "true":
    return "Site under maintenance"
```

For service-specific configuration:

```
HSET config:email_service smtp_host "smtp.example.com" smtp_port "587" max_retry_attempts "3" timeout_seconds "30"
HSET config:payment_service api_key "encrypted_key_here" webhook_url "https://example.com/webhooks/payment" currency "USD"
```

Each service has its own configuration hash. For environment-specific config:

```
HSET config:production:database host "prod-db.example.com" port "5432" max_connections "100"
HSET config:staging:database host "staging-db.example.com" port "5432" max_connections "20"
```

Applications load the appropriate environment's configuration. This pattern enables feature flags, A/B tests, and operational settings to be changed instantly across distributed systems.

**Session Storage with Complex State:**

Web application sessions often contain multiple pieces of information that hashes organize naturally.

```
HSET session:abc123def456 user_id "1000" username "alice_johnson" role "admin" login_timestamp "1700000000" last_activity "1700050000" ip_address "192.168.1.100" user_agent "Mozilla/5.0..." csrf_token "random_token_here" shopping_cart_id "cart:user:1000" preferences_loaded "true" two_factor_verified "true"
```

Each session stores comprehensive state. Checking session validity:

```
last_activity = HGET session:abc123def456 last_activity
current_time = get_current_timestamp()
if current_time - int(last_activity) > 3600:
    # Session expired
    DEL session:abc123def456
```

Updating last activity on each request:

```
HSET session:abc123def456 last_activity "1700051000"
```

Checking user permissions:

```
role = HGET session:abc123def456 role
if role == "admin":
    # Allow admin action
```

Retrieving session for request processing:

```
session_data = HGETALL session:abc123def456
```

This gets all session information in one operation. For multi-step forms or wizards storing intermediate state:

```
HSET session:abc123def456 form_step "3" form_data_step1 '{"name":"Alice","email":"..."}' form_data_step2 '{"address":"...","city":"..."}' form_data_step3 '{"payment":"..."}'
```

Each form step saves its data to the session hash. When the form completes, you have all data available. Setting session expiration:

```
EXPIRE session:abc123def456 86400
```

The session expires after 24 hours. Refreshing expiration on activity:

```
HSET session:abc123def456 last_activity "1700052000"
EXPIRE session:abc123def456 86400
```

**Real-Time Analytics and Metrics:**

Hashes efficiently store metrics and counters with multiple dimensions.

```
HSET metrics:api:2024-11-23 total_requests "150423" successful_requests "148901" failed_requests "1522" avg_response_time_ms "45.3" max_response_time_ms "2301" total_bytes_sent "52419840" unique_users "15234"
```

Hourly metrics for detailed analysis:

```
HSET metrics:api:2024-11-23:14 total_requests "6234" successful_requests "6180" failed_requests "54" avg_response_time_ms "42.1"
```

Incrementing counters as requests occur:

```
HINCRBY metrics:api:2024-11-23 total_requests 1
HINCRBY metrics:api:2024-11-23 successful_requests 1
```

Updating average response time (using additional calculations):

```
current_avg = HGET metrics:api:2024-11-23 avg_response_time_ms
current_count = HGET metrics:api:2024-11-23 total_requests
new_avg = calculate_new_average(current_avg, current_count, new_response_time)
HSET metrics:api:2024-11-23 avg_response_time_ms new_avg
```

Per-endpoint metrics:

```
HSET metrics:endpoint:/api/users:2024-11-23 requests "23456" avg_response_time "23.4" errors "45"
HSET metrics:endpoint:/api/products:2024-11-23 requests "45678" avg_response_time "67.8" errors "123"
```

Retrieving daily metrics for dashboards:

```
metrics = HGETALL metrics:api:2024-11-23
```

This provides all daily statistics in one call. For error tracking by type:

```
HSET errors:2024-11-23 http_500 "45" http_503 "12" http_400 "234" timeout_errors "23" database_errors "18"
HINCRBY errors:2024-11-23 http_500 1
```

This pattern enables real-time dashboards and monitoring. With automatic expiration:

```
EXPIRE metrics:api:2024-11-23 2592000
```

Metrics older than 30 days automatically clean up.

**Feature Flags Per User or Organization:**

Granular feature flag management uses hashes to store flag states per entity.

```
HSET features:user:1000 dark_mode "true" beta_features "true" advanced_analytics "false" new_dashboard "true" experimental_ai "false"
```

Checking if a user has a feature enabled:

```
has_beta = HGET features:user:1000 beta_features
if has_beta == "true":
    # Show beta features
```

Enabling a feature for a user:

```
HSET features:user:1000 experimental_ai "true"
```

For organization-level features:

```
HSET features:org:500 api_access "true" advanced_reports "true" custom_branding "true" max_users "100" max_storage_gb "500"
```

Combining user and organization features:

```
user_features = HGETALL features:user:1000
org_features = HGETALL features:org:500
merged_features = merge(user_features, org_features)
```

For gradual rollouts with percentages stored as metadata:

```
HSET features:global:new_checkout enabled "true" rollout_percentage "25" eligible_roles "premium,enterprise"
```

Your application checks both the flag and eligibility criteria. For A/B testing assignments:

```
HSET ab_tests:user:1000 homepage_layout "variant_b" checkout_flow "control" email_template "variant_a"
```

This ensures users see consistent variants across sessions. Bulk enabling features:

```
HSET features:user:1000 dark_mode "true" new_dashboard "true" advanced_analytics "true"
```

Multiple features toggle in one operation.

**Caching Complex Objects:**

While strings can store serialized JSON, hashes offer advantages for structured data that's frequently updated partially.

```
HSET cache:article:5000 title "Understanding Redis Data Structures" author "Alice Johnson" author_id "1000" published_at "1700000000" view_count "1523" like_count "234" comment_count "45" category "Technology" tags "redis,database,caching" content "Full article content here..." last_updated "1700050000"
```

Updating view count without touching other fields:

```
HINCRBY cache:article:5000 view_count 1
```

This is more efficient than retrieving JSON, parsing it, incrementing, serializing, and storing back. Updating multiple metrics:

```
HINCRBY cache:article:5000 view_count 1
HINCRBY cache:article:5000 like_count 1
HSET cache:article:5000 last_updated "1700051000"
```

For cache invalidation:

```
EXPIRE cache:article:5000 3600
```

Or selective field updates when source data changes:

```
HSET cache:article:5000 title "Understanding Redis Data Structures (Updated)"
HSET cache:article:5000 last_updated "1700052000"
```

This hybrid approach combines caching benefits with structured field access, avoiding full serialization overhead for partial updates.

---

## 6. Bitmaps: Compact Binary Data Storage

### What Are Redis Bitmaps?

Redis Bitmaps are not a separate data type but rather a set of bit-oriented operations on strings. A bitmap treats a string as an array of bits, where each bit can be 0 or 1. This enables extremely memory-efficient storage for binary data, flags, or any scenario where you're tracking boolean states for many items.

The power of bitmaps lies in their space efficiency. Instead of using strings or sets to track states, where each entry might consume dozens of bytes, bitmaps use a single bit per item. For example, tracking whether 100 million users logged in today would require approximately 12 MB with a bitmap, versus potentially gigabytes with other data structures.

Bitmaps support operations at the bit level: setting bits, getting bits, counting set bits, and bitwise operations (AND, OR, XOR, NOT). These operations are extremely fast and enable sophisticated analytics on large datasets with minimal memory usage.

### Core Bitmap Operations and Commands

**Setting and Getting Individual Bits:**

```
SETBIT user:1000:daily_login:2024-11-23 0 1
SETBIT user:1000:daily_login:2024-11-23 1 1
SETBIT user:1000:daily_login:2024-11-23 2 0
GETBIT user:1000:daily_login:2024-11-23 0
GETBIT user:1000:daily_login:2024-11-23 2
```

SETBIT sets a bit at a specific offset (position) to 1 or 0. The offset is zero-indexed. The first command sets bit 0 to 1, the second sets bit 1 to 1, the third sets bit 2 to 0. GETBIT retrieves the bit value at an offset, returning 1 or 0. In this example, we might be tracking which hours of the day a user was active (bit 0 = midnight, bit 1 = 1 AM, etc.).

**Counting Set Bits:**

```
BITCOUNT user:1000:daily_login:2024-11-23
BITCOUNT user:1000:daily_login:2024-11-23 0 2
```

BITCOUNT returns the number of bits set to 1 in the entire bitmap or a specified byte range. The first command counts all set bits. The second counts set bits in bytes 0 through 2 (note: the range is in bytes, not bits). If bits 0, 1, 4, 5, 6 are set to 1, BITCOUNT returns 5.

**Finding First Set or Unset Bit:**

```
BITPOS user:1000:daily_login:2024-11-23 1
BITPOS user:1000:daily_login:2024-11-23 0
BITPOS user:1000:daily_login:2024-11-23 1 2 5
```

BITPOS returns the position of the first bit set to the specified value (1 or 0). The first command finds the first bit set to 1. The second finds the first bit set to 0. The third searches only in bytes 2 through 5. This is useful for finding available slots or the first occurrence of a state.

**Bitwise Operations Between Bitmaps:**

```
SETBIT bitmap:a 0 1
SETBIT bitmap:a 1 1
SETBIT bitmap:a 2 0
SETBIT bitmap:b 0 1
SETBIT bitmap:b 1 0
SETBIT bitmap:b 2 1
BITOP AND result:and bitmap:a bitmap:b
BITOP OR result:or bitmap:a bitmap:b
BITOP XOR result:xor bitmap:a bitmap:b
BITOP NOT result:not bitmap:a
```

BITOP performs bitwise operations between one or more bitmaps, storing the result in a destination key. AND performs logical AND (result bit is 1 only if both source bits are 1). OR performs logical OR (result bit is 1 if either source bit is 1). XOR performs exclusive OR (result bit is 1 if exactly one source bit is 1). NOT performs bitwise NOT (inverts all bits). These operations enable complex set-like queries on bitmap data.

**Getting Bitmap Length:**

```
STRLEN user:1000:daily_login:2024-11-23
```

STRLEN returns the length of the string in bytes. Since bitmaps are strings, this tells you how much memory the bitmap occupies. A bitmap with 1000 bits occupies 125 bytes (1000 bits / 8 bits per byte).

### Advanced Bitmap Patterns

**Efficient Boolean Arrays:**

Bitmaps excel at representing large arrays of boolean values:

```
SETBIT features:enabled 0 1
SETBIT features:enabled 1 0
SETBIT features:enabled 2 1
```

Each bit position represents a feature ID, and the bit value represents enabled (1) or disabled (0). For 10,000 features, this uses approximately 1.25 KB instead of potentially hundreds of kilobytes with other structures.

**Sparse Bitmaps:**

When setting a bit at a high offset, Redis automatically extends the bitmap:

```
SETBIT sparse:bitmap 1000000 1
```

This creates a bitmap large enough to hold bit position 1,000,000 (approximately 125 KB). The intermediate bits are all 0. While this creates a potentially large string, it's still more memory-efficient than alternatives for sparse data where most values are 0.

### Production Use Cases for Bitmaps

**Daily Active User (DAU) Tracking:**

One of the most common bitmap use cases is tracking daily active users with extreme memory efficiency.

```
SETBIT users:active:2024-11-23 1000 1
SETBIT users:active:2024-11-23 1001 1
SETBIT users:active:2024-11-23 1002 1
```

When user 1000 logs in on November 23rd, you set bit 1000 to 1. The bit offset corresponds to the user ID. Counting daily active users:

```
BITCOUNT users:active:2024-11-23
```

This instantly returns the count. For 100 million users, the bitmap consumes only about 12 MB of memory. Finding users active on multiple specific days:

```
BITOP AND users:active:both users:active:2024-11-23 users:active:2024-11-24
BITCOUNT users:active:both
```

This finds users who were active on both days. Finding users active on any of several days:

```
BITOP OR users:active:week users:active:2024-11-23 users:active:2024-11-24 users:active:2024-11-25 users:active:2024-11-26 users:active:2024-11-27 users:active:2024-11-28 users:active:2024-11-29
BITCOUNT users:active:week
```

This calculates weekly active users (WAU). Finding users active last week but not this week (churn):

```
BITOP AND users:active:lastweek users:active:2024-11-17 users:active:2024-11-18...
BITOP AND users:active:thisweek users:active:2024-11-24 users:active:2024-11-25...
BITOP ANDNOT users:churned users:active:lastweek users:active:thisweek
BITCOUNT users:churned
```

This identifies churned users. For retention cohorts, track users who signed up on a specific day:

```
SETBIT users:signup:2024-11-01 1000 1
```

Then see how many returned on later days:

```
BITOP AND users:retention:day7 users:signup:2024-11-01 users:active:2024-11-08
retention_count = BITCOUNT users:retention:day7
signup_count = BITCOUNT users:signup:2024-11-01
retention_rate = retention_count / signup_count
```

This calculates 7-day retention. With automatic expiration:

```
EXPIRE users:active:2024-11-23 2592000
```

Old daily bitmaps clean up after 30 days.

**Feature Usage Tracking:**

Track which users have used specific features to identify engaged users and power users.

```
SETBIT feature:search:used 1000 1
SETBIT feature:export:used 1000 1
SETBIT feature:advanced_filters:used 1000 1
```

When user 1000 uses a feature for the first time, set their bit. Finding users who used multiple features:

```
BITOP AND users:power_users feature:search:used feature:export:used feature:advanced_filters:used feature:api:used
BITCOUNT users:power_users
```

This identifies power users who've used all four features. Finding users who haven't used a feature:

```
BITOP NOT users:never_searched feature:search:used
BITOP AND users:active_never_searched users:active:recent users:never_searched
```

This finds active users who've never used search—a target group for an onboarding campaign. Tracking feature adoption over time:

```
adoption_rate = BITCOUNT feature:search:used / total_users
```

**Real-Time Analytics for Events:**

Track whether users completed specific events or actions within time windows.

```
SETBIT event:completed_tutorial:2024-11 1000 1
SETBIT event:made_purchase:2024-11 1000 1
SETBIT event:invited_friend:2024-11 1000 1
```

Finding users who completed the tutorial but didn't make a purchase:

```
BITOP ANDNOT users:tutorial_no_purchase event:completed_tutorial:2024-11 event:made_purchase:2024-11
```

These users might benefit from a promotional offer. Finding users who completed all onboarding steps:

```
BITOP AND users:fully_onboarded event:completed_tutorial:2024-11 event:added_profile_pic:2024-11 event:connected_account:2024-11
```

**IP Address Blocking and Allowlisting:**

Efficiently track blocked or allowed IP addresses using bitmaps.

```
# Convert IP 192.168.1.100 to integer: (192 * 256^3) + (168 * 256^2) + (1 * 256) + 100 = 3232235876
SETBIT ips:blocked 3232235876 1
```

Each IP address converts to a 32-bit integer used as the bit offset. Checking if an IP is blocked:

```
GETBIT ips:blocked 3232235876
```

This returns 1 if blocked, 0 if allowed. For IPv4, the bitmap would theoretically need 4 billion bits (approximately 500 MB) to cover all addresses, but in practice, you block a tiny fraction, making this extremely memory-efficient. For rate limiting by IP:

```
SETBIT ratelimit:ip:2024-11-23:14 3232235876 1
```

Track which IPs made requests in a specific hour. Exceeded rate limit check requires counting requests (more complex, better suited to other structures), but bitmap can track "has made a request this hour" efficiently.

**Permissions and Access Control at Scale:**

Track permissions for millions of users across thousands of resources.

```
SETBIT permissions:read:document:5000 1000 1
SETBIT permissions:write:document:5000 1000 1
SETBIT permissions:delete:document:5000 1001 0
```

User 1000 has read and write permissions on document 5000. Checking permissions:

```
GETBIT permissions:write:document:5000 1000
```

Finding all users with write permission on a document:

```
BITCOUNT permissions:write:document:5000
```

Finding users with both read AND write permissions:

```
BITOP AND permissions:readwrite:document:5000 permissions:read:document:5000 permissions:write:document:5000
```

For resources requiring multiple permissions, this enables fast permission checks. Revoking all permissions for a user across resources requires more complex orchestration, but bitmaps provide the underlying efficiency.

**Attendance and Check-in Systems:**

Track daily attendance for employees, students, or members.

```
SETBIT attendance:employee:1000:2024-11 23 1
SETBIT attendance:employee:1000:2024-11 24 1
SETBIT attendance:employee:1000:2024-11 25 0
```

Each bit represents a day of the month. Bit 23 set to 1 means the employee was present on November 23rd. Counting days present in November:

```
BITCOUNT attendance:employee:1000:2024-11
```

Finding perfect attendance (all days present):

```
BITCOUNT attendance:employee:1000:2024-11
```

If this equals the number of workdays, the employee had perfect attendance. Finding employees present on a specific day:

```
# Create a bitmap for the day across all employees
SETBIT attendance:all:2024-11-23 1000 1
SETBIT attendance:all:2024-11-23 1001 1
BITCOUNT attendance:all:2024-11-23
```

This counts how many employees were present on November 23rd.

**A/B Test Participation Tracking:**

Track which users have been exposed to different experiment variants.

```
SETBIT experiment:new_ui:variant_a 1000 1
SETBIT experiment:new_ui:variant_b 1001 1
SETBIT experiment:new_ui:control 1002 1
```

Users are assigned to variants, and their participation is marked. Finding overlap between experiments:

```
BITOP AND users:both_experiments experiment:new_ui:variant_a experiment:new_pricing:variant_b
```

This finds users in variant A of the UI test AND variant B of the pricing test, helping analyze interaction effects. Counting users per variant:

```
BITCOUNT experiment:new_ui:variant_a
```

Ensuring mutually exclusive assignment:

```
BITOP OR experiment:new_ui:assigned experiment:new_ui:variant_a experiment:new_ui:variant_b experiment:new_ui:control
BITCOUNT experiment:new_ui:assigned
```

The count should equal the sum of individual variant counts if assignment is exclusive.

**Site-wide Feature Availability:**

Track which features are enabled globally or for specific tiers/plans.

```
SETBIT plan:free:features 0 1      # Basic search
SETBIT plan:free:features 1 1      # Profile
SETBIT plan:free:features 2 0      # Advanced analytics
SETBIT plan:premium:features 0 1    # Basic search
SETBIT plan:premium:features 1 1    # Profile  
SETBIT plan:premium:features 2 1    # Advanced analytics
SETBIT plan:premium:features 3 1    # API access
```

Each bit represents a feature ID. Checking if a plan includes a feature:

```
GETBIT plan:premium:features 2
```

Finding features available in premium but not free:

```
BITOP ANDNOT features:premium_only plan:premium:features plan:free:features
```

This identifies upsell opportunities. Comparing plans:

```
BITOP XOR features:difference plan:premium:features plan:enterprise:features
```

This shows features that differ between plans.

---

## 7. HyperLogLog: Probabilistic Cardinality Estimation

### What Are Redis HyperLogLogs?

HyperLogLog (HLL) is a probabilistic data structure used for estimating the cardinality (count of unique elements) in a set. Unlike traditional sets that store every element, HyperLogLog uses a fixed amount of memory (approximately 12 KB per HLL) to estimate the count of unique items with a standard error of 0.81%, regardless of how many items you've added.

The magic of HyperLogLog lies in its memory efficiency. Whether you're counting 100 unique items or 100 billion unique items, the memory usage remains constant at about 12 KB. This makes HyperLogLog perfect for scenarios where you need approximate counts of unique items but can't afford the memory overhead of storing every item (as you would with sets).

The trade-off is accuracy. HyperLogLog provides an estimate, not an exact count. For most analytics use cases, the 0.81% error rate is perfectly acceptable. For example, if you have 1 million unique visitors, HyperLogLog might report 998,100 or 1,001,900—close enough for dashboards, reporting, and decision-making.

Redis implements the HyperLogLog algorithm efficiently, providing commands to add elements, count unique elements, and merge multiple HyperLogLogs. The data structure uses probabilistic counting based on hashing and bit patterns to achieve its remarkable space efficiency.

### Core HyperLogLog Operations and Commands

**Adding Elements:**

```
PFADD unique_visitors:2024-11-23 "user:1000"
PFADD unique_visitors:2024-11-23 "user:1001"
PFADD unique_visitors:2024-11-23 "user:1002"
PFADD unique_visitors:2024-11-23 "user:1000"
```

PFADD adds elements to a HyperLogLog. The name "PF" comes from Philippe Flajolet, one of the inventors of the algorithm. You can add multiple elements at once:

```
PFADD unique_visitors:2024-11-23 "user:1003" "user:1004" "user:1005"
```

Notice that adding "user:1000" twice doesn't affect the count—HyperLogLog automatically handles duplicates. PFADD returns 1 if the internal representation was modified, 0 if it wasn't (meaning the element was likely already counted).

**Counting Unique Elements:**

```
PFCOUNT unique_visitors:2024-11-23
```

PFCOUNT returns the estimated count of unique elements added to the HyperLogLog. This is an approximation with a standard error of 0.81%. If you've added 1,000 unique users, PFCOUNT might return something between 992 and 1,008—close enough for most use cases.

**Counting Across Multiple HyperLogLogs:**

```
PFCOUNT unique_visitors:2024-11-23 unique_visitors:2024-11-24 unique_visitors:2024-11-25
```

PFCOUNT can accept multiple HyperLogLog keys and return the estimated count of unique elements across all of them. This internally merges the HyperLogLogs temporarily to compute the union count. This is perfect for calculating metrics like "unique visitors this week" by combining daily HyperLogLogs.

**Merging HyperLogLogs:**

```
PFMERGE unique_visitors:week_48 unique_visitors:2024-11-23 unique_visitors:2024-11-24 unique_visitors:2024-11-25 unique_visitors:2024-11-26 unique_visitors:2024-11-27 unique_visitors:2024-11-28 unique_visitors:2024-11-29
PFCOUNT unique_visitors:week_48
```

PFMERGE combines multiple HyperLogLogs into a destination HyperLogLog. This is useful for aggregating data across time periods or data sources. After merging, you can query the weekly aggregated HyperLogLog without needing to merge the daily ones each time.

### Production Use Cases for HyperLogLogs

**Unique Visitor Tracking at Scale:**

For high-traffic websites with millions of daily visitors, tracking unique visitors with traditional sets would consume enormous memory. HyperLogLog provides near-exact counts with minimal memory.

```
PFADD analytics:unique_visitors:2024-11-23 "ip:192.168.1.100"
PFADD analytics:unique_visitors:2024-11-23 "ip:192.168.1.101"
PFADD analytics:unique_visitors:2024-11-23 "ip:192.168.1.102"
```

Each visitor's IP address (or user ID, or cookie ID) is added to the day's HyperLogLog. Getting the daily unique visitor count:

```
PFCOUNT analytics:unique_visitors:2024-11-23
```

This returns an estimate in constant time, regardless of whether you had 1,000 or 100 million visitors. For weekly unique visitors:

```
PFCOUNT analytics:unique_visitors:2024-11-23 analytics:unique_visitors:2024-11-24 analytics:unique_visitors:2024-11-25 analytics:unique_visitors:2024-11-26 analytics:unique_visitors:2024-11-27 analytics:unique_visitors:2024-11-28 analytics:unique_visitors:2024-11-29
```

This combines seven daily HyperLogLogs to estimate weekly unique visitors. For monthly aggregation, create a merged HyperLogLog:

```
PFMERGE analytics:unique_visitors:2024-11 analytics:unique_visitors:2024-11-01 analytics:unique_visitors:2024-11-02 ... analytics:unique_visitors:2024-11-30
PFCOUNT analytics:unique_visitors:2024-11
```

Comparing this approach to using sets: for 10 million unique daily visitors, a set would consume approximately 200-300 MB per day (assuming 20-30 bytes per entry), totaling 6-9 GB per month. HyperLogLog uses 12 KB per day, totaling approximately 360 KB per month—a reduction of over 99.99%.

With automatic expiration for old data:

```
EXPIRE analytics:unique_visitors:2024-11-23 7776000
```

This keeps daily HyperLogLogs for 90 days, after which they automatically delete.

**Unique Page Views Per Article:**

Content platforms track unique viewers per article to understand engagement without storing every visitor.

```
PFADD article:12345:unique_views "user:1000"
PFADD article:12345:unique_views "user:1001"
PFADD article:12345:unique_views "user:1000"
```

When a user views an article, add them to the article's HyperLogLog. Getting unique view count:

```
PFCOUNT article:12345:unique_views
```

For millions of articles, storing unique viewers in sets would be prohibitive. With HyperLogLog, each article uses only 12 KB regardless of popularity. Finding the most uniquely viewed articles:

```
article_ids = get_all_article_ids()
for article_id in article_ids:
    count = PFCOUNT f"article:{article_id}:unique_views"
    store_in_sorted_set(article_id, count)
```

You periodically update a sorted set with unique view counts for ranking. For time-windowed unique views:

```
PFADD article:12345:unique_views:2024-11 "user:1000"
```

Monthly HyperLogLogs allow you to track how unique viewership changes over time without maintaining historical visitor lists.

**Search Query Diversity Analysis:**

Understanding how many unique search queries users perform helps gauge content coverage and search effectiveness.

```
PFADD search:unique_queries:2024-11-23 "redis tutorial"
PFADD search:unique_queries:2024-11-23 "redis data structures"
PFADD search:unique_queries:2024-11-23 "how to use redis"
```

Each search query is added to the daily HyperLogLog. Counting unique queries:

```
PFCOUNT search:unique_queries:2024-11-23
```

This reveals search diversity—are users performing thousands of different searches (high diversity) or repeatedly searching for the same things (low diversity)? For category-specific analysis:

```
PFADD search:unique_queries:category:electronics:2024-11-23 "laptop deals"
PFADD search:unique_queries:category:books:2024-11-23 "fiction bestsellers"
```

Comparing search diversity across categories:

```
electronics_diversity = PFCOUNT search:unique_queries:category:electronics:2024-11-23
books_diversity = PFCOUNT search:unique_queries:category:books:2024-11-23
```

High diversity might indicate good category navigation, while low diversity might suggest users struggle to find items.

**API Rate Limiting by Unique Users:**

Some rate limiting scenarios care about unique users rather than total requests. HyperLogLog efficiently tracks unique API consumers.

```
PFADD api:unique_users:2024-11-23:14 "api_key:abc123"
PFADD api:unique_users:2024-11-23:14 "api_key:def456"
```

Each API request adds the API key to the hourly HyperLogLog. Monitoring unique users per hour:

```
PFCOUNT api:unique_users:2024-11-23:14
```

If unique user count suddenly spikes, it might indicate a DDoS attack or viral event. For daily active API consumers:

```
PFMERGE api:unique_users:2024-11-23 api:unique_users:2024-11-23:00 api:unique_users:2024-11-23:01 ... api:unique_users:2024-11-23:23
PFCOUNT api:unique_users:2024-11-23
```

This aggregates hourly data into daily metrics.

**E-commerce: Unique Product Views:**

Track how many unique users view each product without storing user lists.

```
PFADD product:5000:unique_viewers "user:1000"
PFADD product:5000:unique_viewers "session:abc123"
```

Using either user IDs (for logged-in users) or session IDs (for guests), you track unique viewers. Getting view count:

```
PFCOUNT product:5000:unique_viewers
```

This metric helps identify popular products and potential inventory needs. For conversion analysis, compare unique viewers to purchasers:

```
unique_viewers = PFCOUNT product:5000:unique_viewers
total_purchases = GET product:5000:purchase_count
conversion_rate = total_purchases / unique_viewers
```

For time-based trending:

```
PFADD product:5000:unique_viewers:2024-11-23 "user:1000"
today_views = PFCOUNT product:5000:unique_viewers:2024-11-23
yesterday_views = PFCOUNT product:5000:unique_viewers:2024-11-22
growth_rate = (today_views - yesterday_views) / yesterday_views
```

**Social Media: Unique Post Reach:**

Social platforms track how many unique users saw each post without maintaining view lists.

```
PFADD post:98765:reach "user:1000"
PFADD post:98765:reach "user:1001"
PFADD post:98765:reach "user:1002"
```

When a post appears in a user's feed, add them to the post's reach HyperLogLog. Calculating reach:

```
PFCOUNT post:98765:reach
```

For viral posts with millions of views, HyperLogLog uses only 12 KB versus potentially hundreds of megabytes for sets. Comparing organic versus promoted reach:

```
PFADD post:98765:reach:organic "user:1000"
PFADD post:98765:reach:promoted "user:1001"
organic_reach = PFCOUNT post:98765:reach:organic
promoted_reach = PFCOUNT post:98765:reach:promoted
total_reach = PFCOUNT post:98765:reach:organic post:98765:reach:promoted
```

The total reach accounts for users who saw the post through both channels (union calculation).

**Gaming: Unique Daily/Monthly Active Players:**

Game platforms track DAU (Daily Active Users) and MAU (Monthly Active Users) efficiently.

```
PFADD game:500:dau:2024-11-23 "player:12345"
PFADD game:500:dau:2024-11-23 "player:67890"
```

Each player login adds them to the daily HyperLogLog. Daily active count:

```
PFCOUNT game:500:dau:2024-11-23
```

Monthly active users:

```
PFMERGE game:500:mau:2024-11 game:500:dau:2024-11-01 game:500:dau:2024-11-02 ... game:500:dau:2024-11-30
PFCOUNT game:500:mau:2024-11
```

Or calculate without pre-merging:

```
PFCOUNT game:500:dau:2024-11-01 game:500:dau:2024-11-02 ... game:500:dau:2024-11-30
```

For stickiness metrics (DAU/MAU ratio):

```
dau = PFCOUNT game:500:dau:2024-11-23
mau = PFCOUNT game:500:mau:2024-11
stickiness = dau / mau
```

Higher stickiness indicates players engage frequently. For feature-specific engagement:

```
PFADD game:500:feature:multiplayer:users:2024-11-23 "player:12345"
multiplayer_users = PFCOUNT game:500:feature:multiplayer:users:2024-11-23
total_users = PFCOUNT game:500:dau:2024-11-23
adoption_rate = multiplayer_users / total_users
```

**IoT and Sensor Networks: Unique Device Tracking:**

IoT platforms track unique devices reporting data without storing device lists.

```
PFADD sensors:active:2024-11-23 "device:sensor-001"
PFADD sensors:active:2024-11-23 "device:sensor-002"
PFADD sensors:active:2024-11-23 "device:sensor-003"
```

Each sensor report adds the device to the daily HyperLogLog. Active device count:

```
PFCOUNT sensors:active:2024-11-23
```

For networks with millions of sensors, this provides instant counts. Detecting network issues:

```
expected_devices = 1000000
actual_devices = PFCOUNT sensors:active:2024-11-23
if actual_devices < expected_devices * 0.95:
    alert("Significant device dropout detected")
```

For geographic distribution:

```
PFADD sensors:active:region:north:2024-11-23 "device:sensor-001"
PFADD sensors:active:region:south:2024-11-23 "device:sensor-002"
north_count = PFCOUNT sensors:active:region:north:2024-11-23
south_count = PFCOUNT sensors:active:region:south:2024-11-23
```

**Email Campaign: Unique Opens and Clicks:**

Marketing platforms track unique opens and clicks for email campaigns.

```
PFADD campaign:5000:unique_opens "user:1000"
PFADD campaign:5000:unique_opens "user:1001"
PFADD campaign:5000:unique_clicks "user:1000"
```

When a user opens an email, add them to the opens HyperLogLog. When they click, add to clicks. Calculating metrics:

```
unique_opens = PFCOUNT campaign:5000:unique_opens
unique_clicks = PFCOUNT campaign:5000:unique_clicks
click_through_rate = unique_clicks / unique_opens
```

For campaign comparison:

```
campaign_a_opens = PFCOUNT campaign:5000:unique_opens
campaign_b_opens = PFCOUNT campaign:5001:unique_opens
```

Finding users who opened campaign A or B (reach):

```
total_reach = PFCOUNT campaign:5000:unique_opens campaign:5001:unique_opens
```

**Video Streaming: Unique Viewers Per Content:**

Streaming platforms track unique viewers per video or episode.

```
PFADD video:12345:unique_viewers "user:1000"
PFADD video:12345:unique_viewers "user:1001"
```

Each video playback adds the viewer. Getting unique viewer count:

```
PFCOUNT video:12345:unique_viewers
```

For series-level metrics:

```
PFMERGE series:500:unique_viewers video:12345:unique_viewers video:12346:unique_viewers video:12347:unique_viewers
series_viewers = PFCOUNT series:500:unique_viewers
```

This estimates how many unique users watched any episode of the series. For retention analysis:

```
ep1_viewers = PFCOUNT video:12345:unique_viewers
ep2_viewers = PFCOUNT video:12346:unique_viewers
# This doesn't give exact dropoff but provides directional insight
```

For more accurate retention, you'd need to track viewers of episode 1 who also watched episode 2, which requires intersection—not directly supported by HyperLogLog. For that, you'd combine with other data structures.

---

## Conclusion: Choosing the Right Data Structure

Understanding Redis's rich collection of data structures enables you to build highly efficient applications. Each data structure serves specific use cases:

**Strings** are your default choice for simple values, caching, counters, and distributed locks. Use them when you need atomic operations on single values or when storing serialized data that doesn't require field-level access.

**Lists** excel at queues, message passing, activity feeds, and any scenario requiring ordered processing. Their blocking operations make them perfect for producer-consumer patterns in distributed systems.

**Sets** handle unique collections, relationships, and membership testing. Use them for tagging systems, social graphs, analytics requiring set operations, and deduplication.

**Sorted Sets** provide ordered rankings with scores, making them ideal for leaderboards, priority queues, time-series data, rate limiting with sliding windows, and any scenario requiring sorted access with scores.

**Hashes** represent structured objects with multiple fields. They're perfect for user profiles, shopping carts, product catalogs, configuration management, and any scenario where you need field-level access to object attributes.

**Bitmaps** offer extreme memory efficiency for binary states across large sets. Use them for daily active users, feature usage tracking, permissions, attendance, and any boolean flag tracking at scale.

**HyperLogLogs** provide approximate unique counts with fixed memory. They're essential for analytics at scale where exact counts aren't critical but memory efficiency is paramount.

The key to mastering Redis is understanding these data structures deeply and recognizing which structure fits your access patterns, memory constraints, and performance requirements. Often, production systems combine multiple data structures—using sorted sets for leaderboards, hashes for user profiles, and HyperLogLogs for analytics—each playing to its strengths.

Redis's performance comes from choosing the right data structure for each job. A poorly chosen structure can lead to memory bloat, slow queries, and scaling challenges. A well-chosen structure makes operations that would be complex in traditional databases become simple, fast, and memory-efficient.

As you build applications with Redis, start with the simplest structure that meets your needs, then optimize based on actual usage patterns. Monitor memory consumption, query performance, and access patterns. Redis's versatility means there's usually an elegant solution—you just need to find the right data structure for the job.