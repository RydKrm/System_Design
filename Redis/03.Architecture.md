# Redis Architecture and Data Storage: Deep Dive

## Redis Architecture Overview

Redis follows a client-server architecture with a single-threaded event-driven model that achieves exceptional performance through intelligent design choices. Understanding Redis's architecture is crucial for leveraging its full potential and avoiding common pitfalls.

```
┌─────────────────────────────────────────────────────────────────┐
│                        CLIENT APPLICATIONS                       │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐       │
│  │  Web App │  │   API    │  │  Worker  │  │  Service │       │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘       │
└───────┼─────────────┼─────────────┼─────────────┼──────────────┘
        │             │             │             │
        └─────────────┴─────────────┴─────────────┘
                          │
        ┌─────────────────▼─────────────────┐
        │     REDIS CLIENT LIBRARIES        │
        │  (redis-py, jedis, node-redis)    │
        └─────────────────┬─────────────────┘
                          │
                          │ RESP Protocol
                          │ (TCP Connection)
                          │
        ┌─────────────────▼─────────────────┐
        │       REDIS SERVER INSTANCE        │
        │                                    │
        │  ┌──────────────────────────────┐ │
        │  │     EVENT LOOP (Single       │ │
        │  │     Threaded Core)           │ │
        │  │                              │ │
        │  │  ┌────────────────────────┐ │ │
        │  │  │  Command Processing    │ │ │
        │  │  │  Queue                 │ │ │
        │  │  └────────────────────────┘ │ │
        │  └──────────────────────────────┘ │
        │                                    │
        │  ┌──────────────────────────────┐ │
        │  │   IN-MEMORY DATA STORAGE     │ │
        │  │                              │ │
        │  │  ┌────────────────────────┐ │ │
        │  │  │   Key Space (Dict)     │ │ │
        │  │  │   ┌──────────────────┐ │ │ │
        │  │  │   │  Key → Value     │ │ │ │
        │  │  │   │  Key → Value     │ │ │ │
        │  │  │   │  Key → Value     │ │ │ │
        │  │  │   └──────────────────┘ │ │ │
        │  │  └────────────────────────┘ │ │
        │  │                              │ │
        │  │  ┌────────────────────────┐ │ │
        │  │  │  Expires Dictionary    │ │ │
        │  │  │  (TTL Management)      │ │ │
        │  │  └────────────────────────┘ │ │
        │  └──────────────────────────────┘ │
        │                                    │
        │  ┌──────────────────────────────┐ │
        │  │   BACKGROUND THREADS         │ │
        │  │  (Redis 4.0+)                │ │
        │  │                              │ │
        │  │  • Lazy Freeing              │ │
        │  │  • AOF Rewrite               │ │
        │  │  • Large Key Deletion        │ │
        │  └──────────────────────────────┘ │
        │                                    │
        │  ┌──────────────────────────────┐ │
        │  │    PERSISTENCE LAYER         │ │
        │  │                              │ │
        │  │  ┌─────────┐  ┌───────────┐ │ │
        │  │  │   RDB   │  │    AOF    │ │ │
        │  │  │ Snapshots│  │   Logs    │ │ │
        │  │  └─────────┘  └───────────┘ │ │
        │  └──────────────────────────────┘ │
        └────────────────────────────────────┘
                          │
                          │
        ┌─────────────────▼─────────────────┐
        │         DISK STORAGE               │
        │                                    │
        │  • dump.rdb (RDB snapshots)       │
        │  • appendonly.aof (AOF log)       │
        │  • Configuration files            │
        └────────────────────────────────────┘
```

### Single-Threaded Event Loop Model

Redis's core operates on a single-threaded event loop, which might seem counterintuitive in an era of multi-core processors. However, this design choice provides several critical advantages.

**Why Single-Threaded?**

The single-threaded model eliminates the complexity and overhead of thread synchronization, locks, and context switching. When Redis processes a command, it has exclusive access to all data structures, meaning no race conditions, no deadlocks, and no need for complex locking mechanisms. This simplicity translates directly to speed.

Consider what happens when a client sends a command: Redis's event loop receives the command, parses it, executes it against in-memory data structures, and sends the response—all without interruption. No other command executes simultaneously. This atomic execution model makes operations predictable and fast.

**How It Works:**

Redis uses an I/O multiplexing mechanism (epoll on Linux, kqueue on BSD/macOS) to handle thousands of concurrent client connections efficiently. The event loop monitors all client sockets for activity. When data arrives, Redis reads the command, executes it, and writes the response back. While one command executes, other connections wait in the event queue.

```
┌─────────────────────────────────────────────────────────┐
│              REDIS EVENT LOOP FLOW                      │
│                                                         │
│  ┌──────────────────────────────────────────────────┐ │
│  │  1. Wait for Events (epoll/kqueue)               │ │
│  │     - Client connections                         │ │
│  │     - Commands ready to read                     │ │
│  │     - Responses ready to write                   │ │
│  │     - Timer events (evictions, expiry)           │ │
│  └──────┬───────────────────────────────────────────┘ │
│         │                                              │
│         ▼                                              │
│  ┌──────────────────────────────────────────────────┐ │
│  │  2. Read Commands from Clients                   │ │
│  │     - Parse RESP protocol                        │ │
│  │     - Validate command syntax                    │ │
│  └──────┬───────────────────────────────────────────┘ │
│         │                                              │
│         ▼                                              │
│  ┌──────────────────────────────────────────────────┐ │
│  │  3. Execute Command                              │ │
│  │     - Lookup/modify in-memory data               │ │
│  │     - Atomic operation (no interruption)         │ │
│  └──────┬───────────────────────────────────────────┘ │
│         │                                              │
│         ▼                                              │
│  ┌──────────────────────────────────────────────────┐ │
│  │  4. Write Response to Client                     │ │
│  │     - Format RESP response                       │ │
│  │     - Add to output buffer                       │ │
│  └──────┬───────────────────────────────────────────┘ │
│         │                                              │
│         ▼                                              │
│  ┌──────────────────────────────────────────────────┐ │
│  │  5. Handle Housekeeping Tasks                    │ │
│  │     - Expire keys (lazy + active)                │ │
│  │     - Evict keys if memory limit reached         │ │
│  │     - Replicate to slaves                        │ │
│  └──────┬───────────────────────────────────────────┘ │
│         │                                              │
│         └──────────────► Loop Back to Step 1          │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

**Performance Implications:**

Because Redis doesn't spend CPU cycles on thread management, nearly all processing power goes to actual work. Memory access patterns are more cache-friendly since a single thread accesses data structures sequentially. The lack of context switching means predictable, consistent performance.

However, this model has limitations. Long-running commands block the entire server. If you execute `KEYS *` on a database with millions of keys, Redis becomes unresponsive until it completes. This is why Redis provides alternative commands like `SCAN` for operations that could be slow.

**Multi-Threading in Modern Redis:**

Starting with Redis 4.0, certain heavy operations moved to background threads:

- **Lazy Freeing**: Deleting large keys (like a hash with millions of fields) now happens asynchronously via `UNLINK` command
- **AOF Rewrite**: The process of rewriting append-only files runs in a background thread
- **Background Saves**: RDB snapshots use fork() but perform I/O in a child process

Redis 6.0 introduced threaded I/O for network operations. Multiple threads handle reading from and writing to client sockets, but command execution remains single-threaded. This improves throughput on systems with many cores and high network traffic without sacrificing the simplicity of single-threaded command processing.

```
┌────────────────────────────────────────────────────────┐
│         REDIS 6.0+ THREADING MODEL                     │
│                                                        │
│  ┌──────────────────────────────────────────────────┐ │
│  │        I/O THREADS (Network Layer)               │ │
│  │                                                  │ │
│  │  ┌────────┐  ┌────────┐  ┌────────┐           │ │
│  │  │Thread 1│  │Thread 2│  │Thread 3│           │ │
│  │  │Read/   │  │Read/   │  │Read/   │           │ │
│  │  │Write   │  │Write   │  │Write   │           │ │
│  │  └───┬────┘  └───┬────┘  └───┬────┘           │ │
│  └──────┼───────────┼───────────┼─────────────────┘ │
│         │           │           │                    │
│         └───────────┴───────────┘                    │
│                     │                                │
│                     ▼                                │
│  ┌──────────────────────────────────────────────────┐ │
│  │     MAIN THREAD (Command Processing)             │ │
│  │                                                  │ │
│  │  • Parse commands                                │ │
│  │  • Execute commands (SINGLE-THREADED)            │ │
│  │  • Modify data structures                        │ │
│  │  • Prepare responses                             │ │
│  └──────────────────────────────────────────────────┘ │
│                     │                                │
│                     ▼                                │
│  ┌──────────────────────────────────────────────────┐ │
│  │     BACKGROUND THREADS                           │ │
│  │                                                  │ │
│  │  • Lazy key deletion (UNLINK)                    │ │
│  │  • AOF rewrite                                   │ │
│  │  • Module background tasks                       │ │
│  └──────────────────────────────────────────────────┘ │
└────────────────────────────────────────────────────────┘
```

---

## How Redis Stores Data in Memory

Redis stores all data in RAM, organized in highly efficient data structures. Understanding this internal organization helps you optimize memory usage and performance.

### The Main Dictionary Structure

At the heart of Redis is the **keyspace dictionary**—a hash table that maps keys to values. This is essentially a giant hash map where keys are strings and values can be any Redis data type.

```
┌────────────────────────────────────────────────────────────┐
│                  REDIS KEYSPACE DICTIONARY                  │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │                    Hash Table                        │ │
│  │                                                      │ │
│  │  Index  │  Hash Bucket → Key-Value Entry Chain      │ │
│  │  ─────────────────────────────────────────────────  │ │
│  │    0    │  → "user:1000" → [Hash Object]            │ │
│  │    1    │  → "cart:500" → [Hash Object]             │ │
│  │    2    │  → "session:abc" → [String]               │ │
│  │    3    │  → NULL                                    │ │
│  │    4    │  → "leaderboard" → [Sorted Set]           │ │
│  │    5    │  → "queue:jobs" → [List]                  │ │
│  │   ...   │  ...                                       │ │
│  │  1023   │  → "tags:post:1" → [Set]                  │ │
│  │                                                      │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  Each Entry Contains:                                      │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  • Key (string)                                      │ │
│  │  • Pointer to Value (actual data structure)          │ │
│  │  • Metadata (type, encoding, LRU info)               │ │
│  │  • Pointer to next entry (collision chain)           │ │
│  └──────────────────────────────────────────────────────┘ │
└────────────────────────────────────────────────────────────┘
```

**Hash Table Implementation:**

Redis uses a hash table with separate chaining for collision resolution. When two keys hash to the same bucket, they form a linked list. Redis maintains a load factor and automatically resizes the hash table when it becomes too full, redistributing keys across more buckets to maintain O(1) average lookup time.

**Progressive Rehashing:**

When the dictionary needs to grow, Redis doesn't rehash everything at once (which would block the server). Instead, it uses **progressive rehashing**: it maintains two hash tables temporarily, gradually moving entries from the old table to the new one over many operations. During this period, lookups check both tables, and each command operation moves a few entries.

```
┌─────────────────────────────────────────────────────────┐
│          PROGRESSIVE REHASHING PROCESS                  │
│                                                         │
│  Initial State: Hash table getting full                 │
│  ┌────────────────────────────────────┐                │
│  │   OLD HASH TABLE (Size: 1024)      │                │
│  │   [███████████████████] 90% full   │                │
│  └────────────────────────────────────┘                │
│                                                         │
│  Rehashing Triggered: Allocate new table               │
│  ┌────────────────────────────────────┐                │
│  │   OLD TABLE (Size: 1024)           │                │
│  │   [███████████████████] 90% full   │                │
│  └────────────────────────────────────┘                │
│                   ↓ migrate entries                     │
│  ┌────────────────────────────────────┐                │
│  │   NEW TABLE (Size: 2048)           │                │
│  │   [█░░░░░░░░░░░░░░░░░] 10% full    │                │
│  └────────────────────────────────────┘                │
│                                                         │
│  During Operations: Gradual migration                   │
│  • Each command moves 1-100 entries                     │
│  • New keys go directly to new table                    │
│  • Lookups check both tables                            │
│                                                         │
│  ┌────────────────────────────────────┐                │
│  │   OLD TABLE (Size: 1024)           │                │
│  │   [████░░░░░░░░░░░░░░] 40% full    │                │
│  └────────────────────────────────────┘                │
│                   ↓ continuing migration                │
│  ┌────────────────────────────────────┐                │
│  │   NEW TABLE (Size: 2048)           │                │
│  │   [████████████░░░░░░] 60% full    │                │
│  └────────────────────────────────────┘                │
│                                                         │
│  Complete: Old table deallocated                        │
│  ┌────────────────────────────────────┐                │
│  │   NEW TABLE (Size: 2048)           │                │
│  │   [██████████████████] 90% full    │                │
│  └────────────────────────────────────┘                │
└─────────────────────────────────────────────────────────┘
```

### Memory Layout of Redis Objects

Every value in Redis is represented as a **Redis Object** (robj), which contains metadata about the value's type and encoding.

```
┌────────────────────────────────────────────────────────────┐
│                     REDIS OBJECT (robj)                     │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Type (4 bits)                                       │ │
│  │  • STRING (0)                                        │ │
│  │  • LIST (1)                                          │ │
│  │  • SET (2)                                           │ │
│  │  • ZSET (3)                                          │ │
│  │  • HASH (4)                                          │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Encoding (4 bits)                                   │ │
│  │  • RAW (raw string)                                  │ │
│  │  • INT (integer)                                     │ │
│  │  • HT (hash table)                                   │ │
│  │  • ZIPLIST (compact list)                            │ │
│  │  • INTSET (integer set)                              │ │
│  │  • SKIPLIST (skip list)                              │ │
│  │  • EMBSTR (embedded string)                          │ │
│  │  • QUICKLIST (list of ziplists)                      │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  LRU/LFU (24 bits)                                   │ │
│  │  • Last access time (for LRU eviction)               │ │
│  │  • Access frequency (for LFU eviction)               │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Reference Count (32 bits)                           │ │
│  │  • For memory management                             │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Pointer to Actual Data (64 bits)                    │ │
│  │  • Points to the actual data structure               │ │
│  └──────────────────────────────────────────────────────┘ │
└────────────────────────────────────────────────────────────┘
```

**Encoding Optimization:**

Redis automatically chooses the most memory-efficient encoding for each data type based on its contents. For example:

- **Strings**: Small integers are stored directly in the object (no separate allocation). Strings under 44 bytes use embedded string encoding (EMBSTR). Larger strings use separate allocations (RAW).

- **Lists**: Modern Redis uses quicklists (a list of compressed ziplists) for all lists, balancing memory efficiency with performance.

- **Hashes**: Small hashes (under 512 entries with values under 64 bytes by default) use ziplists. Larger hashes use hash tables.

- **Sets**: Small sets containing only integers use intsets. Regular sets use hash tables.

- **Sorted Sets**: Small sorted sets use ziplists. Larger ones use a combination of hash table (for O(1) score lookup by member) and skip list (for O(log N) range queries).

### String Storage

Strings are the simplest data type, but Redis optimizes their storage significantly.

```
┌────────────────────────────────────────────────────────────┐
│                STRING STORAGE STRATEGIES                    │
│                                                            │
│  1. INTEGER ENCODING (for numbers -2^63 to 2^63-1)        │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Key: "counter"                                      │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ robj                                           │ │ │
│  │  │  type: STRING                                  │ │ │
│  │  │  encoding: INT                                 │ │ │
│  │  │  ptr: 12345 (stored directly, no allocation) │ │ │
│  │  └────────────────────────────────────────────────┘ │ │
│  │  Memory: ~20 bytes total                            │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  2. EMBSTR ENCODING (strings ≤ 44 bytes)                  │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Key: "user:name"                                    │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ robj + embedded string data                    │ │ │
│  │  │  type: STRING                                  │ │ │
│  │  │  encoding: EMBSTR                              │ │ │
│  │  │  data: "Alice" (embedded in same allocation)  │ │ │
│  │  └────────────────────────────────────────────────┘ │ │
│  │  Memory: ~35 bytes (single allocation)              │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  3. RAW ENCODING (strings > 44 bytes)                     │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Key: "article:content"                              │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ robj                                           │ │ │
│  │  │  type: STRING                                  │ │ │
│  │  │  encoding: RAW                                 │ │ │
│  │  │  ptr ───────┐                                  │ │ │
│  │  └─────────────┼──────────────────────────────────┘ │ │
│  │                │                                     │ │
│  │                ▼                                     │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ Separate memory allocation                     │ │ │
│  │  │ "This is a long article content..."           │ │ │
│  │  └────────────────────────────────────────────────┘ │ │
│  │  Memory: ~25 bytes (robj) + string length + 1      │ │
│  └──────────────────────────────────────────────────────┘ │
└────────────────────────────────────────────────────────────┘
```

**Why Multiple Encodings?**

Memory allocation has overhead. Each malloc() call adds metadata. By embedding small strings directly in the Redis object, you eliminate one allocation and its overhead. For integers, storing the value directly in the pointer field (which is 64 bits) saves even more—no string conversion needed.

### List Storage

Lists in modern Redis use the **quicklist** encoding—a hybrid structure combining the memory efficiency of ziplists with the performance of linked lists.

```
┌────────────────────────────────────────────────────────────┐
│                    QUICKLIST STRUCTURE                      │
│                                                            │
│  List Key: "messages:user:1000"                            │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Quicklist Node 1                                    │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ Ziplist (Compressed)                           │ │ │
│  │  │ ["msg1", "msg2", "msg3", "msg4", "msg5"]      │ │ │
│  │  │ Size: ~200 bytes                               │ │ │
│  │  └────────────────────────────────────────────────┘ │ │
│  │         │                                            │ │
│  │         ▼ (next pointer)                             │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Quicklist Node 2                                    │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ Ziplist (Compressed)                           │ │ │
│  │  │ ["msg6", "msg7", "msg8", "msg9", "msg10"]     │ │ │
│  │  │ Size: ~200 bytes                               │ │ │
│  │  └────────────────────────────────────────────────┘ │ │
│  │         │                                            │ │
│  │         ▼ (next pointer)                             │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Quicklist Node 3                                    │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ Ziplist (Compressed)                           │ │ │
│  │  │ ["msg11", "msg12", "msg13"]                    │ │ │
│  │  │ Size: ~120 bytes                               │ │ │
│  │  └────────────────────────────────────────────────┘ │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  Configuration:                                            │
│  • list-max-ziplist-size: 8KB per ziplist node           │
│  • list-compress-depth: compress middle nodes             │
│                                                            │
│  Benefits:                                                 │
│  • Memory efficient (compressed ziplists)                  │
│  • Fast head/tail operations (direct node access)          │
│  • Good cache locality                                     │
└────────────────────────────────────────────────────────────┘
```

**Ziplist Compression:**

Ziplists are contiguous memory blocks where each element is stored with minimal overhead. For lists that aren't accessed from the middle frequently, Redis can compress the middle nodes using LZF compression, keeping only head and tail nodes uncompressed for fast LPUSH/RPUSH/LPOP/RPOP operations.

### Set Storage

Sets use either intsets (for small sets of integers) or hash tables.

```
┌────────────────────────────────────────────────────────────┐
│                     SET STORAGE                             │
│                                                            │
│  INTSET (small sets of integers only)                      │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Key: "active_user_ids"                              │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ encoding: INTSET                               │ │ │
│  │  │ ┌──────────────────────────────────────────┐  │ │ │
│  │  │ │ Integer Array (sorted)                   │  │ │ │
│  │  │ │ [1000, 1001, 1002, 1005, 1010]          │  │ │ │
│  │  │ │ Encoding: int16/int32/int64 (auto)      │  │ │ │
│  │  │ └──────────────────────────────────────────┘  │ │ │
│  │  └────────────────────────────────────────────────┘ │ │
│  │  Memory: header + (element_count × int_size)        │ │
│  │  Very compact, binary search for membership         │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  HASH TABLE (larger sets or non-integer members)          │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Key: "tags:article:5000"                            │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ encoding: HT                                   │ │ │
│  │  │ ┌──────────────────────────────────────────┐  │ │ │
│  │  │ │ Hash Table                               │  │ │ │
│  │  │ │  Bucket 0: "redis" → NULL                │  │ │ │
│  │  │ │  Bucket 1: NULL                          │  │ │ │
│  │  │ │  Bucket 2: "database" → NULL             │  │ │ │
│  │  │ │  Bucket 3: "caching" → NULL              │  │ │ │
│  │  │ │  ...                                     │  │ │ │
│  │  │ │  (values are NULL, only keys matter)     │  │ │ │
│  │  │ └──────────────────────────────────────────┘  │ │ │
│  │  └────────────────────────────────────────────────┘ │ │
│  │  Memory: hash table overhead + key storage          │ │
│  │  O(1) membership testing                            │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                            │
│  Conversion Threshold (configurable):                      │
│  • set-max-intset-entries: 512 (default)                  │
│  • Converts to HT if exceeded or non-integer added        │
└────────────────────────────────────────────────────────────┘
```

**Intset Optimization:**

When all set members are integers and the set is small, Redis stores them in a sorted array using the smallest integer encoding that fits the range. If values fit in 16-bit integers, it uses 2 bytes per element; if they need 32-bit, it uses 4 bytes; for large values, it uses 8 bytes. Membership testing uses binary search (O(log N)), which is fast for small sets and much more memory-efficient than hash tables.

### Hash Storage

Hashes optimize for small objects using ziplists, switching to hash tables when they grow.

```
┌────────────────────────────────────────────────────────────┐
│                     HASH STORAGE                           │
│                                                            │
│  ZIPLIST (small hashes)                                    │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  Key: "user:1000"                                    │  │
│  │  Conditions: ≤ 512 fields, each ≤ 64 bytes           │  │
│  │  ┌────────────────────────────────────────────────┐  │  │
│  │  │ encoding: ZIPLIST                              │  │  │
│  │  │ Contiguous memory block:                       │  │  │
│  │  │ ┌──────────────────────────────────────────┐   │  │  │
│  │  │ │ [field1][value1][field2][value2]...      │   │  │  │
│  │  │ │ ["name"]["Alice"]["email"]["a@ex.com"]   │   │  │  │
│  │  │ │ ["age"]["28"]["city"]["New York"]        │   │  │  │
│  │  │ └──────────────────────────────────────────┘   │  │  │
│  │  └────────────────────────────────────────────────┘  │  │
│  │  Memory: Very compact, ~50-70% less than HT          │  │
│  │  Access: O(N) but fast for small N due to locality   │  │
│  └──────────────────────────────────────────────────────┘  │
│                                                            │
│  HASH TABLE (larger hashes)                                │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  Key: "product:12345"                                │  │
│  │  ┌────────────────────────────────────────────────┐ │   │
│  │  │ encoding: HT                                   │ │   │
│  │  │ ┌──────────────────────────────────────────┐  │ │    │
│  │  │ │ Hash Table (field → value)               │  │ │    │
│  │  │ │                                          │  │ │    │
│  │  │ │ Bucket 0: "name" → "Laptop"              │  │ │    │
│  │  │ │ Bucket 1: "price" → "999.99"             │  │ │    │
│  │  │ │ Bucket 2: NULL                           │  │ │    │
│  │  │ │ Bucket 3: "stock" → "150"                │  │ │    │
│  │  │ │ Bucket 4: "brand" → "TechCo"             │  │ │    │
│  │  │ │ Bucket 5: "category" → "Electronics"     │  │ │    │
│  │  │ │ ...                                      │   │ │   │
│  │  │ └──────────────────────────────────────────┘   │ │   │
│  │  └────────────────────────────────────────────────┘ │   │
│  │  Memory: Larger overhead but O(1) field access      │   │
│  └─────────────────────────────────────────────────────┘   │
│  Configuration:                                            │
│  • hash-max-ziplist-entries: 512                           │
│  • hash-max-ziplist-value: 64 bytes                        │
│  • Exceeding either converts to hash table                 │
└────────────────────────────────────────────────────────────┘
```

**Why Ziplist for Small Hashes?**

Hash tables have significant per-entry overhead—pointers, bucket arrays, and collision handling structures. For small hashes (like user profiles with 10-20 fields), this overhead dwarfs the actual data. Ziplists store field-value pairs sequentially in memory with minimal overhead. While access is O(N), for N < 100, linear scanning through contiguous memory is often faster than hash table lookups due to CPU cache effects.

### Sorted Set Storage

Sorted sets use the most complex internal structure, combining hash tables and skip lists.

```
┌────────────────────────────────────────────────────────────┐
│                 SORTED SET STORAGE                         │
│                                                            │
│  ZIPLIST (small sorted sets)                               │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  Key: "leaderboard:mini"                             │  │
│  │  Conditions: ≤ 128 entries, each ≤ 64 bytes          │  │
│  │  ┌────────────────────────────────────────────────┐  │  │
│  │  │ encoding: ZIPLIST                              │  │  │
│  │  │ [member1][score1][member2][score2]...          │  │  │
│  │  │ ["Alice"][1000.0]["Bob"][2500.0]["Charlie"]... │  │  │
│  │  └────────────────────────────────────────────────┘  │  │
│  │  Sorted by score, very memory efficient              │  │
│  └──────────────────────────────────────────────────────┘  │
│                                                            │
│  SKIPLIST + HASH TABLE (larger sorted sets)                │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  Key: "leaderboard:global"                           │  │
│  │                                                      │  │
│  │  1. HASH TABLE (member → score lookup)               │  │
│  │  ┌────────────────────────────────────────────────┐  │  │
│  │  │ "Alice" → 1000.0                               │  │  │
│  │  │ "Bob" → 2500.0                                 │  │  │
│  │  │ "Charlie" → 1800.0                             │  │  │
│  │  │ "David" → 3200.0                               │  │  │
│  │  └────────────────────────────────────────────────┘  │  │
│  │  Purpose: O(1) score lookup by member                │  │
│  │                                                      │  │
│  │  2. SKIP LIST (sorted by score)                      │  │
│  │  ┌────────────────────────────────────────────────┐  │  │
│  │  │ Level 3: HEAD ─────────────────────→ NULL      │  │  │
│  │  │                                                │  │  │
│  │  │ Level 2: HEAD ──────→ [2500] ──────→ NULL      │  │  │
│  │  │                        Bob                     │  │  │
│  │  │ Level 1: HEAD ──→ [1800] ──→ [3200] ──→ NULL   │  │  │
│  │  │              Charlie        David              │  │  │
│  │  │ Level 0: HEAD → [1000] → [1800] → [2500]       │  │  │
│  │  │              Alice   Charlie   Bob             │  │  │
│  │  │              → [3200] → NULL                   │  │  │
│  │  │                David                           │  │  │
│  │  └────────────────────────────────────────────────┘  │  │
│  │  Purpose: O(log N) range queries, rank operations    │  │
│  │                                                      │  │
│  │  Both structures point to the same member objects!   │  │
│  └──────────────────────────────────────────────────────┘  │
│                                                            │
│  Skip List Explained:                                      │
│  • Probabilistic data structure                            │
│  • Multiple layers of linked lists                         │
│  • Higher levels "skip" elements for fast traversal        │
│  • Average O(log N) insertion, deletion, search            │
│  • Better than balanced trees for range queries            │
└────────────────────────────────────────────────────────────┘
```

**Why Dual Structure?**

Sorted sets need to support two types of operations efficiently:

1. **Score lookup by member**: "What's Alice's score?" → Hash table provides O(1)
2. **Range queries by score/rank**: "Give me players ranked 10-20" → Skip list provides O(log N)

Using both structures means some memory duplication, but both point to the same underlying member strings, minimizing redundancy. This dual structure makes sorted sets incredibly versatile.

**Skip List Advantage:**

Skip lists are simpler than balanced trees (like red-black trees) and perform similarly. They're easier to implement and debug, require less memory rebalancing, and handle range operations naturally—you traverse the list collecting elements, which is cache-friendly.

---

## Memory Management and Optimization

Redis includes sophisticated memory management to maximize efficiency and prevent memory exhaustion.

### Memory Allocation

```
┌───────────────────────────────────────────────────────────┐
│                 MEMORY ALLOCATION LAYERS                  │
│                                                           │
│  ┌──────────────────────────────────────────────────────┐ │
│  │           APPLICATION LAYER (Redis Commands)         │ │
│  │  SET key value / HSET hash field value / etc.        │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │          REDIS MEMORY ALLOCATOR WRAPPER              │ │
│  │  • Tracks allocated memory                           │ │
│  │  • Enforces maxmemory limits                         │ │
│  │  • Triggers eviction when needed                     │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │       MEMORY ALLOCATOR (jemalloc/tcmalloc/libc)      │ │
│  │                                                      │ │
│  │  jemalloc (default, recommended):                    │ │
│  │  • Optimized for multi-threaded applications         │ │
│  │  • Reduces memory fragmentation                      │ │
│  │  • Better performance than glibc malloc              │ │
│  │                                                      │ │
│  │  Memory Arena Structure:                             │ │
│  │  ┌────────────────────────────────────────────────┐ │ │
│  │  │ Small allocations: 8, 16, 32, 64 bytes...      │ │ │
│  │  │ ┌──┐┌──┐┌──┐┌──┐┌──┐                           │ │ │
│  │  │ └──┘└──┘└──┘└──┘└──┘                           │ │ │
│  │  ├────────────────────────────────────────────────┤ │ │
│  │  │ Medium allocations: up to 4KB                  │ │ │
│  │  │ ┌─────┐┌─────┐┌─────┐                          │ │ │
│  │  │ └─────┘└─────┘└─────┘                          │ │ │
│  │  ├────────────────────────────────────────────────┤ │ │
│  │  │ Large allocations: > 4KB                       │ │ │
│  │  │ ┌──────────────┐┌──────────────┐               │ │ │
│  │  │ └──────────────┘└──────────────┘               │ │ │
│  │  └────────────────────────────────────────────────┘ │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │                 OPERATING SYSTEM                     │ │
│  │  • Virtual memory management                         │ │
│  │  • Physical RAM allocation                           │ │
│  │  • Page management                                   │ │
│  └──────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────┘
```

**jemalloc Benefits:**

Redis defaults to jemalloc because it significantly reduces memory fragmentation compared to glibc's malloc. Fragmentation occurs when memory has many small free chunks that can't be used for larger allocations. jemalloc uses size-segregated arenas and other techniques to minimize fragmentation, often keeping fragmentation under 5% versus 20-30% with standard malloc.

### Memory Fragmentation

```
┌────────────────────────────────────────────────────────────┐
│                  MEMORY FRAGMENTATION                      │
│                                                            │
│  Ideal State (No Fragmentation):                           │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ [Used][Used][Used][Used][Used][Used][Free────────]   │  │
│  └──────────────────────────────────────────────────────┘  │
│  Memory Efficiency: 100%                                   │
│                                                            │
│  Fragmented State (After many alloc/free cycles):          │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ [Used][Free][Used][Free][Free][Used][Free][Used]     │  │
│  │ [Free][Used][Free][Free][Used][Free][Used][Free]     │  │
│  └──────────────────────────────────────────────────────┘  │
│  Memory Efficiency: ~60% (40% wasted in small fragments)   │
│                                                            │
│  Checking Fragmentation:                                   │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ INFO memory                                          │  │
│  │ used_memory: 1000000000 (1GB actual data)            │  │
│  │ used_memory_rss: 1500000000 (1.5GB OS reports)       │  │
│  │ mem_fragmentation_ratio: 1.5 (50% fragmentation)     │  │
│  └──────────────────────────────────────────────────────┘  │
│                                                            │
│  Fragmentation Ratio Interpretation:                       │
│  • ratio < 1.0: System is swapping (bad!)                  │
│  • ratio = 1.0-1.3: Healthy range                          │
│  • ratio > 1.5: Significant fragmentation                  │
│  • ratio > 2.0: Critical, consider restart/defrag          │
│                                                            │
│  Active Defragmentation (Redis 4.0+):                      │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ CONFIG SET activedefrag yes                          │  │
│  │                                                      │  │
│  │ Process:                                             │  │
│  │ 1. Identify fragmented memory regions                │  │
│  │ 2. Copy data to contiguous memory                    │  │
│  │ 3. Update internal pointers                          │  │
│  │ 4. Free old fragmented regions                       │  │
│  │                                                      │  │
│  │ Done incrementally to avoid blocking                 │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────┘
```

**Causes of Fragmentation:**

Fragmentation increases when you frequently modify variable-size data. For example, if you repeatedly update hash fields with varying value sizes, or append to strings causing reallocation, or delete keys of different sizes. Over time, memory becomes Swiss cheese—plenty of total free space, but in unusable tiny chunks.

**Active Defragmentation:**

Modern Redis can defragment memory in the background. It identifies fragmented data, copies it to contiguous memory regions, updates all references, and frees the old fragmented space. This happens incrementally during idle time to avoid impacting performance. Configuration allows you to tune how aggressively Redis defragments based on your fragmentation ratio and available CPU.

### Eviction Policies

When Redis reaches its memory limit (set by `maxmemory`), it must evict keys to make room for new data.

```
┌────────────────────────────────────────────────────────────┐
│                    EVICTION POLICIES                       │
│                                                            │
│  Configuration: maxmemory-policy <policy>                  │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ 1. noeviction (default)                              │  │
│  │    • Return errors on write commands when limit hit  │  │
│  │    • Read commands still work                        │  │
│  │    ┌────────────────────────────────────────────┐    │  │
│  │    │ Memory: [████████████████████████] 100%    │    │  │
│  │    │ SET newkey → ERROR: OOM                    │    │  │
│  │    │ GET existingkey → OK                       │    │  │
│  │    └────────────────────────────────────────────┘    │  │
│  └──────────────────────────────────────────────────────┘  │
│                                                            │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 2. allkeys-lru                                      │   │
│  │    • Evict least recently used keys from all keys   │   │
│  │    ┌────────────────────────────────────────────┐   │   │
│  │    │ Keys sorted by last access time:           │   │   │
│  │    │ key1 (10 sec ago) ← KEEP                   │   │   │
│  │    │ key2 (1 min ago)  ← KEEP                   │   │   │
│  │    │ key3 (1 hour ago) ← KEEP                   │   │   │
│  │    │ key4 (1 day ago)  ← EVICT THIS             │   │   │
│  │    └────────────────────────────────────────────┘   │   │
│  │    Best for: General caching scenarios              │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                            │
│  ┌─────────────────────────────────────────────────────┐   │
│  │ 3. allkeys-lfu                                      │   │
│  │    • Evict least frequently used keys               │   │
│  │    ┌────────────────────────────────────────────┐   │   │
│  │    │ Keys sorted by access frequency:           │   │   │
│  │    │ key1 (1000 accesses) ← KEEP                │   │   │
│  │    │ key2 (500 accesses)  ← KEEP                │   │   │
│  │    │ key3 (100 accesses)  ← KEEP                │   │   │
│  │    │ key4 (5 accesses)    ← EVICT THIS          │   │   │
│  │    └────────────────────────────────────────────┘   │   │
│  │    Best for: Identifying long-term popular data     │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                            │
│  ┌─────────────────────────────────────────────────────┐ │
│  │ 4. volatile-lru                                      │ │
│  │    • Evict LRU keys, but only from keys with TTL     │ │
│  │    ┌────────────────────────────────────────────┐   │ │
│  │    │ cache:user:1000 (TTL 3600s, old) ← EVICT   │   │ │
│  │    │ cache:product:500 (TTL 7200s, recent)      │   │ │
│  │    │ permanent:config (no TTL) ← NEVER EVICT    │   │ │
│  │    └────────────────────────────────────────────┘   │ │
│  │    Best for: Mixed permanent + cache data           │ │
│  └─────────────────────────────────────────────────────┘ │
│                                                           │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 5. volatile-lfu                                      │ │
│  │    • Evict LFU keys with TTL                         │ │
│  │    Similar to volatile-lru but uses frequency        │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 6. volatile-ttl                                      │ │
│  │    • Evict keys with nearest expiration time         │ │
│  │    ┌────────────────────────────────────────────┐    │ │
│  │    │ key1 (expires in 10 sec)  ← EVICT THIS     │    │ │
│  │    │ key2 (expires in 1 hour)  ← KEEP           │    │ │
│  │    │ key3 (expires in 1 day)   ← KEEP           │    │ │
│  │    └────────────────────────────────────────────┘    │ │
│  │    Best for: Time-sensitive cache data               │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 7. allkeys-random                                    │ │
│  │    • Randomly evict any key                          │ │
│  │    Fast but unpredictable                            │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 8. volatile-random                                   │ │
│  │    • Randomly evict keys with TTL                    │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  LRU/LFU Implementation:                                  │
│  • Approximate algorithm (not exact)                      │
│  • Samples 5 keys (configurable: maxmemory-samples)       │
│  • Evicts the best candidate from sample                  │
│  • Trade-off: speed vs accuracy                           │
└───────────────────────────────────────────────────────────┘
```

**Choosing an Eviction Policy:**

For pure cache use cases, `allkeys-lru` or `allkeys-lfu` work best. They treat all data as cache and evict the least useful keys. For mixed workloads where some keys are permanent (like configuration) and others are cache, use `volatile-lru` or `volatile-lfu`—only cached keys (those with TTL) get evicted. For time-sensitive cache, `volatile-ttl` evicts keys expiring soonest, which often correlates with least useful data.

**LRU Approximation:**

True LRU requires tracking access order for all keys, which would be expensive. Redis uses an approximated LRU: when eviction triggers, it samples a few random keys (default 5), checks their last access time, and evicts the oldest. This is much faster than maintaining a full LRU list and works well in practice. Increasing the sample size improves accuracy but uses more CPU.

### Key Expiration

```
┌────────────────────────────────────────────────────────────┐
│                    KEY EXPIRATION SYSTEM                   │
│                                                            │
│  Expires Dictionary:                                       │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ Separate hash table tracking keys with TTL:          │  │
│  │                                                      │  │
│  │  "session:abc123" → 1700060000 (Unix timestamp)      │  │
│  │  "cache:product:500" → 1700055000                    │  │
│  │  "ratelimit:user:1000" → 1700050060                  │  │
│  │                                                      │  │
│  │ Keys without TTL are NOT in this dictionary          │  │
│  └──────────────────────────────────────────────────────┘  │
│                                                            │
│  Two Expiration Strategies:                                │
│                                                            │
│  1. LAZY EXPIRATION (Passive)                              │
│  ┌──────────────────────────────────────────────────────┐  │
│  │  Client: GET session:abc123                          │  │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │  Redis: Check if key has expired                     │ │
│  │         │                                            │ │
│  │         ├─Yes→ Delete key, return NULL               │ │
│  │         └─No──→ Return value                         │ │
│  │                                                      │ │
│  │  • Happens on key access                             │ │
│  │  • O(1) overhead per operation                       │ │
│  │  • Memory not freed until key accessed               │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  2. ACTIVE EXPIRATION (Proactive)                         │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Runs periodically (10 times per second by default)  │ │
│  │                                                      │ │
│  │  Algorithm (per iteration):                          │ │
│  │  ┌────────────────────────────────────────────────┐  │ │
│  │  │ 1. Sample 20 random keys with TTL              │  │ │
│  │  │ 2. Delete all expired keys from sample         │  │ │
│  │  │ 3. If > 25% were expired, repeat               │  │ │
│  │  │    (up to time limit to avoid blocking)        │  │ │
│  │  └────────────────────────────────────────────────┘  │ │
│  │                                                      │ │
│  │  • Ensures expired keys don't accumulate             │ │
│  │  • Adaptive: runs more when many keys expire         │ │
│  │  • Time-limited to avoid blocking commands           │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  Expiration in Replication:                               │
│  ┌──────────────────────────────────────────────────────┐ │
│  │  Master → Slave Communication                        │ │
│  │                                                      │ │
│  │  Master: Detects key expiration                      │ │
│  │          Sends DEL command to slaves                 │ │
│  │                                                      │ │
│  │  Slaves: Don't expire keys independently             │ │
│  │          Wait for DEL from master                    │ │
│  │          Ensures consistency                         │ │
│  └──────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────┘
```

**Why Two Strategies?**

Lazy expiration is efficient—expired keys are deleted when accessed, with minimal overhead. However, if a key is never accessed after expiring, it wastes memory. Active expiration proactively scans for expired keys, preventing memory leaks from forgotten keys. The combination ensures both efficiency and memory hygiene.

**Expiration Precision:**

Expiration times are stored as Unix timestamps (millisecond precision). Lazy expiration is exact—the moment you access an expired key, it's deleted. Active expiration might leave expired keys in memory for up to 100ms (between scan cycles), which is usually acceptable.

---

## Persistence Mechanisms

Redis provides two main persistence mechanisms to prevent data loss during restarts or crashes.

### RDB (Redis Database) Snapshots

```
┌────────────────────────────────────────────────────────────┐
│                  RDB SNAPSHOT PROCESS                      │
│                                                            │
│  Configuration: save <seconds> <changes>                   │
│  Example: save 900 1     (after 900 sec if ≥1 change)      │
│           save 300 10    (after 300 sec if ≥10 changes)    │
│           save 60 10000  (after 60 sec if ≥10000 changes)  │
│                                                            │
│  Snapshot Workflow:                                        │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 1. Trigger Condition Met                             │ │
│  │    (Time + change threshold reached)                 │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 2. Redis forks a child process                       │ │
│  │    ┌──────────────────────────────────────────────┐ │ │
│  │    │ Parent Process (Main Redis)                  │ │ │
│  │    │ • Continues serving clients                  │ │ │
│  │    │ • Uses Copy-on-Write (COW)                   │ │ │
│  │    └──────────────────────────────────────────────┘ │ │
│  │    ┌──────────────────────────────────────────────┐ │ │
│  │    │ Child Process (Snapshot Writer)              │ │ │
│  │    │ • Has snapshot of memory at fork time        │ │ │
│  │    │ • Writes to temp file                        │ │ │
│  │    └──────────────────────────────────────────────┘ │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 3. Child serializes data to disk                     │ │
│  │    ┌──────────────────────────────────────────────┐ │ │
│  │    │ Write to: temp-<pid>.rdb                     │ │ │
│  │    │                                              │ │ │
│  │    │ Format: Binary, compressed                   │ │ │
│  │    │ • Magic header (REDIS)                       │ │ │
│  │    │ • Version                                    │ │ │
│  │    │ • Database selector                          │ │ │
│  │    │ • Key-value pairs                            │ │ │
│  │    │ • Metadata (expiration, type, encoding)      │ │ │
│  │    │ • Checksum (CRC64)                           │ │ │
│  │    └──────────────────────────────────────────────┘ │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 4. Atomic rename                                     │ │
│  │    rename(temp-<pid>.rdb, dump.rdb)                  │ │
│  │    • Ensures atomicity                               │ │
│  │    • Old dump.rdb replaced only when new is complete │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 5. Child process exits                               │ │
│  │    Parent receives signal, cleanup complete          │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  Copy-on-Write (COW) Mechanism:                           │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ At fork time:                                        │ │
│  │ ┌────────────────────────────────────────────────┐  │ │
│  │ │ Memory pages: [Page1][Page2][Page3][Page4]     │  │ │
│  │ │               ▲                         ▲      │  │ │
│  │ │               │                         │      │  │ │
│  │ │         Parent Process            Child Process│  │ │
│  │ │         (both reference same pages initially)  │  │ │
│  │ └────────────────────────────────────────────────┘  │ │
│  │                                                     │ │
│  │ When parent modifies data:                          │ │
│  │ ┌────────────────────────────────────────────────┐  │ │
│  │ │ Original: [Page1][Page2][Page3][Page4]         │  │ │
│  │ │                    ▲                           │  │ │
│  │ │                    │ Child still sees original │  │ │
│  │ │                                                │  │ │
│  │ │ Modified: [Page1][Page2'][Page3][Page4]        │  │ │
│  │ │                    ▲                           │  │ │
│  │ │                    │ Parent gets new copy      │  │ │
│  │ └────────────────────────────────────────────────┘  │ │
│  │                                                      │ │
│  │ • Only modified pages are copied                     │ │
│  │ • Child always sees consistent snapshot              │ │
│  │ • Memory overhead only for changes during snapshot   │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  RDB File Structure:                                      │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ [REDIS0009]  ← Magic + Version                       │ │
│  │ [AUX fields] ← Metadata (Redis version, creation)    │ │
│  │ [SELECT 0]   ← Database 0                            │ │
│  │ [RESIZE 1000 500] ← Hash table sizes                 │ │
│  │ [KEY "user:1000"][TYPE hash][VALUE ...]              │ │
│  │ [EXPIRY 1700000000] ← If key has TTL                 │ │
│  │ [KEY "session:abc"][TYPE string][VALUE ...]          │ │
│  │ ...                                                  │ │
│  │ [SELECT 1]   ← Database 1                            │ │
│  │ [KEY ...][VALUE ...]                                 │ │
│  │ ...                                                  │ │
│  │ [EOF]        ← End marker                            │ │
│  │ [CHECKSUM]   ← CRC64 for integrity                   │ │
│  └──────────────────────────────────────────────────────┘ │
└────────────────────────────────────────────────────────────┘
```

**RDB Advantages:**

RDB creates compact, single-file snapshots perfect for backups. The file is smaller than AOF and loads faster on restart. Since the child process handles saving, the parent Redis instance continues serving requests with minimal performance impact. RDB is ideal for disaster recovery—you can easily copy the .rdb file to backup storage.

**RDB Disadvantages:**

Data loss risk exists between snapshots. If Redis crashes 5 minutes after the last snapshot, you lose 5 minutes of data. The fork() operation can be expensive on large datasets—the OS must set up copy-on-write page tables, which takes time proportional to memory size. On systems with constrained memory, fork() might fail if there isn't enough free memory for potential COW copies.

**Manual Snapshots:**

You can trigger snapshots manually:
- `SAVE`: Synchronous save (blocks Redis until complete)—rarely used in production
- `BGSAVE`: Background save (forks child process)—safe for production use

### AOF (Append Only File)

```
┌────────────────────────────────────────────────────────────┐
│                  AOF (APPEND ONLY FILE)                    │
│                                                            │
│  Configuration: appendonly yes                             │
│                appendfsync <policy>                        │
│                                                            │
│  Write Process:                                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 1. Client Command                                    │ │
│  │    SET user:1000:name "Alice"                        │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 2. Command Execution                                 │ │
│  │    • Modify in-memory data                           │ │
│  │    • Command succeeds                                │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 3. Append to AOF Buffer                              │ │
│  │    • Command serialized in RESP protocol             │ │
│  │    • Stored in memory buffer first                   │ │
│  │    ┌──────────────────────────────────────────────┐  │ │
│  │    │ *3\r\n$3\r\nSET\r\n$15\r\nuser:1000:name\r\n │  │ │
│  │    │ $5\r\nAlice\r\n                              │  │ │
│  │    └──────────────────────────────────────────────┘  │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 4. Flush to Disk (based on appendfsync policy)       │ │
│  │                                                      │ │
│  │    Policy Options:                                   │ │
│  │    ┌──────────────────────────────────────────────┐  │ │
│  │    │ always:                                      │  │ │
│  │    │ • fsync() after every command                │  │ │
│  │    │ • Slowest, most durable                      │  │ │
│  │    │ • ~1ms per write on SSD                      │  │ │
│  │    │ • No data loss                               │  │ │
│  │    ├──────────────────────────────────────────────┤  │ │
│  │    │ everysec (default, recommended):             │  │ │
│  │    │ • fsync() once per second (background)       │  │ │
│  │    │ • Good performance                           │  │ │
│  │    │ • Up to 1 second of data loss                │  │ │
│  │    ├──────────────────────────────────────────────┤  │ │
│  │    │ no:                                          │  │ │
│  │    │ • Let OS decide when to flush                │  │ │
│  │    │ • Fastest                                    │  │ │
│  │    │ • Up to 30 seconds of data loss (Linux)      │  │ │
│  │    └──────────────────────────────────────────────┘  │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  AOF File Growth:                                         │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Initial state:                                       │ │
│  │ SET key1 "value1"                                    │ │
│  │ SET key2 "value2"                                    │ │
│  │ SET key3 "value3"                                    │ │
│  │ File size: 150 bytes                                 │ │
│  │                                                      │ │
│  │ After modifications:                                 │ │
│  │ SET key1 "value1"                                    │ │
│  │ SET key2 "value2"                                    │ │
│  │ SET key3 "value3"                                    │ │
│  │ DEL key2                                             │ │
│  │ SET key1 "newvalue1"                                 │ │
│  │ SET key3 "newvalue3"                                 │ │
│  │ File size: 300 bytes (redundant entries!)            │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  AOF Rewrite Process:                                     │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Trigger: auto-aof-rewrite-percentage 100             │ │
│  │         auto-aof-rewrite-min-size 64mb               │ │
│  │         (or manual: BGREWRITEAOF command)            │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 1. Fork child process                                │ │
│  │    • Child gets snapshot of current data             │ │
│  │    • Parent continues normal operations              │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 2. Child writes minimal commands                     │ │
│  │    ┌──────────────────────────────────────────────┐ │ │
│  │    │ Scans current database state                 │ │ │
│  │    │ Writes shortest command sequence:            │ │ │
│  │    │                                              │ │ │
│  │    │ Original: SET key1 "value1"                  │ │ │
│  │    │          SET key1 "newvalue1"                │ │ │
│  │    │                                              │ │ │
│  │    │ Rewritten: SET key1 "newvalue1"              │ │ │
│  │    │           (only final state)                 │ │ │
│  │    │                                              │ │ │
│  │    │ For hashes: HMSET instead of many HSETs      │ │ │
│  │    └──────────────────────────────────────────────┘ │ │
│  └───────────────────────┬─────────────────────────────┘ │
│                          │                               │
│                          ▼                               │
│  ┌─────────────────────────────────────────────────────┐ │
│  │ 3. Parent buffers new writes                        │ │
│  │    • Normal AOF continues                           │ │
│  │    • New writes also saved to rewrite buffer        │ │
│  │    ┌──────────────────────────────────────────────┐ │ │
│  │    │ Regular AOF: commands as they come           │ │ │
│  │    │ Rewrite Buffer: same commands buffered       │ │ │
│  │    └──────────────────────────────────────────────┘ │ │
│  └───────────────────────┬─────────────────────────────┘ │
│                          │                               │
│                          ▼                               │
│  ┌─────────────────────────────────────────────────────┐ │
│  │ 4. Child finishes, signals parent                   │ │
│  │    • Temporary AOF file complete                    │ │
│  └───────────────────────┬─────────────────────────────┘ │
│                          │                               │
│                          ▼                               │
│  ┌─────────────────────────────────────────────────────┐ │
│  │ 5. Parent appends buffer to new AOF                 │ │
│  │    • Writes buffered commands (happened during rewrite)│
│  │    • Atomic rename: temp AOF → appendonly.aof        │ │
│  │    • Old AOF deleted                                 │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  Result:                                                  │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Old AOF: 300 bytes (many redundant commands)         │ │
│  │ New AOF: 150 bytes (only current state)              │ │
│  │ 50% reduction typical (can be 90%+ reduction!)       │ │
│  └──────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────┘
```

**AOF Advantages:**

AOF provides better durability than RDB. With `appendfsync everysec`, you lose at most 1 second of data. With `appendfsync always`, you lose no data (but performance suffers). AOF is human-readable (RESP protocol), making it easier to inspect and even manually repair if corrupted. You can replay or selectively apply commands.

**AOF Disadvantages:**

AOF files are typically larger than RDB files because they contain the full command history. Loading AOF on restart is slower than loading RDB. AOF files can grow unbounded without rewriting. The fsync() calls add I/O overhead, though `everysec` balances this well.

### Hybrid Persistence (RDB + AOF)

```
┌──────────────────────────────────────────────────────────┐
│              HYBRID PERSISTENCE (Redis 4.0+)             │
│                                                          │
│  Configuration: aof-use-rdb-preamble yes                 │
│                                                          │
│  Combined File Structure:                                │
│  ┌─────────────────────────────────────────────────────┐ │
│  │ appendonly.aof                                      │ │
│  │                                                     │ │
│  │ ┌────────────────────────────────────────────────┐  │ │
│  │ │ RDB FORMAT (Compact snapshot of data)          │  │ │
│  │ │ • Magic header                                 │  │ │
│  │ │ • All current keys and values                  │  │ │
│  │ │ • Compressed, binary format                    │  │ │
│  │ │ • Fast to load                                 │  │ │
│  │ └────────────────────────────────────────────────┘  │ │
│  │ ┌────────────────────────────────────────────────┐  │ │
│  │ │ AOF FORMAT (Recent commands since snapshot)    │  │ │
│  │ │ • RESP protocol commands                       │  │ │
│  │ │ • SET key1 "value"                             │  │ │
│  │ │ • HSET hash field value                        │  │ │
│  │ │ • DEL oldkey                                   │  │ │
│  │ └────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────┘ │
│                                                          │
│  Benefits:                                               │
│  • Fast loading (RDB preamble loads quickly)             │
│  • Good durability (AOF tail has recent changes)         │
│  • Compact file size (RDB compressed, AOF only recent)   |
│  • Best of both worlds                                   │
│                                                          │
│  Startup Process:                                        │
│  ┌──────────────────────────────────────────────────────┐│
│  │ 1. Load RDB preamble → bulk of data restored         ││
│  │ 2. Replay AOF commands → recent changes applied      ││
│  │ 3. Redis ready to serve                              ││
│  └──────────────────────────────────────────────────────┘│
└──────────────────────────────────────────────────────────┘
```

**Best Practice:**

Modern Redis deployments often use hybrid persistence: the fast loading of RDB with the durability of AOF. Rewrite creates an RDB snapshot with subsequent commands in AOF format. This provides quick restarts and minimal data loss.

---

## Redis Replication Architecture

Redis supports master-slave replication for high availability and read scalability.

```
┌───────────────────────────────────────────────────────────┐
│                  REDIS REPLICATION                        │
│                                                           │
│  ┌──────────────────────────────────────────────────────┐ │
│  │                   MASTER                             │ │
│  │                                                      │ │
│  │  • Handles all writes                                │ │
│  │  • Sends replication stream to replicas              │ │
│  │  • Maintains replication backlog                     │ │
│  │                                                      │ │
│  │  ┌────────────────────────────────────────────────┐  │ │
│  │  │ Replication Backlog (circular buffer)          │  │ │
│  │  │ • Recent write commands                        │  │ │
│  │  │ • Allows partial resync                        │  │ │
│  │  │ • Size: repl-backlog-size (1MB default)        │  │ │
│  │  └────────────────────────────────────────────────┘  │ │
│  └───────────┬──────────────┬────────────┬──────────────┘ │
│              │              │            │                │
│              │ Replication  │            │                │
│              │ Stream       │            │                │
│              ▼              ▼            ▼                │
│  ┌─────────────────┐ ┌─────────────┐ ┌────────────────┐   │
│  │   REPLICA 1     │ │  REPLICA 2  │ │  REPLICA 3     │   │
│  │                 │ │             │ │                │   │
│  │  • Read-only    │ │  • Read-    │ │  • Read-only   │   │
│  │    (by default) │ │    only     │ │                │   │
│  │  • Async repl   │ │  • Can be   │ │  • Geographic  │   │
│  │  • Same DC      │ │    writable │ │    DR site     │   │
│  └─────────────────┘ └─────────────┘ └────────────────┘   │
│                                                           │
│  Replication Process:                                     │
│                                                           │
│  INITIAL SYNC (Full Resync):                              │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 1. Replica: PSYNC ? -1 (no previous replication)     │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 2. Master: Starts BGSAVE (RDB snapshot)              │ │
│  │          • Forks child process                       │ │
│  │          • Child writes RDB to disk                  │ │
│  │          • Master buffers new writes                 │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 3. Master: Sends RDB file to replica                 │ │
│  │          • Network transfer (can be slow/large)      │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 4. Replica: Flushes old data, loads RDB              │ │
│  │           • Temporarily unavailable                  │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 5. Master: Sends buffered writes                     │ │
│  │          • Commands that arrived during RDB          │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 6. Replica: In sync, receives replication stream     │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  ONGOING REPLICATION:                                     │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Master receives: SET key "value"                     │ │
│  │         │                                            │ │
│  │         ├─→ Execute locally                          │ │
│  │         │                                            │ │
│  │         └─→ Send to all replicas                     │ │
│  │             (asynchronous)                           │ │
│  │                                                      │ │
│  │ Replica receives command, executes it                │ │
│  │ • Small lag (typically < 1ms on same DC)             │ │
│  │ • Can lag more on slow networks or heavy load        │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  PARTIAL RESYNC (After brief disconnect):                 │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 1. Replica reconnects                                │ │
│  │    PSYNC <replication_id> <offset>                   │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 2. Master checks replication backlog                 │ │
│  │    • If offset in backlog: partial resync            │ │
│  │    • If offset too old: full resync                  │ │
│  │           │                                          │ │
│  │           ▼ (offset found)                           │ │
│  │ 3. Master sends missing commands from backlog        │ │
│  │    • Much faster than full resync                    │ │
│  │    • Replica applies and catches up                  │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  Replication Lag Monitoring:                              │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Master: INFO replication                             │ │
│  │ • connected_slaves: 3                                │ │
│  │ • slave0: lag=0 (in sync)                            │ │
│  │ • slave1: lag=250 (250ms behind)                     │ │
│  │ • slave2: lag=0                                      │ │
│  └──────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────┘
```

**Replication Characteristics:**

Redis replication is asynchronous by default—the master doesn't wait for replicas to acknowledge writes. This provides high performance but means replicas can lag slightly. For critical writes, you can use `WAIT` command to ensure N replicas received the write before proceeding.

Replicas are read-only by default (configurable). They can serve read queries, providing horizontal read scalability. Writes must go to the master. Replicas can have their own replicas (chained replication), though this increases complexity and lag.

**Replication Use Cases:**

- **High Availability**: If master fails, promote a replica to master
- **Read Scaling**: Distribute read queries across multiple replicas
- **Data Locality**: Place replicas geographically close to users
- **Disaster Recovery**: Keep replicas in different data centers
- **Backup**: Take RDB snapshots from replicas without impacting master

---

## Redis Cluster Architecture

For horizontal scaling and automatic sharding, Redis Cluster distributes data across multiple nodes.

```
┌───────────────────────────────────────────────────────────┐
│                    REDIS CLUSTER                          │
│                                                           │
│  Hash Slots: 16384 total (0-16383)                        │
│  • Each key mapped to a slot via CRC16(key) % 16384       │
│  • Slots distributed across master nodes                  │
│                                                           │
│  Example 3-Node Cluster:                                  │
│                                                           │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ MASTER 1                                             │ │
│  │ Slots: 0-5460 (5461 slots)                           │ │
│  │ Keys: user:1000, product:500, ...                    │ │
│  │       │                                              │ │
│  │       └─→ Replicates to ───┐                         │ │
│  └──────────────────────────────┼───────────────────────┘ │
│                                 │                         │
│  ┌──────────────────────────────┼───────────────────────┐ │
│  │ MASTER 2                     │                       │ │
│  │ Slots: 5461-10922 (5462 slots)                       │ │
│  │ Keys: session:abc, cart:123, ...                     │ │
│  │       │                      ││                      │ │
│  │       └─→ Replicates to ───┐││                       │ │
│  └──────────────────────────────┼┼──────────────────────┘ │
│                                 ││                        │
│  ┌──────────────────────────────┼┼────────────────────────┐ │
│  │ MASTER 3                     ││                        │ │
│  │ Slots: 10923-16383 (5461 slots)                      │ │
│  │ Keys: cache:data, leaderboard, ...                   │ │
│  │       │                      ││                      │ │
│  │       └─→ Replicates to ───┐││                       │ │
│  └──────────────────────────────┼┼┼─────────────────────┘ │
│                                 │││                       │
│  ┌──────────────────────────────▼││ ────────────────────┐ │
│  │ REPLICA 1 (for Master 1)      ││                     │ │
│  │ • Read-only copy              ││                     │ │
│  │ • Can be promoted to master   ││                     │ │
│  └────────────────────────────────┼┼────────────────────┘ │
│                                   ││                      │
│  ┌────────────────────────────────▼┼────────────────────┐ │
│  │ REPLICA 2 (for Master 2)       │                     │ │
│  └─────────────────────────────────┼────────────────────┘ │
│                                    │                      │
│  ┌─────────────────────────────────▼────────────────────┐ │
│  │ REPLICA 3 (for Master 3)                             │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  Key Routing:                                             │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Client wants: GET user:1000                          │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ Calculate: CRC16("user:1000") % 16384 = 3456         │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ Slot 3456 is on Master 1 (slots 0-5460)              │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ Connect to Master 1, execute GET                     │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  MOVED Redirection:                                       │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Client → Master 2: GET user:1000                     │ │
│  │                    (wrong node)                      │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ Master 2: MOVED 3456 192.168.1.1:6379                │ │
│  │          (slot 3456 is on Master 1)                  │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ Client: Updates slot mapping cache                   │ │
│  │        Connects to Master 1                          │ │
│  │        Executes GET user:1000                        │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  ASK Redirection (during slot migration):                 │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Slot being migrated from Master 1 → Master 2         │ │
│  │                                                      │ │
│  │ Client → Master 1: GET migrating_key                 │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ Master 1: Key not here anymore                       │ │
│  │          ASK 5000 192.168.1.2:6379                   │ │
│  │          (temporary, just for this key)              │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ Client: ASKING (special flag)                        │ │
│  │        GET migrating_key on Master 2                 │ │
│  │        (don't update slot cache, temporary)          │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  Hash Tags (for multi-key operations):                    │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Problem: MGET user:1000 user:2000                    │ │
│  │         • Keys might be on different nodes           │ │
│  │         • Multi-key ops require same slot            │ │
│  │                                                      │ │
│  │ Solution: Hash tags {user}:1000, {user}:2000         │ │
│  │         • CRC16 calculated on "user" only            │ │
│  │         • Both hash to same slot                     │ │
│  │         • Can use multi-key operations               │ │
│  │                                                      │ │
│  │ Example:                                             │ │
│  │ SET {user:1000}:profile "..."                        │ │
│  │ SET {user:1000}:settings "..."                       │ │
│  │ SET {user:1000}:preferences "..."                    │ │
│  │ MGET {user:1000}:profile {user:1000}:settings        │ │
│  │ (all on same node due to {user:1000} hash tag)       │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  Failover Process:                                        │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 1. Master 1 fails                                    │ │
│  │    • Network partition or crash                      │ │
│  │    • Replicas and other masters detect failure       │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 2. Replicas of Master 1 start election               │ │
│  │    • Wait random delay (150-500ms)                   │ │
│  │    • Request votes from other masters                │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 3. Replica 1 wins election (majority votes)          │ │
│  │    • Promotes itself to master                       │ │
│  │    • Takes over slots 0-5460                         │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 4. Cluster updates configuration                     │ │
│  │    • All nodes learn new topology                    │ │
│  │    • Clients update their slot mappings              │ │
│  │           │                                          │ │ 
│  │           ▼                                          │ │
│  │ 5. Cluster operational with new Master 1'            │ │
│  │    (Old Master 1 rejoins as replica when recovered)  │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  Slot Migration (resharding):                             │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Adding new node Master 4 to cluster:                 │ │
│  │                                                      │ │
│  │ 1. Join Master 4 to cluster (initially 0 slots)      │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 2. Migrate slots from existing masters               │ │
│  │    • Master 1: Move slots 0-1365 → Master 4          │ │
│  │    • Master 2: Move slots 5461-6826 → Master 4       │ │
│  │    • Master 3: Move slots 10923-12288 → Master 4     │ │
│  │    (Total: ~4096 slots to Master 4)                  │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 3. For each slot migration:                          │ │
│  │    a. Mark slot as migrating on source               │ │
│  │    b. Mark slot as importing on destination          │ │
│  │    c. Migrate keys one by one (MIGRATE command)      │ │
│  │    d. Update cluster config when done                │ │
│  │           │                                          │ │
│  │           ▼                                          │ │
│  │ 4. Cluster rebalanced across 4 masters               │ │
│  │    • Each master now has ~4096 slots                 │ │
│  │    • Better distribution, more capacity              │ │
│  └──────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────┘
```

**Cluster Advantages:**

- **Automatic Sharding**: Data automatically distributed across nodes
- **Horizontal Scalability**: Add nodes to increase capacity
- **High Availability**: Automatic failover with replicas
- **No Proxy Needed**: Clients connect directly to nodes

**Cluster Limitations:**

- **Multi-key Operations**: Only work if keys are in same slot (use hash tags)
- **No Multi-database**: Cluster only uses database 0
- **Complexity**: More moving parts than single-instance Redis
- **Network Overhead**: Cluster gossip protocol for node communication

---

## Complete Data Flow Example

Let's trace a complete operation through Redis's architecture:

```
┌───────────────────────────────────────────────────────────┐
│         COMPLETE OPERATION FLOW: SET user:1000 "Alice"    │
│                                                           │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 1. CLIENT APPLICATION                                │ │
│  │    app.py: redis.set("user:1000", "Alice")           │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 2. REDIS CLIENT LIBRARY (redis-py)                   │ │
│  │    • Serialize command to RESP protocol              │ │
│  │    • *3\r\n$3\r\nSET\r\n$9\r\nuser:1000\r\n...       │ │
│  │    • Send over TCP socket                            │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼ TCP/IP Network                 │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 3. REDIS SERVER - EVENT LOOP                         │ │
│  │    • epoll detects data on socket                    │ │
│  │    • Read command from socket buffer                 │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 4. COMMAND PARSER                                    │ │
│  │    • Parse RESP protocol                             │ │
│  │    • Extract: command=SET, args=["user:1000","Alice"]│ │
│  │    • Validate syntax                                 │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 5. COMMAND EXECUTION                                 │ │
│  │    ┌──────────────────────────────────────────────┐ │ │
│  │    │ a. Check maxmemory                           │ │ │
│  │    │    • If limit reached, run eviction          │ │ │
│  │    ├──────────────────────────────────────────────┤ │ │
│  │    │ b. Allocate memory for new string            │ │ │
│  │    │    • jemalloc allocates ~60 bytes            │ │ │
│  │    │    • 20 bytes for robj                       │ │ │
│  │    │    • 40 bytes for key + value (embedded)     │ │ │
│  │    ├──────────────────────────────────────────────┤ │ │
│  │    │ c. Create Redis object                       │ │ │
│  │    │    • type: STRING                            │ │ │
│  │    │    • encoding: EMBSTR (value ≤ 44 bytes)     │ │ │
│  │    │    • data: "Alice"                           │ │ │
│  │    ├──────────────────────────────────────────────┤  │ │
│  │    │ d. Insert into keyspace dictionary           │  │ │
│  │    │    • Hash "user:1000"                        │  │ │
│  │    │    • Find bucket in hash table               │  │ │
│  │    │    • Add entry: "user:1000" → robj           │  │ │
│  │    ├──────────────────────────────────────────────┤  │ │
│  │    │ e. Update statistics                         │  │ │
│  │    │    • Increment keys counter                  │  │ │
│  │    │    • Update memory usage tracking            │  │ │
│  │    └──────────────────────────────────────────────┘  │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 6. PERSISTENCE (if enabled)                          │ │
│  │    ┌──────────────────────────────────────────────┐  │ │
│  │    │ AOF: Append command to buffer                │  │ │
│  │    │      *3\r\n$3\r\nSET\r\n$9\r\nuser:1000...   │  │ │
│  │    │      (will fsync based on policy)            │  │ │
│  │    ├──────────────────────────────────────────────┤  │ │
│  │    │ RDB: Increment dirty counter                 │  │ │
│  │    │      (triggers BGSAVE if threshold met)      │  │ │
│  │    └──────────────────────────────────────────────┘  │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 7. REPLICATION (if replicas exist)                   │ │
│  │    • Add command to replication buffer               │ │
│  │    • Send to all connected replicas                  │ │
│  │    • Asynchronous (don't wait for ack)               │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 8. RESPONSE FORMATTING                               │ │
│  │    • Format reply: +OK\r\n                           │ │
│  │    • Write to client output buffer                   │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼                                │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 9. SEND RESPONSE TO CLIENT                           │ │
│  │    • Event loop writes to socket                     │ │
│  │    • TCP sends response to client                    │ │
│  └───────────────────────┬──────────────────────────────┘ │
│                          │                                │
│                          ▼ TCP/IP Network                 │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ 10. CLIENT RECEIVES RESPONSE                         │ │
│  │     • Client library parses "+OK"                    │ │
│  │     • Returns True to application                    │ │
│  │     • Application continues                          │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  Total Time (typical):                                    │
│  • Network RTT: 0.5ms                                     │
│  • Command execution: 0.01ms                              │
│  • Total: ~0.5-1ms for simple operations                  │
│                                                           │
│  Memory State After Operation:                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Keyspace Dictionary:                                 │ │
│  │   ...                                                │ │
│  │   "user:999"  → [String: "Bob"]                      │ │
│  │   "user:1000" → [String: "Alice"] ← NEW ENTRY        │ │
│  │   "user:1001" → [String: "Charlie"]                  │ │
│  │   ...                                                │ │
│  │                                                      │ │
│  │ Memory Used: +60 bytes                               │ │
│  │ Keys Count: +1                                       │ │
│  └──────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────┘
```

---

## Memory Efficiency Tips

Understanding Redis's memory layout helps optimize usage:

```
┌───────────────────────────────────────────────────────────┐
│              MEMORY OPTIMIZATION STRATEGIES               │
│                                                           │
│  1. Use Hashes for Small Objects                          │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ BAD: Separate keys for user fields                   │ │
│  │ SET user:1000:name "Alice"      → 60 bytes           │ │
│  │ SET user:1000:email "a@ex.com"  → 75 bytes           │ │
│  │ SET user:1000:age "28"          → 55 bytes           │ │
│  │ Total: 190 bytes                                     │ │
│  │                                                      │ │
│  │ GOOD: Single hash                                    │ │
│  │ HSET user:1000 name "Alice" email "a@ex.com" age 28  │ │
│  │ Total: 70 bytes (63% reduction!)                     │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  2. Keep Hash/Set/List Sizes Small for Ziplist Encoding   │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Small hash (≤512 fields, ≤64 bytes each):            │ │
│  │ • Uses ziplist encoding                              │ │
│  │ • ~50% memory savings                                │ │
│  │                                                      │ │
│  │ Split large objects:                                 │ │
│  │ BAD:  HSET user:1000 (1000 fields)                   │ │
│  │       → Hash table encoding, large overhead          │ │
│  │                                                      │ │
│  │ GOOD: HSET user:1000:profile (20 fields)             │ │
│  │       HSET user:1000:settings (15 fields)            │ │
│  │       HSET user:1000:prefs (10 fields)               │ │
│  │       → All use ziplist, much less memory            │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  3. Use Appropriate Data Types                            │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ For unique counts at scale:                          │ │
│  │ BAD:  SADD unique_visitors (100M users)              │ │
│  │       → ~2GB memory                                  │ │
│  │                                                      │ │
│  │ GOOD: PFADD unique_visitors (100M users)             │ │
│  │       → 12KB memory (99.99% reduction!)              │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  4. Set Expiration on Temporary Data                      │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ Always use TTL for:                                  │ │
│  │ • Cache entries: EX 3600                             │ │
│  │ • Sessions: EX 86400                                 │ │
│  │ • Temporary tokens: EX 300                           │ │
│  │ • Rate limit counters: EX 60                         │ │
│  │                                                      │ │
│  │ Prevents memory leaks from forgotten data            │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  5. Use Efficient Key Naming                              │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ BAD:  user_profile_information:1000                  │ │
│  │       (33 bytes for key)                             │ │
│  │                                                      │ │
│  │ GOOD: u:1000:p                                       │ │
│  │       (7 bytes for key)                              │ │
│  │                                                      │ │
│  │ With 1M keys: Saves 26MB just on key names!          │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  6. Enable Compression (if CPU allows)                    │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ list-compress-depth 1                                │ │
│  │ • Compresses list nodes except head/tail             │ │
│  │ • 30-50% memory reduction for lists                  │ │
│  │ • Minimal CPU overhead                               │ │
│  └──────────────────────────────────────────────────────┘ │
│                                                           │
│  7. Monitor and Analyze Memory Usage                      │
│  ┌──────────────────────────────────────────────────────┐ │
│  │ MEMORY USAGE key                                     │ │
│  │ • Shows bytes used by specific key                   │ │
│  │                                                      │ │
│  │ MEMORY DOCTOR                                        │ │
│  │ • Provides memory optimization suggestions           │ │
│  │                                                      │ │
│  │ MEMORY STATS                                         │ │
│  │ • Detailed memory allocation breakdown               │ │
│  │                                                      │ │
│  │ redis-cli --bigkeys                                  │ │
│  │ • Finds largest keys in database                     │ │
│  └──────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────┘
```

---

## Conclusion

Redis's architecture is a masterclass in simplicity meeting performance. The single-threaded event loop eliminates concurrency complexity while delivering exceptional throughput. In-memory storage with carefully optimized data structures provides microsecond latencies. Multiple encoding strategies automatically minimize memory usage. Persistence options balance durability with performance. Replication enables high availability and read scaling.

Understanding this architecture helps you:
- Choose appropriate data structures for your use cases
- Optimize memory usage through encoding awareness
- Configure persistence based on durability needs
- Design systems that leverage Redis's strengths
- Avoid pitfalls like blocking commands or unbounded data growth
- Scale horizontally with clustering when needed

Redis's elegance lies in doing a few things exceptionally well: fast in-memory operations, rich data structures, reliable persistence, and straightforward replication. By understanding how Redis stores and manages data internally, you can build applications that fully exploit its remarkable performance and versatility.