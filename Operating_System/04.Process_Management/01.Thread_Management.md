# Deep Dive into Threads: Understanding Concurrent Execution

## Introduction to Threads

A thread is one of the most fundamental yet elegant concepts in modern computing. To truly understand threads, we need to first understand what a process is, and then see why threads were invented as a lighter-weight alternative for concurrent execution within a process.

When you run a program—say, a web browser—the operating system creates a **process**. A process is the operating system's representation of a running program. It's an isolated environment that contains everything the program needs to execute: the program's code, its data, open files, network connections, and most importantly, its own private memory space.

But here's the problem: what if your web browser needs to do multiple things simultaneously? It needs to download a file, render a webpage, play a video, and respond to your mouse clicks—all at the same time. Creating a separate process for each task would be incredibly wasteful because each process would need its own complete memory space, its own copy of the browser's code, and the operating system would have to manage all these heavy processes separately.

This is where **threads** come in. A thread is like a lightweight process—it's an independent path of execution that can run concurrently with other threads, but unlike processes, threads within the same process share the same memory space, code, and resources. Think of a process as a house, and threads as the people living in that house. They all share the same rooms (memory), the same kitchen (code), and the same utilities (file handles, network connections), but each person can be doing different tasks independently.

```
PROCESS VS THREAD VISUALIZATION
================================

Single Process (Traditional):
┌─────────────────────────────────────────┐
│          PROCESS                         │
│                                          │
│  ┌────────────────────────────────┐     │
│  │     Code Segment               │     │
│  │  [Program Instructions]        │     │
│  └────────────────────────────────┘     │
│                                          │
│  ┌────────────────────────────────┐     │
│  │     Data Segment               │     │
│  │  [Global Variables]            │     │
│  └────────────────────────────────┘     │
│                                          │
│  ┌────────────────────────────────┐     │
│  │     Heap                       │     │
│  │  [Dynamic Memory]              │     │
│  └────────────────────────────────┘     │
│                                          │
│  ┌────────────────────────────────┐     │
│  │     Stack                      │     │
│  │  [Function Calls, Local Vars] │     │
│  └────────────────────────────────┘     │
│                                          │
│  Single Execution Thread                │
│  PC: 0x1000 → [Executing here]          │
└─────────────────────────────────────────┘


Multi-Threaded Process:
┌─────────────────────────────────────────────────────────┐
│          PROCESS (Shared Resources)                      │
│                                                          │
│  ┌────────────────────────────────────┐  ◄── SHARED    │
│  │     Code Segment                   │                 │
│  │  [Program Instructions]            │                 │
│  └────────────────────────────────────┘                 │
│                                                          │
│  ┌────────────────────────────────────┐  ◄── SHARED    │
│  │     Data Segment                   │                 │
│  │  [Global Variables]                │                 │
│  └────────────────────────────────────┘                 │
│                                                          │
│  ┌────────────────────────────────────┐  ◄── SHARED    │
│  │     Heap                           │                 │
│  │  [Dynamic Memory - malloc/new]     │                 │
│  └────────────────────────────────────┘                 │
│                                                          │
│  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐   │
│  │  Thread 1    │ │  Thread 2    │ │  Thread 3    │   │
│  │              │ │              │ │              │   │
│  │ ┌──────────┐ │ │ ┌──────────┐ │ │ ┌──────────┐ │   │
│  │ │  Stack   │ │ │ │  Stack   │ │ │ │  Stack   │ │ ◄─ PRIVATE
│  │ │          │ │ │ │          │ │ │ │          │ │   │
│  │ └──────────┘ │ │ └──────────┘ │ │ └──────────┘ │   │
│  │              │ │              │ │              │   │
│  │ PC: 0x1000   │ │ PC: 0x2500   │ │ PC: 0x1A00   │ ◄─ PRIVATE
│  │ Registers    │ │ Registers    │ │ Registers    │   │
│  │ Thread ID: 1 │ │ Thread ID: 2 │ │ Thread ID: 3 │   │
│  └──────────────┘ └──────────────┘ └──────────────┘   │
│                                                          │
│  All threads execute independently but share memory!    │
└─────────────────────────────────────────────────────────┘
```

---

## What Exactly is a Thread?

### The Thread Control Block (TCB)

At the operating system level, a thread is represented by a data structure called the Thread Control Block (TCB), similar to how a process is represented by a Process Control Block (PCB). The TCB contains everything the operating system needs to manage and schedule the thread:

**Thread ID (TID)**: A unique identifier for the thread, just like a process has a PID (Process ID).

**Program Counter (PC)**: The memory address of the next instruction this thread will execute. This is crucial—each thread has its own PC, allowing it to execute a different part of the program's code independently.

**Register Set**: A complete copy of all CPU registers (general-purpose registers, stack pointer, flags register, etc.). When the OS switches from one thread to another (context switch), it saves the current register values to the old thread's TCB and loads the saved register values from the new thread's TCB.

**Stack Pointer (SP)**: Points to the top of this thread's stack. Each thread has its own stack for function calls, local variables, and return addresses. This is why threads can execute different functions simultaneously.

**Thread State**: Indicates whether the thread is running, ready to run, blocked (waiting for I/O or a lock), or terminated.

**Scheduling Information**: Priority level, scheduling policy, CPU time used, etc. The OS uses this to decide which thread gets CPU time.

**Pointer to Parent Process**: Links back to the process (PCB) that owns this thread, so the OS knows which memory space and resources this thread can access.

```
THREAD CONTROL BLOCK (TCB) STRUCTURE
=====================================

┌─────────────────────────────────────────────┐
│           THREAD CONTROL BLOCK              │
├─────────────────────────────────────────────┤
│  Thread ID (TID):           12847           │
│  Parent Process ID (PID):   5023            │
│  Thread State:              RUNNING         │
├─────────────────────────────────────────────┤
│  EXECUTION CONTEXT (saved during switch):   │
│                                             │
│  Program Counter (PC):      0x00401A2C     │
│  Stack Pointer (SP):        0x7FFF8420     │
│                                             │
│  General Registers:                         │
│    R1:  0x00000005                         │
│    R2:  0x7FFF8000                         │
│    R3:  0x00000001                         │
│    ... (all registers)                      │
│                                             │
│  Flags Register:            0x0246          │
├─────────────────────────────────────────────┤
│  SCHEDULING INFO:                           │
│    Priority:                5               │
│    CPU Time Used:           127 ms          │
│    Time Quantum Remaining:  8 ms            │
├─────────────────────────────────────────────┤
│  STACK INFO:                                │
│    Stack Base:              0x7FFF0000     │
│    Stack Size:              8 MB            │
│    Stack Limit:             0x7FFF8000     │
├─────────────────────────────────────────────┤
│  Pointer to Process (PCB):  0x8000A400     │
│  Thread-Local Storage:      0x7000B000     │
└─────────────────────────────────────────────┘
```

### What Threads Share vs. What They Don't

Understanding what threads share and what they don't is crucial to understanding how they work:

**Shared Resources (All threads in a process share these):**

**Code Segment**: All threads execute from the same program code. Thread 1 might be executing function A while Thread 2 executes function B, but they're both running code from the same executable.

**Data Segment**: Global variables and static variables are shared. If Thread 1 modifies a global variable, Thread 2 immediately sees the change. This is powerful but dangerous—it's the source of race conditions and the reason we need synchronization.

**Heap**: Dynamically allocated memory (from malloc in C or new in C++) is shared. If Thread 1 allocates an object on the heap and passes the pointer to Thread 2, both threads can access it.

**File Descriptors**: Open files, sockets, and other I/O resources are shared. All threads in a process can read from and write to the same files.

**Process ID**: All threads have the same parent process ID (PID).

**Signal Handlers**: Signals are delivered at the process level, though specific threads can handle them.

**Working Directory and Environment Variables**: Shared among all threads.

**Private Resources (Each thread has its own):**

**Thread ID (TID)**: Uniquely identifies the thread.

**Stack**: Each thread has its own stack. This is absolutely essential—it's what allows threads to execute different function calls simultaneously. Thread 1 might be deep in a recursive function while Thread 2 is in a completely different function call chain.

**Program Counter (PC)**: Each thread executes independently, potentially at different locations in the code.

**Register Set**: When a thread is running on a CPU core, it uses the physical CPU registers. When it's not running, its register values are saved in its TCB.

**Thread-Local Storage (TLS)**: Special memory that looks like global variables to each thread but actually has a separate copy per thread.

**Signal Mask**: Each thread can block or unblock different signals.

```
MEMORY LAYOUT OF A MULTI-THREADED PROCESS
==========================================

High Memory (0xFFFFFFFF)
        ↓
┌────────────────────────────────────┐
│         KERNEL SPACE               │  ◄── OS kernel (not accessible)
├────────────────────────────────────┤
│                                    │
│      Thread 1 Stack                │  ◄── PRIVATE to Thread 1
│  ┌──────────────────────────────┐ │
│  │ Local variables              │ │
│  │ Function parameters          │ │
│  │ Return addresses             │ │
│  └──────────────────────────────┘ │
│          ↓ (grows downward)        │
├────────────────────────────────────┤
│                                    │
│      Thread 2 Stack                │  ◄── PRIVATE to Thread 2
│  ┌──────────────────────────────┐ │
│  │ Local variables              │ │
│  │ Function parameters          │ │
│  │ Return addresses             │ │
│  └──────────────────────────────┘ │
│          ↓ (grows downward)        │
├────────────────────────────────────┤
│                                    │
│      Thread 3 Stack                │  ◄── PRIVATE to Thread 3
│  ┌──────────────────────────────┐ │
│  │ Local variables              │ │
│  │ Function parameters          │ │
│  │ Return addresses             │ │
│  └──────────────────────────────┘ │
│          ↓ (grows downward)        │
├────────────────────────────────────┤
│                                    │
│      [Gap for more stacks]         │
│                                    │
├────────────────────────────────────┤
│          HEAP                      │  ◄── SHARED by all threads
│  ┌──────────────────────────────┐ │
│  │ malloc/new allocations       │ │
│  │ (grows upward) ↑             │ │
│  └──────────────────────────────┘ │
├────────────────────────────────────┤
│      BSS Segment                   │  ◄── SHARED
│  (Uninitialized global/static)    │
├────────────────────────────────────┤
│      Data Segment                  │  ◄── SHARED
│  (Initialized global/static vars)  │
├────────────────────────────────────┤
│      Code Segment (Text)           │  ◄── SHARED
│  (Program instructions - read-only)│
└────────────────────────────────────┘
Low Memory (0x00000000)

Key Points:
- Each thread has its own stack (separate memory region)
- All threads share the heap, data, and code segments
- Stacks typically have guard pages to detect overflow
- Stack size is usually limited (1-8 MB per thread)
```

---

## Why Do We Need Threads?

### The Problem with Single-Threaded Processes

Imagine you're writing a web server. It needs to handle multiple client connections simultaneously. With a single-threaded process:

```
SINGLE-THREADED WEB SERVER (BLOCKING)
======================================

Time →
Client 1 connects: [Processing request..................] Done
                    (3 seconds waiting for database)
                    
Client 2 connects:                                        [Processing...] 
                    ↑
                    Waits here! Can't start until Client 1 finishes!

Client 3 connects:                                                      [Processing...]
                    ↑
                    Waits even longer!

Problem: Server can only handle one client at a time.
If Client 1's request takes 3 seconds, Clients 2 and 3 must wait.
This is terrible for responsiveness and throughput!
```

You could create a new process for each client connection, but processes are heavy:

**High Memory Overhead**: Each process needs its own complete memory space. If your base process uses 50 MB, and you have 1000 clients, that's potentially 50 GB of memory just for duplicated code and data.

**Slow Creation Time**: Creating a process involves allocating memory, copying page tables, setting up a new address space—it typically takes milliseconds, which is an eternity in computing terms.

**Expensive Context Switching**: When the OS switches between processes, it must save and restore the entire memory context, flush TLB entries, and switch page tables. This is expensive.

**Difficult Communication**: Processes have separate memory spaces, so they need special mechanisms (pipes, shared memory, sockets) to communicate. This is complex and slow.

### How Threads Solve These Problems

Threads provide concurrency without the overhead of processes:

```
MULTI-THREADED WEB SERVER
==========================

Time →
                [Thread 1: Client 1] [Processing............] Done
                [Thread 2: Client 2] [Processing........] Done
                [Thread 3: Client 3] [Processing.....] Done

All threads run simultaneously (on multi-core CPU) or are 
interleaved efficiently (on single-core CPU).

When Thread 1 blocks waiting for database:
  Thread 2 and Thread 3 continue executing!
  
Memory usage: Base 50 MB + (1 MB per thread × 1000) = 1.05 GB
              Much better than 50 GB for processes!

Context switching between threads: ~microseconds
Context switching between processes: ~milliseconds
Thread creation time: ~microseconds
Process creation time: ~milliseconds
```

### Real-World Use Cases

**Web Browsers**: Modern browsers use dozens or hundreds of threads:

- Rendering thread: Draws pixels on screen
- JavaScript execution thread: Runs web page scripts
- Network threads: Download files, images, and data
- UI thread: Responds to mouse clicks and keyboard input
- Media threads: Decode and play videos/audio

Without threads, a slow-loading image would freeze your entire browser!

**Video Games**: Games use threads extensively:

- Rendering thread: Draws graphics at 60+ FPS
- Physics thread: Calculates collision detection and physics
- AI thread: Computes enemy behavior
- Audio thread: Plays sound effects and music
- Network thread: Handles multiplayer communication

**Database Systems**: Databases handle thousands of concurrent queries:

- Each client connection gets a thread
- Threads share the same cache and buffer pool (the heap)
- Much more efficient than separate processes

**GUI Applications**: Desktop applications use threads to stay responsive:

- UI thread: Must respond immediately to user input
- Background threads: Handle long-running operations (file saves, computations)

Without this separation, clicking a button during a long operation would freeze the entire application.

---

## How Threads Execute Code from Memory

### The Execution Model

Let's trace exactly how threads execute code, starting from the moment a thread is created:

**Step 1: Thread Creation**

When you create a thread (using pthread_create in C, std::thread in C++, or Thread in Java), the OS performs these operations:

```
THREAD CREATION PROCESS
=======================

Application calls: pthread_create(&thread_id, NULL, function, arg)
                          ↓
┌─────────────────────────────────────────────────────────┐
│  OPERATING SYSTEM (Thread Creation)                     │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. Allocate Thread Control Block (TCB)                 │
│     ┌────────────────────────────────┐                  │
│     │ TID: 12848 (new unique ID)     │                  │
│     │ State: READY                   │                  │
│     │ Parent PID: 5023               │                  │
│     └────────────────────────────────┘                  │
│                                                         │
│  2. Allocate Stack Memory                               │
│     - Request 8 MB from process virtual memory          │
│     - Set up guard page to detect overflow              │
│     - Initialize Stack Pointer to top of stack          │
│       SP = 0x7FFF8000                                   │
│                                                         │
│  3. Initialize Execution Context                        │
│     - Set PC to starting function address               │
│       PC = address of 'function'                        │
│     - Set up initial stack frame:                       │
│       ┌──────────────────────────┐                      │
│       │ Return address (to exit) │ ◄── Top of stack     │
│       ├──────────────────────────┤                      │
│       │ Function argument (arg)  │                      │
│       └──────────────────────────┘                      │
│                                                         │
│  4. Initialize Thread-Local Storage (if any)            │
│                                                         │
│  5. Add thread to scheduler's ready queue               │
│     Ready Queue: [Thread 12845, Thread 12846, ★ 12848]  │
│                                                         │
│  6. Return TID to caller                                │
│                                                         │
└─────────────────────────────────────────────────────────┘
                          ↓
        Thread is now ready to execute!
        Waiting for CPU time from scheduler.
```

**Step 2: Thread Scheduling and Execution**

The OS scheduler decides when each thread runs. On a multi-core system, multiple threads can truly run simultaneously. On a single-core system, the OS rapidly switches between threads, creating the illusion of simultaneity.

```
THREAD SCHEDULING AND EXECUTION
================================

CPU Core 0                      CPU Core 1
─────────                       ─────────
    ↓                               ↓
┌─────────────────┐           ┌─────────────────┐
│  SCHEDULER      │           │  SCHEDULER      │
│  Picks Thread   │           │  Picks Thread   │
└────────┬────────┘           └────────┬────────┘
         │                             │
         ↓                             ↓
    Load Thread 1 TCB            Load Thread 2 TCB
         │                             │
         ↓                             ↓
┌────────────────────────┐    ┌────────────────────────┐
│  Restore Registers:    │    │  Restore Registers:    │
│  PC = 0x00401A2C       │    │  PC = 0x00402B1E       │
│  SP = 0x7FFF8420       │    │  SP = 0x7FFE9320       │
│  R1 = 0x00000005       │    │  R1 = 0x000000FF       │
│  R2 = 0x7FFF8000       │    │  R2 = 0x12345678       │
│  ... (all registers)   │    │  ... (all registers)   │
└────────┬───────────────┘    └────────┬───────────────┘
         │                             │
         ↓                             ↓
    FETCH instruction               FETCH instruction
    from address PC                from address PC
         │                             │
         ↓                             ↓
    DECODE instruction              DECODE instruction
         │                             │
         ↓                             ↓
    EXECUTE instruction             EXECUTE instruction
         │                             │
         ↓                             ↓
    UPDATE PC, registers            UPDATE PC, registers
         │                             │
         ↓                             ↓
    [Repeat cycle]                  [Repeat cycle]
```

**Step 3: Fetching Instructions from Memory**

Both threads are executing code from the same code segment in memory, but at different locations:

```
CODE SEGMENT IN MEMORY (Shared by all threads)
===============================================

Address      Assembly Code               What Thread Executes It?
────────────────────────────────────────────────────────────────
0x00401A2C:  MOV R1, [R2]               ◄── Thread 1's PC points here
0x00401A30:  ADD R1, R3
0x00401A34:  CMP R1, #0
0x00401A38:  JE  0x00401A50
0x00401A3C:  CALL 0x00402000            ← Function call
...

0x00402000:  PUSH R4                    ← Function starts
0x00402004:  MOV R4, R1
0x00402008:  MUL R4, R4
...

0x00402B1E:  LOAD R5, [R6+8]            ◄── Thread 2's PC points here
0x00402B22:  SUB R5, #1
0x00402B26:  STORE [R7], R5
0x00402B2A:  RET                        ← Return from function
...

KEY INSIGHT: Same code segment, different execution points!
Thread 1 might be executing function foo() at 0x00401A2C
Thread 2 might be executing function bar() at 0x00402B1E
Both are reading instructions from the same physical memory,
but following different execution paths based on their PCs.
```

**Step 4: Accessing Memory (Stack vs. Heap)**

Let's see what happens when threads access different types of memory:

```
THREAD MEMORY ACCESS EXAMPLE
=============================

Consider this code running in two threads:

void worker_function(int thread_id) {
    int local_var = thread_id;           // Stack (PRIVATE)
    global_counter++;                    // Data segment (SHARED)
    int* heap_data = malloc(100);        // Heap (SHARED location)
    heap_data[0] = local_var;            // Writing to heap
    printf("Thread %d\n", thread_id);    // Stack for params
    free(heap_data);
}


MEMORY ACCESS BREAKDOWN:
────────────────────────────────────────────────────────────

1. Local Variable (int local_var = thread_id):
   ┌─────────────────────────────────────────────────────┐
   │ Thread 1 Stack (0x7FFF8420):                        │
   │   [local_var = 1]  ◄── Thread 1's private copy      │
   │                                                     │
   │ Thread 2 Stack (0x7FFE9320):                        │
   │   [local_var = 2]  ◄── Thread 2's private copy      │
   └─────────────────────────────────────────────────────┘
   
   Execution: Thread's SP register points to its stack
              PC contains instruction address
              CPU executes: MOV [SP-4], R1  (store to stack)
   
   Result: Each thread has its own copy. No conflict!


2. Global Variable (global_counter++):
   ┌─────────────────────────────────────────────────────┐
   │ Data Segment (0x00601040):                          │
   │   [global_counter = ???]  ◄── SHARED by all threads │
   └─────────────────────────────────────────────────────┘
   
   Execution WITHOUT synchronization (DANGEROUS!):
   
   Thread 1:                   Thread 2:
   LOAD R1, [0x00601040]       LOAD R1, [0x00601040]
   (R1 = 5)                    (R1 = 5)  ← Same value!
   ADD R1, #1                  ADD R1, #1
   (R1 = 6)                    (R1 = 6)  ← Both get 6!
   STORE [0x00601040], R1      STORE [0x00601040], R1
   
   PROBLEM: Both threads read 5, increment to 6, 
            and write 6. Final value is 6, not 7!
            This is a RACE CONDITION!
   
   Solution: Use atomic operations or locks:
   LOCK prefix on x86: LOCK INC [0x00601040]
   This makes the entire read-modify-write atomic.


3. Heap Allocation (int* heap_data = malloc(100)):
   ┌─────────────────────────────────────────────────────┐
   │ Thread 1 Stack:                                     │
   │   [heap_data ptr = 0x12340000]  ◄── Pointer on stack
   │                                    (private to T1)  │
   │ Thread 2 Stack:                                     │
   │   [heap_data ptr = 0x12340100]  ◄── Different ptr   │
   │                                    (private to T2)  │
   │                                                     │
   │ HEAP (Shared):                                      │
   │   0x12340000: [100 bytes]  ◄── Thread 1's allocation│
   │   0x12340100: [100 bytes]  ◄── Thread 2's allocation│
   └─────────────────────────────────────────────────────┘
   
   Execution: malloc() is thread-safe
              It uses locks internally to allocate from heap
              Each thread gets different memory regions
              BUT both regions are in the shared heap space
   
   Key Point: The pointer variable is on each thread's stack
              (private), but the allocated memory is in the
              shared heap. If Thread 1 passes its pointer to
              Thread 2, both can access the same memory!
```

**Step 5: Function Calls and Stack Usage**

When a thread calls a function, it uses its private stack:

```
FUNCTION CALL STACK MECHANICS
==============================

Code:
────
void function_c(int x) {
    int result = x * 2;
    printf("%d\n", result);
}

void function_b(int y) {
    int temp = y + 10;
    function_c(temp);
}

void function_a() {
    int value = 5;
    function_b(value);
}

// Thread 1 calls function_a()
// Thread 2 also calls function_a()


Thread 1's Stack (0x7FFF0000 - 0x7FFF8000):
────────────────────────────────────────────

High Address (0x7FFF8000) ← SP initially points here
        ↓
┌───────────────────────────────────────┐
│ [Unused stack space]                  │
├───────────────────────────────────────┤ ← SP after function_a
│ function_a's frame:                   │
│   [value = 5]                         │
│   [return address to main]            │
├───────────────────────────────────────┤ ← SP after function_b
│ function_b's frame:                   │
│   [temp = 15]                         │
│   [parameter y = 5]                   │
│   [return address to function_a]      │
├───────────────────────────────────────┤ ← SP after function_c
│ function_c's frame:                   │
│   [result = 30]                       │
│   [parameter x = 15]                  │
│   [return address to function_b]      │ ← Current SP
└───────────────────────────────────────┘
        ↓
Low Address (0x7FFF0000)


Thread 2's Stack (0x7FFE0000 - 0x7FFE8000):
────────────────────────────────────────────

High Address (0x7FFE8000)
        ↓
┌───────────────────────────────────────┐
│ function_a's frame:                   │
│   [value = 5]                         │
│   [return address to main]            │
├───────────────────────────────────────┤
│ function_b's frame:                   │
│   [temp = 15]                         │
│   [parameter y = 5]                   │
│   [return address to function_a]      │
├───────────────────────────────────────┤
│ function_c's frame:                   │
│   [result = 30]                       │
│   [parameter x = 15]                  │
│   [return address to function_b]      │ ← T2's current SP
└───────────────────────────────────────┘
        ↓
Low Address (0x7FFE0000)

KEY OBSERVATIONS:
─────────────────
1. Both threads execute the SAME functions (same code addresses)
2. But each has its own stack with its own local variables
3. The stacks are in completely different memory regions
4. Return addresses ensure each thread returns to its own caller
5. Stack Pointer (SP) register is saved/restored during context switches
```

---

## Thread Context Switching

Context switching is how the OS gives the illusion of concurrent execution on a single CPU core. Let's see exactly what happens:

```
THREAD CONTEXT SWITCH (Detailed)
=================================

Scenario: Thread 1 is running, OS decides to switch to Thread 2

BEFORE SWITCH - Thread 1 is running on CPU:
───────────────────────────────────────────
CPU Registers:
  PC = 0x00401A2C    (executing Thread 1's code)
  SP = 0x7FFF8420    (Thread 1's stack)
  R1 = 0x00000005
  R2 = 0x7FFF8000
  ... (all other registers)

Memory:
  Thread 1's stack has active function calls
  Thread 1's TCB is marked as RUNNING


INTERRUPT/TIMER TICK:
──────────────────────
Timer generates interrupt → CPU jumps to OS scheduler


STEP 1: Save Thread 1's Context
────────────────────────────────
OS Scheduler code executes:

save_context(current_thread):
  TCB[Thread 1].PC = CPU.PC // Save 0x00401A2C 
  TCB[Thread 1].SP = CPU.SP // Save 0x7FFF8420 
  TCB[Thread 1].R1 = CPU.R1 // Save 0x00000005 
  TCB[Thread 1].R2 = CPU.R2 // Save 0x7FFF8000 
  
  ... (save all registers) 
  
  TCB[Thread 1].Flags = CPU.Flags 
  TCB[Thread 1].state = READY // No longer RUNNING

STEP 2: Select Next Thread 
─────────────────────────── 
Scheduling algorithm runs: 
next_thread = select_thread() // Returns Thread 2 
TCB[Thread 2].state = RUNNING

STEP 3: Restore Thread 2's Context 
─────────────────────────────────── 
restore_context(next_thread):

CPU.PC = TCB[Thread 2].PC // Load 0x00402B1E 
CPU.SP = TCB[Thread 2].SP // Load 0x7FFE9320
CPU.R1 = TCB[Thread 2].R1 // Load 0x000000FF 
CPU.R2 = TCB[Thread 2].R2 // Load 0x12345678 

... (restore all registers) 

CPU.Flags = TCB[Thread 2].Flags

STEP 4: Resume Execution
───────────────────────── 
CPU now executes from Thread 2's PC:

- Fetch instruction from 0x00402B1E
- Stack operations use SP = 0x7FFE9320 (Thread 2's stack)
- Thread 2 has no idea it was ever paused!

AFTER SWITCH - Thread 2 is now running:
──────────────────────────────────────── 
CPU Registers: 
PC = 0x00402B1E (executing Thread 2's code) 
SP = 0x7FFE9320 (Thread 2's stack) 
R1 = 0x000000FF 
R2 = 0x12345678 ... (Thread 2's register values)

Memory: Thread 2's stack is now active Thread 1's stack is unchanged (frozen in time) Thread 1's TCB has saved state for next time

TIMING: 
─────── 
Context switch takes: 1-10 microseconds
- Save registers: ~0.5 μs
- Scheduling decision: ~0.5-5 μs
- Restore registers: ~0.5 μs
- TLB flush (if needed): ~1-3 μs

Cost is low because threads share the same memory space! (Process switches would also require page table switch)

```
---

# What Happens During Blocking Operations
---

````md

When a thread performs a blocking operation (like disk I/O or waiting for a lock), it cannot continue running.  
Instead of wasting CPU cycles, the OS switches to another READY thread.  
This makes the system fast and efficient.

---

# THREAD BLOCKING AND WAKEUP

Thread 1 executes:

```c
data = read_file("large_file.dat");  // Takes ~10 ms
````

### Execution Flow (with explanation)

```
┌──────────────┐
│ Executing... │
│     ...      │
│ read_file()  │
└──────┬───────┘
       │
       ▼
1. System call to kernel
2. Kernel initiates I/O
3. Thread 1 marked BLOCKED
4. Save Thread 1's context
5. Scheduler selects Thread 2
6. Restore Thread 2's context
```

### What does this mean?

- **System call**: User-mode switches to kernel mode.
    
- **Kernel starts disk read**: Actual I/O happens asynchronously.
    
- **Thread becomes BLOCKED**: It cannot proceed until the disk finishes.
    
- **Another thread runs**: CPU is never idle; it always finds work.
    

```
┌──────────────┐
│ Executing... │   ← Thread 2 now runs
│     ...      │
│     ...      │
└──────────────┘
```

... **10 ms passes** while disk is working ...

---

## I/O COMPLETION EVENT (with explanation)

```
Disk Controller → OS Scheduler

1. I/O complete interrupt!
2. Data is now in kernel buffer
3. Thread 1 marked READY
4. Thread 1 can run again
```

### What's happening here?

- The **disk controller** notifies the CPU that the requested data has arrived.
    
- The OS moves the blocked thread back to the **READY queue**.
    
- The scheduler decides _when_ Thread 1 should resume — maybe immediately, maybe later.
    

---

## Thread 1 Wakes Up (with explanation)

```
┌─────────-─────┐        ◄── Scheduler selects Thread 1
│  Wakes up!    │
│ data is ready │
│ Continue...   │
└──────┬────────┘
       │
       ▼
Thread 1 resumes after read_file()
Context restored
```

### Meaning

- Thread 1 picks up **exactly where it left off**.
    
- It continues after the `read_file()` call.
    
- It never knows it was paused — the illusion of continuity!
    

---

# THREAD STATES (with explanation)

```
 ┌─────────┐        schedule()        ┌─────────┐
 │  READY  │ ───────────────────────► │ RUNNING │
 └────▲────┘                          └────┬────┘
      │                                    │
      │ block on I/O or lock               │ I/O complete or lock available
      │                                    │
      ▼                                    │
 ┌─────────┐ ◄────────────────────────---──┘
 │ BLOCKED │
 └─────────┘
```

### Explanation

- **READY**: Thread is waiting to be executed.
    
- **RUNNING**: Thread currently using CPU.
    
- **BLOCKED**: Thread waiting for I/O or a lock — _not eligible to run_.
    

This three-state model is used in every modern OS.

---

# MULTI-THREADED WEB SERVER EXAMPLE

_(Explained as a full real-world timeline)_

Your server creates a new thread for each client.

```c
read()          → may block (waiting for client request)
read_file()     → may block (waiting for disk)
send()          → may block (waiting for network)
```

So each request triggers multiple thread state changes.

---

# EXECUTION TIMELINE (Explained)

### **Time: 10ms — Thread 1 Created**

- Thread 1 starts handling Client 1.
    
- Hits `read()` → BLOCKED waiting for network packet.
    
- This saves CPU time.
    

### **Time: 15ms — Thread 2 Created**

- Thread 2 also blocks on `read()`.
    
- OS switches between threads depending on readiness.
    

### **Time: 20ms — Client 1 sends data**

- Interrupt wakes Thread 1.
    
- Thread 1 parses request, then blocks again on disk I/O.
    

### **Time: 25ms — Client 2 sends data**

- Thread 2 wakes.
    
- Also blocks on disk I/O.
    

### **Time: 30–35ms — Disk completes I/O**

- Threads wake one by one.
    
- They send responses and exit.
    

### Why does this matter?

Because:

- CPU does **zero waiting**
    
- Threads only run when work is ready
    
- Memory is shared efficiently
    
- Thousands of clients can be served using hundreds of threads
    

---

# MEMORY LAYOUT EXPLAINED

### Shared:

- **Code segment**: All threads run the same functions.
    
- **Global variables**: All threads see the same values.
    
- **Heap**: All dynamic allocations share the same memory pool.
    

### Private per thread:

- Stack
    
- Registers
    
- Program counter
    
- Local variables
    

This is why threads are lightweight and fast.

---

# RACE CONDITION EXAMPLE (Explained)

The problem:

- Both threads read `account_balance = 1000`
    
- Both subtract 500
    
- Both write back 500
    

The CPU interleaving causes incorrect results.

### Why does this happen?

Because:

- The read–modify–write sequence is **not atomic**
    
- Two threads operate on shared memory without coordination
    

This is the essence of race conditions.

---

# FIX WITH MUTEX (Explained)

A mutex ensures:

- **Only one thread enters the critical section at a time**
    
- All three steps (read, modify, write) become atomic
    
- Other threads trying to enter must wait
    

Result:

- No more inconsistent updates
    
- Correct final account balance
    

---

# Conclusion (Clear and Simple)

Threads give programs:

- **Efficiency** — cheap context switching, shared memory
    
- **Responsiveness** — no CPU waiting during I/O
    
- **Parallelism** — multiple cores run threads truly simultaneously
    

But they require:

- Synchronization (mutexes, locks)
    
- Careful design
    
- Awareness of blocking operations
    

Threads are the backbone of web servers, OS schedulers, browsers, game engines, and more.

--- 


