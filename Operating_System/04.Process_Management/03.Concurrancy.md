# Deep Dive into Concurrency: Parallel Execution and Its Challenges

## Introduction to Concurrency

Concurrency is the ability of a system to handle multiple tasks that make progress during overlapping time periods. It's important to distinguish concurrency from parallelism, though the terms are often used interchangeably:

**Concurrency**: Multiple tasks making progress by rapidly switching between them (can happen on a single CPU core through time-sharing).

**Parallelism**: Multiple tasks literally executing at the exact same time (requires multiple CPU cores).

Think of concurrency like a chef cooking multiple dishes. On a single stove (single core), the chef switches between tasks: stir the soup, flip the burger, check the pasta, back to stirring the soup. The dishes progress concurrently through rapid task switching. With multiple stoves (multiple cores), the chef can literally work on multiple dishes simultaneously—that's parallelism.

```
CONCURRENCY VS PARALLELISM
==========================

CONCURRENCY (Single Core):
Time →
0ms    10ms   20ms   30ms   40ms   50ms
├──────┼──────┼──────┼──────┼──────┤
│Task A│Task B│Task C│Task A│Task B│Task C│
└──────┴──────┴──────┴──────┴──────┴──────

Tasks interleaved through context switching
All tasks make progress, but not simultaneously


PARALLELISM (Multi-Core):
Time →
Core 0: [Task A ████████████████████████████ ]
Core 1: [Task B ████████████████████████████ ]
Core 2: [Task C ████████████████████████████ ]

Tasks execute simultaneously
True parallel execution


CONCURRENT + PARALLEL (Multi-Core, Many Tasks):
Core 0: [A][C][E][A][C][E] ← Concurrent switching
Core 1: [B][D][F][B][D][F] ← Concurrent switching
        └─ Parallel execution between cores ─┘

Best of both worlds: Multiple cores each handling
multiple tasks through concurrency
```

Modern systems use both: multiple cores provide parallelism, and each core handles multiple tasks through concurrency. Your 4-core CPU might be running 200+ processes, achieving this through concurrent execution on each core plus parallelism across cores.

---

## How Concurrency Works: Process Level

### Process-Based Concurrency

When multiple processes run concurrently, each process has its own isolated memory space. The operating system manages their execution through scheduling and context switching.

```
PROCESS-LEVEL CONCURRENCY
=========================

System with 3 processes on single CPU core:

Process A: Web Browser
Process B: Text Editor  
Process C: Music Player

┌─────────────────────────────────────────────────────┐
│            OPERATING SYSTEM SCHEDULER               │
│                                                     │
│  Ready Queue: [Process A, Process B, Process C]    │
│                                                     │
│  Scheduling Algorithm: Round Robin (10ms quantum)  │
└─────────────────────────────────────────────────────┘
                         ↓
              ┌──────────────────────┐
              │     CPU Core 0       │
              └──────────────────────┘

Timeline:
─────────

0-10ms:   Process A runs (Web Browser rendering page)
          ↓
10ms:     Timer interrupt → Context switch
          ↓
10-20ms:  Process B runs (Text Editor syntax highlighting)
          ↓
20ms:     Timer interrupt → Context switch
          ↓
20-30ms:  Process C runs (Music Player decoding audio)
          ↓
30ms:     Timer interrupt → Context switch
          ↓
30-40ms:  Back to Process A (continues rendering)
          ↓
          [Cycle repeats...]


MEMORY ISOLATION:
─────────────────

Process A Memory Space:
┌──────────────────────────┐
│ Code: 0x00400000         │
│ Data: 0x00600000         │
│ Heap: 0x01000000         │
│ Stack: 0x7FFFFFFF        │
└──────────────────────────┘

Process B Memory Space:
┌──────────────────────────┐
│ Code: 0x00400000  ← Same virtual address!
│ Data: 0x00600000  ← But different physical memory
│ Heap: 0x01000000  ← Complete isolation
│ Stack: 0x7FFFFFFF │
└──────────────────────────┘

Process C Memory Space:
┌──────────────────────────┐
│ Code: 0x00400000         │
│ Data: 0x00600000         │
│ Heap: 0x01000000         │
│ Stack: 0x7FFFFFFF        │
└──────────────────────────┘

Key: MMU translates same virtual addresses to 
     different physical RAM locations for each process
```

### Process Communication (IPC)

Since processes are isolated, they need special mechanisms to communicate:

```
INTER-PROCESS COMMUNICATION (IPC)
==================================

1. PIPES:
   ──────
   Process A ──[write]──► | Pipe Buffer | ──[read]──► Process B
   
   Example: Shell commands
     cat file.txt | grep "error" | wc -l
     
   Process A (cat) writes to pipe
   Process B (grep) reads from pipe
   Unidirectional, kernel-managed buffer


2. SHARED MEMORY:
   ──────────────
   ┌─────────────────────────────────────┐
   │      Physical RAM                   │
   │  ┌──────────────────────────────┐   │
   │  │   Shared Memory Region       │   │
   │  │   [Data accessible to both]  │   │
   │  └──────────────────────────────┘   │
   └─────────────────────────────────────┘
            ↑                    ↑
            │                    │
      Process A              Process B
      (maps to 0x10000000)   (maps to 0x20000000)
   
   Fastest IPC method
   Requires synchronization (mutexes, semaphores)


3. MESSAGE QUEUES:
   ───────────────
   Process A → [msg1][msg2][msg3] → Queue → Process B reads
   
   Structured message passing
   OS manages queue
   Processes don't need to run simultaneously


4. SOCKETS:
   ────────
   Process A ←──[TCP/UDP]──→ Process B
   
   Can communicate across network
   Even between different machines
   Standard for client-server applications


5. SIGNALS:
   ────────
   Process A ──[SIGUSR1]──► Process B
   
   Asynchronous notifications
   Limited data (just signal number)
   Example: kill command sends signals
```

---

## How Concurrency Works: Thread Level

### Thread-Based Concurrency

Threads within the same process share memory, making communication easier but introducing new challenges:

```
THREAD-LEVEL CONCURRENCY
========================

Single Process with 3 threads:

┌─────────────────────────────────────────────────────┐
│              PROCESS: Web Server                    │
├─────────────────────────────────────────────────────┤
│                                                     │
│  SHARED MEMORY:                                     │
│  ┌──────────────────────────────────────────────┐   │
│  │ Code Segment: [Server functions]             │   │
│  │ Global Data: [Request counter, config, etc.] │   │
│  │ Heap: [Dynamically allocated data]           │   │
│  └──────────────────────────────────────────────┘   │
│         ↑              ↑              ↑             │
│         │              │              │             │
│  ┌──────┴────┐  ┌──────┴────┐  ┌──────┴────┐        │
│  │ Thread 1  │  │ Thread 2  │  │ Thread 3  │        │
│  │ (Stack)   │  │ (Stack)   │  │ (Stack)   │        │
│  │ Handle    │  │ Handle    │  │ Handle    │        │
│  │ Client A  │  │ Client B  │  │ Client C  │        │
│  └───────────┘  └───────────┘  └───────────┘        │
│                                                     │
└─────────────────────────────────────────────────────┘

Timeline (Single Core):
───────────────────────

0-5ms:    Thread 1 executes (processing Client A's request)
          Reads from socket, parses HTTP headers
          ↓
5ms:      Thread 1 blocks on disk I/O (reading file)
          → Context switch
          ↓
5-10ms:   Thread 2 executes (processing Client B's request)
          Accesses shared global variable: request_counter++
          ↓
10ms:     Thread 2 blocks on network send
          → Context switch
          ↓
10-15ms:  Thread 3 executes (processing Client C's request)
          Also accesses: request_counter++  ← DANGER!
          ↓
15ms:     Disk I/O complete → Thread 1 wakes up
          Context switch to Thread 1
          ↓
15-20ms:  Thread 1 continues, sends response
          [Cycle continues...]


COMMUNICATION IS EASY (But Dangerous!):
────────────────────────────────────────

// Shared global variable
int request_counter = 0;

Thread 1:                    Thread 2:
request_counter++;           request_counter++;
↓                           ↓
Both access same memory!
No IPC mechanism needed
But... RACE CONDITION possible!
```

### Thread Communication

```
THREAD COMMUNICATION PATTERNS
==============================

1. SHARED VARIABLES (Direct Memory Access):
   ────────────────────────────────────────
   
   Global variable: int shared_data = 0;
   
   Thread 1:                Thread 2:
   shared_data = 42;        int x = shared_data;
   
   ✓ Fast (no syscalls)
   ✗ Requires synchronization
   ✗ Race conditions possible


2. THREAD-SAFE QUEUES:
   ────────────────────
   
   ┌──────────────────────────────┐
   │  Producer-Consumer Queue     │
   │  [task1][task2][task3]       │
   └──────────────────────────────┘
          ↑              ↓
     Producer        Consumer
     Thread          Thread
   
   Producer adds work items
   Consumer processes them
   Queue handles synchronization


3. CONDITION VARIABLES:
   ─────────────────────
   
   Thread 1 (waits):
     pthread_mutex_lock(&mutex);
     while (!data_ready)
       pthread_cond_wait(&cond, &mutex);
     process_data();
     pthread_mutex_unlock(&mutex);
   
   Thread 2 (signals):
     pthread_mutex_lock(&mutex);
     data_ready = true;
     pthread_cond_signal(&cond);
     pthread_mutex_unlock(&mutex);
   
   Efficient waiting (thread sleeps, no CPU waste)


4. MESSAGE PASSING (Between threads):
   ───────────────────────────────────
   
   Thread 1 → [Message] → Mailbox → Thread 2
   
   Can use queues, channels, or mailboxes
   Safer than shared memory
   More overhead than direct memory access
```

---

## Problems Created by Concurrency

Concurrent access to shared resources creates serious problems. Let's explore each in detail:

### Problem 1: Race Conditions

A race condition occurs when multiple threads/processes access shared data simultaneously, and the final result depends on the unpredictable timing of their execution.

```
RACE CONDITION: THE CLASSIC EXAMPLE
====================================

Shared Variable: int balance = 1000;

Thread A: Withdraw $500        Thread B: Withdraw $500
────────────────────────────────────────────────────────

Code both threads execute:
──────────────────────────

void withdraw(int amount) {
    int temp = balance;           // Read
    if (temp >= amount) {
        temp = temp - amount;     // Modify
        balance = temp;           // Write
    }
}


DANGEROUS INTERLEAVING:
───────────────────────

Time  Thread A                    Thread B
────  ─────────────────────────────────────────────────
 1    temp = balance;             
      (temp = 1000)
      
 2                                temp = balance;
                                  (temp = 1000)  ← Still sees 1000!
      
 3    if (temp >= 500) → TRUE
      
 4                                if (temp >= 500) → TRUE
      
 5    temp = 1000 - 500;
      (temp = 500)
      
 6                                temp = 1000 - 500;
                                  (temp = 500)
      
 7    balance = 500;
      
 8                                balance = 500;  ← OVERWRITES!

RESULT: balance = 500
EXPECTED: balance = 0

PROBLEM: $1000 withdrawn, but only $500 deducted!
         The bank just lost $500!


ASSEMBLY LEVEL (What CPU actually does):
─────────────────────────────────────────

Thread A:                    Thread B:
MOV R1, [balance]           MOV R1, [balance]    ← Both read 1000
SUB R1, 500                 SUB R1, 500          ← Both compute 500
MOV [balance], R1           MOV [balance], R1    ← Last write wins!

The operations are NOT ATOMIC!
(Multiple CPU instructions = interruptible)
```

### Problem 2: Deadlock

Deadlock occurs when two or more threads are waiting for each other to release resources, creating a circular dependency that prevents any progress.

```
DEADLOCK: THE DINING PHILOSOPHERS
==================================

Classic problem: 5 philosophers, 5 forks, need 2 forks to eat

┌──────────────────────────────────────┐
│                                      │
│         Fork 0                       │
│           ║                          │
│    Phil 0 ╬ Phil 1                   │
│           ║                          │
│  Fork 4 ══╬══ Fork 1                 │
│           ║                          │
│    Phil 4 ╬ Phil 2                   │
│           ║                          │
│  Fork 3 ══╬══ Fork 2                 │
│           ║                          │
│         Phil 3                       │
│                                      │
└──────────────────────────────────────┘

Each philosopher: think → pick up left fork → pick up right fork → eat

DEADLOCK SCENARIO:
──────────────────

Time 0: All philosophers pick up left fork simultaneously
  Phil 0: Holds Fork 0, waits for Fork 1
  Phil 1: Holds Fork 1, waits for Fork 2
  Phil 2: Holds Fork 2, waits for Fork 3
  Phil 3: Holds Fork 3, waits for Fork 4
  Phil 4: Holds Fork 4, waits for Fork 0
  
CIRCULAR WAIT:
  Phil 0 → waits for Phil 1
  Phil 1 → waits for Phil 2
  Phil 2 → waits for Phil 3
  Phil 3 → waits for Phil 4
  Phil 4 → waits for Phil 0
  
Nobody can proceed! DEADLOCK!


REAL CODE EXAMPLE:
──────────────────

pthread_mutex_t lock_A, lock_B;

Thread 1:                      Thread 2:
pthread_mutex_lock(&lock_A);   pthread_mutex_lock(&lock_B);
pthread_mutex_lock(&lock_B);   pthread_mutex_lock(&lock_A);
// Use both resources          // Use both resources
pthread_mutex_unlock(&lock_B); pthread_mutex_unlock(&lock_A);
pthread_mutex_unlock(&lock_A); pthread_mutex_unlock(&lock_B);

DEADLOCK TIMELINE:
──────────────────

Time  Thread 1                Thread 2
────  ─────────────────────────────────────────────
 1    Acquires lock_A ✓
 
 2                            Acquires lock_B ✓
 
 3    Tries to acquire lock_B
      (BLOCKED - Thread 2 has it)
      
 4                            Tries to acquire lock_A
                              (BLOCKED - Thread 1 has it)

STUCK FOREVER!
Thread 1 waits for Thread 2
Thread 2 waits for Thread 1


DEADLOCK CONDITIONS (All 4 must be present):
─────────────────────────────────────────────

1. Mutual Exclusion: Resources can't be shared
2. Hold and Wait: Thread holds resources while waiting for more
3. No Preemption: Resources can't be forcibly taken away
4. Circular Wait: Circular chain of waiting threads
```

### Problem 3: Starvation

Starvation occurs when a thread is perpetually denied access to resources because other threads keep getting priority.

```
STARVATION EXAMPLE
==================

Scenario: Readers-Writers Problem
  - Multiple threads can read simultaneously
  - Only one thread can write (exclusive access)
  - Writers have higher priority

┌────────────────────────────────────────────┐
│  Shared Resource: Database                 │
└────────────────────────────────────────────┘
     ↑       ↑       ↑          ↑
     │       │       │          │
  Reader   Reader  Reader    Writer
  Thread1  Thread2 Thread3   Thread4

Timeline:
─────────

0ms:   Reader 1 starts reading
5ms:   Reader 2 starts reading (can share)
10ms:  Writer arrives (must wait for readers)
12ms:  Reader 3 starts reading (can share)
15ms:  Reader 1 finishes
18ms:  Reader 4 arrives, starts reading
20ms:  Reader 2 finishes
25ms:  Reader 3 finishes
26ms:  Reader 5 arrives, starts reading
...    [More readers keep arriving]

Writer Thread: STILL WAITING!

If readers continuously arrive, writer starves forever
Never gets chance to write!


PRIORITY INVERSION (Another form):
──────────────────────────────────

Low Priority Thread:  Holds lock
Medium Priority Thread: CPU-intensive work
High Priority Thread: Waiting for lock held by Low Priority

Timeline:
─────────
0ms:   Low Priority acquires lock
5ms:   High Priority tries to acquire lock (BLOCKED)
6ms:   Medium Priority becomes runnable
       Medium Priority runs (preempts Low Priority)
       
High Priority thread waits for Low Priority
But Low Priority can't run (Medium Priority hogging CPU)
High Priority effectively runs at Medium Priority!

Famous bug: Mars Pathfinder (1997)
  High-priority thread starved
  System watchdog reset the spacecraft
  Fixed by priority inheritance protocol
```

### Problem 4: Data Corruption

When multiple threads access shared data without proper synchronization, data can become corrupted.

```
DATA CORRUPTION EXAMPLE
========================

struct BankAccount {
    char name[32];
    int balance;
    int transaction_count;
};

struct BankAccount account = {"Alice", 1000, 0};

Thread 1: Deposit $500         Thread 2: Display account
──────────────────────────────────────────────────────────

Thread 1 code:
strcpy(account.name, "Alice Johnson");  // Update name
account.balance += 500;                 // Update balance
account.transaction_count++;            // Update count

Thread 2 code:
printf("Name: %s\n", account.name);
printf("Balance: $%d\n", account.balance);
printf("Transactions: %d\n", account.transaction_count);


CORRUPTION SCENARIO:
────────────────────

Time  Thread 1                    Thread 2
────  ─────────────────────────────────────────────────
 1    strcpy: "Alice Jo..."
      (halfway through)
      
 2                                printf("Name: %s\n", ...)
                                  → Prints: "Alice Jo"  ← Incomplete!
      
 3    strcpy: "...hnson"
      (completes)
      
 4    balance = 1000 + 500;
      (balance = 1500)
      
 5                                printf("Balance: %d\n", ...)
                                  → Prints: 1500
      
 6    transaction_count++;
      (count = 1)
      
 7                                printf("Transactions: %d\n", ...)
                                  → Prints: 1

OUTPUT:
───────
Name: Alice Jo          ← CORRUPTED (partial name)
Balance: $1500
Transactions: 1

Data read in inconsistent state!


WORSE CORRUPTION: Linked List
──────────────────────────────

struct Node {
    int data;
    struct Node* next;
};

List: [A] → [B] → [C] → NULL

Thread 1: Insert X after A     Thread 2: Delete B
──────────────────────────────────────────────────

Thread 1:
  X->next = A->next;  // X->next = B
  A->next = X;        // A->next = X

Thread 2:
  A->next = B->next;  // A->next = C
  free(B);

DANGEROUS INTERLEAVING:
───────────────────────

Time  Thread 1              Thread 2
────  ──────────────────────────────────────
 1    X->next = A->next;
      (X->next = B)
      
 2                          A->next = B->next;
                            (A->next = C)
                            
 3                          free(B);  ← B deleted!
      
 4    A->next = X;
      (A->next = X)

RESULT:
───────
List: [A] → [X] → [B (FREED!)] → ???

X points to freed memory!
Accessing X->next causes:
  - Segmentation fault
  - Undefined behavior
  - Security vulnerability
  - Data corruption

This is a DANGLING POINTER bug!
```

---

## Solutions to Concurrency Problems

### Solution 1: Mutual Exclusion (Mutexes/Locks)

Mutexes ensure only one thread can access a critical section at a time.

```
MUTEX SOLUTION
==============

pthread_mutex_t account_lock = PTHREAD_MUTEX_INITIALIZER;
int balance = 1000;

Thread A: Withdraw $500        Thread B: Withdraw $500
────────────────────────────────────────────────────────

void withdraw(int amount) {
    pthread_mutex_lock(&account_lock);  // ◄── LOCK
    
    // === CRITICAL SECTION ===
    int temp = balance;
    if (temp >= amount) {
        temp = temp - amount;
        balance = temp;
    }
    // === END CRITICAL SECTION ===
    
    pthread_mutex_unlock(&account_lock);  // ◄── UNLOCK
}


PROTECTED EXECUTION:
────────────────────

Time  Thread A                    Thread B
────  ─────────────────────────────────────────────────
 1    pthread_mutex_lock(&lock)
      → SUCCESS (lock acquired)
      
 2                                pthread_mutex_lock(&lock)
                                  → BLOCKED (lock held by A)
      
 3    temp = balance; (temp=1000)
      
 4    temp = 1000 - 500;          [WAITING...]
      
 5    balance = 500;              [WAITING...]
      
 6    pthread_mutex_unlock(&lock)
      → Lock released
      
 7                                → Lock acquired! Resumes
      
 8                                temp = balance; (temp=500)
      
 9                                if (500 >= 500) → TRUE
      
10                                temp = 500 - 500;
      
11                                balance = 0;
      
12                                pthread_mutex_unlock(&lock)

RESULT: balance = 0 ✓ CORRECT!


HOW MUTEX WORKS (Under the Hood):
──────────────────────────────────

Mutex structure:
struct mutex {
    int locked;        // 0 = unlocked, 1 = locked
    thread_t owner;    // Which thread owns it
    queue_t waiters;   // Threads waiting for lock
};

pthread_mutex_lock():
1. Try to atomically set locked = 1
   (Using atomic CPU instruction: CMPXCHG or similar)
2. If successful → Return (got lock)
3. If failed → Add thread to waiters queue
             → Block thread (state: BLOCKED)
             → Context switch to another thread

pthread_mutex_unlock():
1. Set locked = 0
2. Check waiters queue
3. If threads waiting → Wake up first waiter
                      → That thread gets lock
4. Return
```

### Solution 2: Semaphores

Semaphores allow controlled access by multiple threads (counting semaphore) or enforce mutual exclusion (binary semaphore).

```
SEMAPHORE SOLUTION
==================

Binary Semaphore (acts like mutex):
───────────────────────────────────

sem_t semaphore;
sem_init(&semaphore, 0, 1);  // Initial value = 1

Thread code:
sem_wait(&semaphore);    // Decrement (1 → 0), enter
// Critical section
sem_post(&semaphore);    // Increment (0 → 1), exit


Counting Semaphore (resource pool):
────────────────────────────────────

Scenario: Database connection pool (max 3 connections)

sem_t db_connections;
sem_init(&db_connections, 0, 3);  // 3 available connections

Thread 1:
sem_wait(&db_connections);  // count: 3 → 2
use_database();
sem_post(&db_connections);  // count: 2 → 3

Thread 2:
sem_wait(&db_connections);  // count: 2 → 1
use_database();
sem_post(&db_connections);  // count: 1 → 2

Thread 3:
sem_wait(&db_connections);  // count: 1 → 0
use_database();
sem_post(&db_connections);  // count: 0 → 1

Thread 4:
sem_wait(&db_connections);  // count: 0 → BLOCKS!
                            // (waits for someone to finish)


PRODUCER-CONSUMER PROBLEM:
──────────────────────────

Buffer: [slot1][slot2][slot3][slot4][slot5]

sem_t empty_slots = 5;  // Initially all empty
sem_t full_slots = 0;   // Initially none full
pthread_mutex_t buffer_lock;

Producer Thread:
────────────────
while (1) {
    item = produce_item();
    
    sem_wait(&empty_slots);      // Wait for empty slot
    pthread_mutex_lock(&buffer_lock);
    
    add_to_buffer(item);
    
    pthread_mutex_unlock(&buffer_lock);
    sem_post(&full_slots);       // Signal: one more full
}

Consumer Thread:
────────────────
while (1) {
    sem_wait(&full_slots);       // Wait for full slot
    pthread_mutex_lock(&buffer_lock);
    
    item = remove_from_buffer();
    
    pthread_mutex_unlock(&buffer_lock);
    sem_post(&empty_slots);      // Signal: one more empty
    
    consume_item(item);
}

This prevents:
- Producer adding to full buffer
- Consumer removing from empty buffer
- Race conditions on buffer access
```

### Solution 3: Atomic Operations

Atomic operations execute as a single, indivisible unit—they cannot be interrupted.

```
ATOMIC OPERATIONS
=================

Problem: Counter increment
──────────────────────────

NON-ATOMIC (3 instructions):
int counter = 0;

counter++;  // Actually:
            // 1. LOAD counter into register
            // 2. INCREMENT register
            // 3. STORE register to counter
            // ← Can be interrupted between any step!

ATOMIC (1 indivisible operation):
atomic_int counter = 0;

atomic_fetch_add(&counter, 1);  // Atomic increment
                                 // ← Cannot be interrupted!


COMMON ATOMIC OPERATIONS:
─────────────────────────

1. atomic_load() - Atomic read
2. atomic_store() - Atomic write
3. atomic_fetch_add() - Atomic add and return old value
4. atomic_fetch_sub() - Atomic subtract
5. atomic_compare_exchange() - Compare-and-swap (CAS)


COMPARE-AND-SWAP (CAS) - The Foundation:
─────────────────────────────────────────

bool CAS(int* ptr, int expected, int new_value) {
    atomically {
        if (*ptr == expected) {
            *ptr = new_value;
            return true;
        }
        return false;
    }
}

Lock-free increment using CAS:
───────────────────────────────

void atomic_increment(atomic_int* counter) {
    int old_value, new_value;
    do {
        old_value = atomic_load(counter);
        new_value = old_value + 1;
    } while (!CAS(counter, old_value, new_value));
}

How it works:
1. Read current value
2. Calculate new value
3. Try to swap if value unchanged
4. If changed (another thread modified it), retry
5. Eventually succeeds


PERFORMANCE COMPARISON:
───────────────────────

Mutex:
  - Lock acquisition: ~25-100 ns (uncontended)
  - Lock acquisition: ~1-10 µs (contended)
  - Context switch if blocked

Atomic operations:
  - Atomic increment: ~1-5 ns
  - No context switch
  - Lock-free (always makes progress)

Atomic operations are ~10-1000x faster!
Use for simple operations (counters, flags)
Use mutexes for complex critical sections
```

### Solution 4: Deadlock Prevention

```
DEADLOCK PREVENTION STRATEGIES
===============================

Strategy 1: Lock Ordering
──────────────────────────

Problem:
  Thread 1: Lock A → Lock B
  Thread 2: Lock B → Lock A  ← DEADLOCK!

Solution: Always acquire locks in same order

RULE: Always lock in ascending order by address

Thread 1:                    Thread 2:
pthread_mutex_lock(&lock_A); pthread_mutex_lock(&lock_A);
pthread_mutex_lock(&lock_B); pthread_mutex_lock(&lock_B);

// Use resources           // Use resources

pthread_mutex_unlock(&lock_B); pthread_mutex_unlock(&lock_B);
pthread_mutex_unlock(&lock_A); pthread_mutex_unlock(&lock_A);

Now no circular wait possible!


Strategy 2: Try-Lock (Timeout)
───────────────────────────────

if (pthread_mutex_trylock(&lock_A) == 0) {
    // Got lock A
    if (pthread_mutex_trylock(&lock_B) == 0) {
        // Got both locks
        // Do work
        pthread_mutex_unlock(&lock_B);
    } else {
        // Couldn't get lock B, release A and retry
        pthread_mutex_unlock(&lock_A);
    }
}

Avoids holding one lock while waiting for another


Strategy 3: Lock Timeout
─────────────────────────

struct timespec timeout;
timeout.tv_sec = time(NULL) + 5;  // 5 second timeout

if (pthread_mutex_timedlock(&lock, &timeout) != 0) {
    // Timeout! Detect potential deadlock
    handle_error();
}


Strategy 4: Deadlock Detection
───────────────────────────────

Build a resource allocation graph:
- Nodes: Threads and Locks
- Edges: Thread → Lock (waiting)
         Lock → Thread (holding)

Periodically check for cycles:
  Thread A → Lock 1 → Thread B → Lock 2 → Thread A
            └────────── CYCLE = DEADLOCK ──────────┘

If detected:
  - Abort one thread
  - Or force release of locks


Strategy 5: Dining Philosophers Solution
─────────────────────────────────────────

Original
```