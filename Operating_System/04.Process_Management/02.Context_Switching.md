# Deep Dive into Context Switching: The Heart of Multitasking

## Introduction to Context Switching

Context switching is one of the most fundamental mechanisms that makes modern computing possible. It's the technique that allows your computer to run hundreds of programs seemingly at the same time, even though you might only have 4 or 8 CPU cores. When you're listening to music, browsing the web, editing a document, and downloading files simultaneously, context switching is what makes this magical illusion of parallelism work.

Imagine you're a chef in a kitchen working on multiple dishes at once. You can't actually cook everything simultaneously—you're one person with two hands. But you can switch rapidly between tasks: stir the soup, check the oven, chop vegetables, season the meat, back to stirring the soup. By rapidly switching between tasks, all dishes progress toward completion. Context switching is exactly this—the CPU rapidly switches between different programs (processes) or threads, giving each one a small slice of time, creating the illusion that everything is running at once.

The term "context" refers to the complete state of a process or thread at any given moment—all the information needed to pause execution and resume it later exactly where it left off. Think of it like saving your progress in a video game: you save your position, inventory, health, quest status, and when you reload, you continue exactly where you stopped. Context switching is the CPU's way of "saving the game" for one process and "loading the game" for another.

```
THE ILLUSION OF SIMULTANEOUS EXECUTION
=======================================

What User Sees (Apparent Parallelism):
──────────────────────────────────────

Music Player:  [♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪]
Web Browser:   [■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■]
Text Editor:   [▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓]
File Download: [████████████████████████████████]

All appear to run simultaneously!


What Actually Happens (Single CPU Core):
─────────────────────────────────────────

Time →
0ms        10ms       20ms       30ms       40ms       50ms
├──────────┼──────────┼──────────┼──────────┼──────────┤
│  Music   │ Browser  │  Editor  │ Download │  Music   │ Browser
│  Player  │          │          │          │  Player  │
└──────────┴──────────┴──────────┴──────────┴──────────┴────────

CPU rapidly switches between processes!
Each gets a time slice (quantum), typically 1-100ms

During each time slice:
  - Process executes its instructions
  - Makes progress on its task
  - When time expires → CONTEXT SWITCH
  - Next process gets CPU time
  
Result: All processes make forward progress
        User perceives simultaneous execution
```

---

## Why Do We Need Context Switching?

### The Fundamental Problem: Limited Resources

Modern computers run dozens or hundreds of processes simultaneously. A typical system might have:

- 200+ processes running
- 4-16 CPU cores
- Each core can only execute one thread at a time

Without context switching, you'd have two terrible options:

**Option 1: Run processes sequentially** (Ancient "batch processing" systems)

```
SEQUENTIAL EXECUTION (No Context Switching)
============================================

Process A: [████████████████████] Complete (2 hours)
Process B:                        [█████████████] Complete (1 hour)  
Process C:                                       [████] Complete (20 min)

Total time: 3 hours 20 minutes
User experience: TERRIBLE
- Start Process A, wait 2 hours
- Start Process B, wait 1 hour  
- Start Process C, wait 20 minutes
- No interactivity, no responsiveness
- One slow process blocks everything
```

**Option 2: One process per CPU core permanently** (No switching)

```
ONE PROCESS PER CORE (No Context Switching)
============================================

4 CPU Cores:
Core 0: [Process A] ← Running forever
Core 1: [Process B] ← Running forever
Core 2: [Process C] ← Running forever
Core 3: [Process D] ← Running forever

Processes E, F, G, H... Z: WAITING FOREVER
- Only 4 processes can run
- Other 196 processes starve
- Terrible resource utilization
```

### Context Switching Solves These Problems

Context switching enables **time-sharing**: the CPU time is divided among all processes, giving each a fair share. This provides:

**1. Multitasking**: Multiple programs can run "simultaneously" from the user's perspective.

**2. Responsiveness**: Interactive programs (like text editors) get frequent CPU time, so they respond immediately to user input even while background tasks run.

**3. Fair Resource Allocation**: All processes get CPU time based on priority, not just the first four.

**4. Efficient CPU Utilization**: When one process waits for I/O (disk, network), another process can use the CPU. The CPU never sits idle unnecessarily.

**5. Isolation**: Each process runs in isolation. If one crashes, others continue running.

```
WITH CONTEXT SWITCHING (Modern Systems)
========================================

Time →
    ┌────┬────┬────┬────┬────┬────┬────┬────┐
CPU │ A  │ B  │ C  │ D  │ E  │ A  │ B  │ C  │
    └────┴────┴────┴────┴────┴────┴────┴────┘
    10ms each time slice

Benefits:
─────────
✓ All processes make progress
✓ Interactive processes respond quickly
✓ I/O-bound processes don't waste CPU time
✓ Fair allocation of CPU resources
✓ System remains responsive even under load

Example: Text Editor Response Time
───────────────────────────────────
Without context switching: 
  User presses 'a' → waits 2 hours for heavy process to finish → 'a' appears
  
With context switching:
  User presses 'a' → editor gets CPU within 10-50ms → 'a' appears instantly
  (Even while video encoding, file compression, etc. run in background)
```

---

## What is "Context"?

Before understanding context switching, we need to understand what the "context" actually contains. The context is the complete snapshot of a process or thread's state—everything needed to pause and resume execution.

### Process Context (Complete State)

```
COMPLETE PROCESS CONTEXT
=========================

┌─────────────────────────────────────────────────────────┐
│              PROCESS CONTEXT                             │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  1. CPU EXECUTION CONTEXT                               │
│     ┌────────────────────────────────────────┐          │
│     │ Program Counter (PC): 0x00401A5C       │          │
│     │ Stack Pointer (SP):   0x7FFFFFFF00     │          │
│     │ Base Pointer (BP):    0x7FFFFFFF80     │          │
│     │                                        │          │
│     │ General Purpose Registers:             │          │
│     │   RAX: 0x0000000000000042              │          │
│     │   RBX: 0x00007F8A2C100000              │          │
│     │   RCX: 0x0000000000000000              │          │
│     │   RDX: 0x00007FFFFFFFE8                │          │
│     │   RSI: 0x00007FFFFFFFE0                │          │
│     │   RDI: 0x0000000000000001              │          │
│     │   ... (R8-R15 on x86-64)               │          │
│     │                                        │          │
│     │ Flags Register (RFLAGS): 0x0246        │          │
│     │   [Zero Flag, Carry Flag, etc.]        │          │
│     │                                        │          │
│     │ Floating Point Registers:              │          │
│     │   XMM0-XMM15 (128-bit SIMD)            │          │
│     │   FPU state (x87 registers)            │          │
│     └────────────────────────────────────────┘          │
│                                                         │
│  2. MEMORY MANAGEMENT CONTEXT                           │
│     ┌────────────────────────────────────────┐          │
│     │ Page Table Base Register (CR3):        │          │
│     │   Physical Address: 0x0012A000         │          │
│     │   → Points to process's page table     │          │
│     │                                        │          │
│     │ Memory Segments:                       │          │
│     │   Code:  0x00400000 - 0x00450000       │          │
│     │   Data:  0x00600000 - 0x00610000       │          │
│     │   Heap:  0x01000000 - 0x02000000       │          │
│     │   Stack: 0x7FFFFFFF00 - 0x7FFFFFFFF    │          │
│     │                                        │          │
│     │ Translation Lookaside Buffer (TLB):    │          │
│     │   [Cached page translations]           │          │
│     └────────────────────────────────────────┘          │
│                                                         │
│  3. PROCESS METADATA                                    │
│     ┌────────────────────────────────────────┐          │
│     │ Process ID (PID):         1847         │          │
│     │ Parent Process ID (PPID): 1623         │          │
│     │ User ID (UID):            1000         │          │
│     │ Group ID (GID):           1000         │          │
│     │ Process State:            RUNNING      │          │
│     │ Priority:                 20           │          │
│     │ Nice Value:               0            │          │
│     └────────────────────────────────────────┘          │
│                                                         │
│  4. SCHEDULING INFORMATION                              │
│     ┌────────────────────────────────────────┐          │
│     │ CPU Time Used:         1247 ms         │          │
│     │ Time Quantum Remaining: 8 ms           │          │
│     │ Wait Queue:            NULL            │          │
│     │ Sleep Time:            0               │          │
│     │ Last CPU Used:         2               │          │
│     └────────────────────────────────────────┘          │
│                                                         │
│  5. RESOURCE INFORMATION                                │
│     ┌────────────────────────────────────────┐          │
│     │ Open File Descriptors:                 │          │
│     │   FD 0: stdin                          │          │
│     │   FD 1: stdout                         │          │
│     │   FD 2: stderr                         │          │
│     │   FD 3: /home/user/data.txt            │          │
│     │   FD 4: socket (192.168.1.100:8080)    │          │
│     │                                        │          │
│     │ Signal Handlers:                       │          │
│     │   SIGINT:  → custom_handler()          │          │
│     │   SIGTERM: → default                   │          │
│     │                                        │          │
│     │ Environment Variables:                 │          │
│     │   PATH=/usr/bin:/bin                   │          │
│     │   HOME=/home/user                      │          │
│     └────────────────────────────────────────┘          │
│                                                         │
│  6. KERNEL STACK                                        │
│     (Used when process makes system calls)              │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### Thread Context (Lighter Weight)

```
THREAD CONTEXT (Lighter than Process)
======================================

┌─────────────────────────────────────────────────────────┐
│              THREAD CONTEXT                             │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  1. CPU EXECUTION CONTEXT (PRIVATE)                     │
│     ┌────────────────────────────────────────┐          │
│     │ Program Counter (PC): 0x00401A5C       │          │
│     │ Stack Pointer (SP):   0x7FFEF8A00      │ ◄─ Different!
│     │ All Registers (RAX-R15, etc.)          │          │
│     │ Flags Register                         │          │
│     └────────────────────────────────────────┘          │
│                                                         │
│  2. THREAD-SPECIFIC INFO (PRIVATE)                      │
│     ┌────────────────────────────────────────┐          │
│     │ Thread ID (TID):          12847        │          │
│     │ Thread State:             RUNNING      │          │
│     │ Thread Priority:          20           │          │
│     │ Thread-Local Storage:     0x7000B000   │          │
│     │ Signal Mask:              [blocked]    │          │
│     └────────────────────────────────────────┘          │
│                                                         │
│  3. SHARED WITH OTHER THREADS (Not switched)            │
│     ┌────────────────────────────────────────┐          │
│     │ ✗ Memory (Page Table) - SHARED         │          │
│     │ ✗ Open Files - SHARED                  │          │
│     │ ✗ Parent Process - SHARED              │          │
│     │ ✗ Heap Memory - SHARED                 │          │
│     │ ✗ Code Segment - SHARED                │          │
│     └────────────────────────────────────────┘          │
│                                                         │
└─────────────────────────────────────────────────────────┘

KEY DIFFERENCE:
───────────────
Process context switch: Must save AND switch memory context
Thread context switch:  Only save execution context (faster!)
```

---
# TLB Flush Explained

## What is TLB?

**TLB (Translation Lookaside Buffer)** is a small, extremely fast cache inside the CPU that stores recent virtual-to-physical address translations. Think of it as a "cheat sheet" for memory address lookups.

```
WITHOUT TLB:
Every memory access needs page table walk:
  Virtual Address → Page Table (in RAM) → Physical Address
  Cost: ~100-200 CPU cycles

WITH TLB:
Recent translations cached:
  Virtual Address → TLB (instant!) → Physical Address
  Cost: ~1 cycle (hit rate: 95-99%)
```

## What is TLB Flush?

**TLB Flush** = Clearing/invalidating all entries in the TLB cache.

```
BEFORE FLUSH (Process A running):
┌─────────────────────────────────┐
│         TLB Cache               │
├─────────────────────────────────┤
│ Virtual → Physical              │
│ 0x401000 → 0x12A000  (Process A)│
│ 0x402000 → 0x12B000  (Process A)│
│ 0x7FFF000 → 0x15C000 (Process A)│
│ ... (64-512 entries)            │
└─────────────────────────────────┘

AFTER FLUSH (Switching to Process B):
┌─────────────────────────────────┐
│         TLB Cache               │
├─────────────────────────────────┤
│ [EMPTY]                         │
│ [EMPTY]                         │
│ [EMPTY]                         │
│ ... All entries invalidated     │
└─────────────────────────────────┘
```

## Why TLB Flush is Needed

When switching between **processes** (not threads):

- Process A's virtual address 0x401000 → Physical 0x12A000
- Process B's virtual address 0x401000 → Physical 0x8F3000 (different!)

If TLB wasn't flushed, Process B would use Process A's translations = **WRONG MEMORY ACCESS!**

## The Cost

After TLB flush:

- Next 50-100 memory accesses = TLB miss
- Each miss requires slow page table walk (~100-200 cycles)
- Total cost: **~2-10 microseconds** of slowdown

## Thread vs Process

```
PROCESS SWITCH:
- Different memory spaces
- MUST flush TLB ✗ (expensive!)

THREAD SWITCH:
- Same memory space
- NO TLB flush needed ✓ (much faster!)
```

**This is the main reason thread context switches are 3-5x faster than process switches!**
## How Context Switching Works: Step-by-Step

### Process Context Switch (Detailed Mechanics)

Let's trace a complete process context switch from Process A to Process B:

```
PROCESS CONTEXT SWITCH SEQUENCE
================================

Initial State: Process A is running on CPU Core 0
───────────────────────────────────────────────────

┌──────────────────────────────────────┐
│       CPU CORE 0                     │
├──────────────────────────────────────┤
│ Currently Executing: Process A       │
│                                      │
│ PC:     0x00401A5C                   │
│ SP:     0x7FFFFFFF00                 │
│ RAX:    0x42                         │
│ RBX:    0x7F8A2C100000               │
│ ... (all registers have Process A's values)
│                                      │
│ CR3 (Page Table): → Process A's page table
└──────────────────────────────────────┘

Process A is executing:
  MOV RAX, [RBX+8]     ← Currently executing this instruction
  ADD RAX, 0x10
  ...


TRIGGER: Context Switch Event
──────────────────────────────

Three common triggers:
1. Timer Interrupt (time quantum expired)
2. System Call (process requests I/O)
3. Interrupt (hardware device needs attention)

Let's say: Timer Interrupt fires!


STEP 1: HARDWARE INTERRUPT HANDLING (Hardware level)
─────────────────────────────────────────────────────

1.1 Timer chip sends interrupt signal to CPU
    
1.2 CPU finishes current instruction (MOV RAX, [RBX+8])
    
1.3 CPU automatically:
    - Disables interrupts (prevents nested interrupts)
    - Pushes current state to kernel stack:
      * RFLAGS register
      * CS (Code Segment)
      * RIP (Instruction Pointer/PC)
      
    - Looks up Interrupt Descriptor Table (IDT)
    - Jumps to Timer Interrupt Handler address
    
1.4 CPU switches to kernel mode (Ring 0)

    ┌─────────────────────────────────────┐
    │   CPU switches from:                │
    │   User Mode (Ring 3) →              │
    │   Kernel Mode (Ring 0)              │
    │                                     │
    │   Now executing OS kernel code      │
    └─────────────────────────────────────┘


STEP 2: SAVE PROCESS A'S CONTEXT (OS Kernel Code)
──────────────────────────────────────────────────

OS Timer Interrupt Handler executes:

2.1 Save all remaining CPU registers to Process A's PCB
    (The ones not saved by hardware automatically)
    
    Process_A_PCB.saved_state:
      .rax = CPU.RAX        // 0x42
      .rbx = CPU.RBX        // 0x7F8A2C100000
      .rcx = CPU.RCX
      .rdx = CPU.RDX
      .rsi = CPU.RSI
      .rdi = CPU.RDI
      .rbp = CPU.RBP
      .r8  = CPU.R8
      .r9  = CPU.R9
      ... (all 16 general-purpose registers)
      
      .rsp = CPU.RSP        // 0x7FFFFFFF00
      .rip = [saved by hardware on stack]
      .rflags = [saved by hardware on stack]
      
      .xmm0-xmm15 = [SIMD registers]
      .fpu_state = [x87 FPU state]

2.2 Save FPU and SIMD state (if used)
    - Modern CPUs have instructions: FXSAVE, XSAVE
    - Saves floating-point and vector register state
    
2.3 Update Process A's state
    Process_A_PCB.state = READY  (was RUNNING)
    Process_A_PCB.cpu_time_used += 10ms
    
2.4 Update statistics
    Process_A_PCB.last_cpu = 0
    Process_A_PCB.context_switches++


STEP 3: SCHEDULER DECISION (OS Scheduler)
──────────────────────────────────────────

3.1 Call scheduler to pick next process
    next_process = schedule()
    
    Scheduler Algorithm (Simplified):
    ──────────────────────────────────
    - Check ready queue for runnable processes
    - Consider priorities, fairness, CPU affinity
    - Select Process B
    
    Ready Queue before:
      [Process B, Process C, Process A (just added), Process D]
      
    Decision: Run Process B (highest priority in queue)
    
3.2 Remove Process B from ready queue
    
3.3 Update Process B's state
    Process_B_PCB.state = RUNNING
    Process_B_PCB.last_scheduled = current_time()


STEP 4: SWITCH MEMORY CONTEXT (Critical for processes!)
────────────────────────────────────────────────────────

4.1 Switch page tables (This is EXPENSIVE for processes!)
    
    Current: CR3 register → Process A's page table (0x0012A000)
    
    // Change to Process B's page table
    CPU.CR3 = Process_B_PCB.page_table_base  // 0x0015B000
    
    This single instruction:
    - Switches entire virtual memory space
    - Process B now sees its own memory
    - Process A's memory becomes inaccessible
    
4.2 Flush TLB (Translation Lookaside Buffer)
    - TLB caches virtual → physical address translations
    - Must flush because we're switching address spaces
    - Next memory accesses will be slower (TLB misses)
    - This is a MAJOR cost of process context switching!
    
    ┌──────────────────────────────────────────┐
    │  TLB FLUSH IMPACT                        │
    │  ────────────────────                    │
    │  Before: TLB has Process A's translations│
    │  After:  TLB is empty                    │
    │  Result: Next ~100 memory accesses are   │
    │          slower (must rebuild TLB cache) │
    │                                          │
    │  Cost: ~1000 cycles or more!             │
    └──────────────────────────────────────────┘


STEP 5: RESTORE PROCESS B'S CONTEXT
────────────────────────────────────

5.1 Load Process B's saved registers from PCB
    
    CPU.RAX = Process_B_PCB.saved_state.rax  // 0x1A2B3C4D
    CPU.RBX = Process_B_PCB.saved_state.rbx  // 0x00601040
    CPU.RCX = Process_B_PCB.saved_state.rcx
    ... (restore all 16 general-purpose registers)
    
    CPU.RSP = Process_B_PCB.saved_state.rsp  // 0x7FFFAAFF00
    
5.2 Restore FPU and SIMD state
    FXRSTOR Process_B_PCB.saved_state.fpu_state
    // or XRSTOR for newer CPUs
    
5.3 Restore flags
    CPU.RFLAGS = Process_B_PCB.saved_state.rflags


STEP 6: RETURN FROM INTERRUPT (Resume Execution)
─────────────────────────────────────────────────

6.1 OS executes return-from-interrupt instruction: IRET
    
6.2 Hardware automatically:
    - Pops saved values from kernel stack:
      * RIP (Process B's next instruction address)
      * CS (Code segment)
      * RFLAGS
    - Switches back to user mode (Ring 3 → Ring 0)
    - Re-enables interrupts
    - Jumps to Process B's next instruction
    
6.3 Process B resumes execution!
    Process B has NO IDEA it was ever paused!
    From Process B's perspective, it's been running continuously.


FINAL STATE: Process B is now running
──────────────────────────────────────

┌──────────────────────────────────────┐
│       CPU CORE 0                     │
├──────────────────────────────────────┤
│ Currently Executing: Process B       │
│                                      │
│ PC:     0x00502C14                   │
│ SP:     0x7FFFAAFF00                 │
│ RAX:    0x1A2B3C4D                   │
│ RBX:    0x00601040                   │
│ ... (all registers have Process B's values)
│                                      │
│ CR3 (Page Table): → Process B's page table
└──────────────────────────────────────┘

Process B continues executing from where it left off:
  CMP RAX, RBX     ← Executing this instruction now
  JE  0x00502C20
  ...
```

### Visual Timeline of Context Switch

```
CONTEXT SWITCH TIMELINE (Process A → Process B)
================================================

Time (microseconds) →
0    1    2    3    4    5    6    7    8    9    10
├────┼────┼────┼────┼────┼────┼────┼────┼────┼────┤

Process A executing:
[████████] 
          ↓
        Timer
      Interrupt
          ↓
Interrupt Handling:
          [■] Hardware pushes state
              ↓
Save Context:
              [■■] Save all registers to PCB
                  ↓
Scheduler:
                  [■] Pick next process
                    ↓
Switch Memory:
                    [■■] Change page table + TLB flush
                        ↓
Restore Context:
                        [■■] Load registers from PCB
                            ↓
Return from Interrupt:
                            [■] Hardware pops state
                              ↓
Process B executing:
                              [████████████████...]

Total Context Switch Time: ~3-7 microseconds
(On modern systems, varies by CPU architecture)

Breakdown:
──────────
- Interrupt handling:    ~0.5 µs
- Save context:          ~1 µs
- Scheduling decision:   ~0.5-2 µs
- Memory switch + TLB:   ~1-3 µs  ← Most expensive!
- Restore context:       ~1 µs
- Return overhead:       ~0.5 µs
```

---

## Thread Context Switch (Much Faster!)

Thread context switches are significantly faster than process switches because threads share memory:

```
THREAD CONTEXT SWITCH (Thread 1 → Thread 2, Same Process)
===========================================================

Initial State: Thread 1 (TID 1001) running
───────────────────────────────────────────

CPU executing Thread 1:
  PC: 0x00401A5C
  SP: 0x7FFF8420  ← Thread 1's stack
  Registers: [Thread 1's values]


STEP 1: Save Thread 1's Context
────────────────────────────────

thread_switch(Thread_1_TCB, Thread_2_TCB):
  
  // Save CPU state to Thread 1's TCB
  Thread_1_TCB.rip = CPU.RIP
  Thread_1_TCB.rsp = CPU.RSP        // 0x7FFF8420
  Thread_1_TCB.rbp = CPU.RBP
  Thread_1_TCB.rax = CPU.RAX
  Thread_1_TCB.rbx = CPU.RBX
  ... (all registers)
  Thread_1_TCB.rflags = CPU.RFLAGS
  
  Thread_1_TCB.state = READY


STEP 2: Scheduling Decision
────────────────────────────

  next_thread = schedule_thread()
  // Returns Thread 2 (TID 1002)
  
  Thread_2_TCB.state = RUNNING


STEP 3: NO MEMORY SWITCH! (This is the key difference!)
────────────────────────────────────────────────────────

  // SKIP: No page table switch needed!
  // CR3 register stays the same
  // Both threads share the same address space
  
  ✓ No TLB flush required!
  ✓ No cache pollution!
  ✓ Much faster!
  
  ┌───────────────────────────────────────────┐
  │  MEMORY REMAINS THE SAME                  │
  │  ─────────────────────────                │
  │  Code segment:   Still at 0x00400000      │
  │  Heap:           Still at 0x01000000      │
  │  Global data:    Still at 0x00600000      │
  │                                           │
  │  Only thing changing:                     │
  │  Stack pointer moves to Thread 2's stack  │
  └───────────────────────────────────────────┘


STEP 4: Restore Thread 2's Context
───────────────────────────────────

  // Load Thread 2's saved state
  CPU.RIP = Thread_2_TCB.rip
  CPU.RSP = Thread_2_TCB.rsp        // 0x7FFE9320 (different stack!)
  CPU.RBP = Thread_2_TCB.rbp
  CPU.RAX = Thread_2_TCB.rax
  CPU.RBX = Thread_2_TCB.rbx
  ... (all registers)
  CPU.RFLAGS = Thread_2_TCB.rflags


STEP 5: Resume Thread 2
────────────────────────

  return_to_userspace()
  
  Thread 2 continues executing from PC
  Using its own stack at 0x7FFE9320


Final State: Thread 2 running
──────────────────────────────

CPU executing Thread 2:
  PC: 0x00402B1E
  SP: 0x7FFE9320  ← Thread 2's stack
  Registers: [Thread 2's values]
  
Memory space: UNCHANGED (same process)


TIMING COMPARISON:
──────────────────

Process Context Switch: 3-7 microseconds
  - Register save/restore:  ~2 µs
  - Memory switch + TLB:    ~3-4 µs  ← Major cost
  - Scheduling:             ~1 µs

Thread Context Switch: 0.5-2 microseconds
  - Register save/restore:  ~1 µs
  - NO memory switch:       ~0 µs   ← Savings!
  - Scheduling:             ~0.5 µs

Thread switching is 3-5x FASTER!
```

### Why Thread Switches Are Faster: Detailed Comparison

```
COST BREAKDOWN: PROCESS VS THREAD CONTEXT SWITCH
=================================================

┌──────────────────────────────────────────────────────┐
│                  PROCESS SWITCH                      │
├──────────────────────────────────────────────────────┤
│                                                      │
│  Operations Required:                                │
│  ───────────────────                                 │
│  1. Save registers                    ~1 µs          │
│  2. Save FPU/SIMD state              ~0.5 µs         │
│  3. Update process state             ~0.2 µs         │
│  4. Run scheduler                    ~0.5-2 µs       │
│  5. Switch page table (CR3)          ~0.1 µs         │
│  6. Flush TLB                        ~1-3 µs ◄─ EXPENSIVE!
│  7. Restore registers                ~1 µs           │
│  8. Restore FPU/SIMD                 ~0.5 µs         │
│  9. Return from interrupt            ~0.2 µs         │
│                                                      │
│  Total: 5-9 microseconds                             │
│                                                      │
│  Indirect Costs:                                     │
│  ────────────────                                    │
│  - TLB misses for next 50-100 memory accesses        │
│  - Cache pollution (some of old process's            │
│    cache lines evicted for new process)              │
│  - Page table walk overhead                          │
│                                                      │
│  Total indirect cost: ~1000-5000 cycles              │
│                       (~0.5-2.5 µs additional)       │
│                                                      │
└──────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────┐
│                   THREAD SWITCH                      │
├──────────────────────────────────────────────────────┤
│                                                      │
│  Operations Required:                                │
│  ───────────────────                                 │
│  1. Save registers                    ~0.5 µs        │
│  2. Save FPU/SIMD state              ~0.3 µs         │
│  3. Update thread state              ~0.1 µs         │
│  4. Run scheduler                    ~0.3-1 µs       │
│  5. ✗ NO page table switch          ~0 µs ◄─ SAVED! │
│  6. ✗ NO TLB flush                  ~0 µs ◄─ SAVED! │
│  7. Restore registers                ~0.5 µs         │
│  8. Restore FPU/SIMD                 ~0.3 µs         │
│  9. Return                           ~0.1µs          │  
│                                                      │ 
│  Total: 2-3.5 microseconds                           │ 
│                                                      │ 
│  Indirect Costs:                                     │ 
│  ────────────────                                    │ 
│ - TLB: NO flush needed! ✓                            │ 
│ - Cache: Minimal pollution (same address space)      │ 
│ - Some cache contention between threads' stacks      │ 
│                                                      │ 
│ Total indirect cost: ~100-500 cycles                 │ 
│ (~0.05-0.25 µs)                                      │ 
│                                                      │ └──────────────────────────────────────────────────────┘

SPEEDUP: Thread switching is 2-4x faster than process switching!

```

---

## Context Switch Triggers

Context switches don't happen randomly. There are specific events that trigger them:

```

# CONTEXT SWITCH TRIGGERS

1. PREEMPTIVE SCHEDULING: Timer Interrupt (Time Quantum Expired) ─────────────────────────────────────────────────────────────
    
    Time → Process A: [Running for 10ms.........................] ← Quantum expires! ↓ Timer Interrupt ↓ Context Switch ↓ Process B: [Now running...]
    
    Details: ────────
    
    - Hardware timer (APIC timer) configured for periodic interrupts
    - Typical quantum: 1-100ms (varies by OS and scheduler)
    - Linux: CFS scheduler uses variable time slices
    - Windows: Typically 15.6ms quantum (64 Hz timer)
    - Ensures fairness: no process can hog CPU forever
    
    Timer fires: → CPU receives interrupt signal → Jumps to timer interrupt handler → Handler calls scheduler → Scheduler performs context switch
    
2. VOLUNTARY CONTEXT SWITCH: System Call (Process Yields or Blocks) ──────────────────────────────────────────────────────────────────
    
    Process A executing: read_file("data.txt"); ← Makes system call ↓ Trap to kernel ↓ Initiate disk I/O ↓ Process A: RUNNING → BLOCKED ↓ Context switch to Process B ↓ Process B: [Running while A waits for disk...]
    
    Common blocking calls: ──────────────────────
    
    - read(), write() - Wait for I/O
    - sleep() - Voluntary delay
    - wait() - Wait for child process
    - sem_wait() - Wait for semaphore
    - pthread_mutex_lock() - Wait for lock
    - recv() - Wait for network data
    
    Flow: ─────
    
    1. Process makes system call
    2. Kernel determines operation will block
    3. Kernel marks process as BLOCKED
    4. Kernel calls scheduler
    5. Scheduler picks different READY process
    6. Context switch occurs
    7. When I/O completes, interrupt wakes blocked process (moves it from BLOCKED → READY)
3. INTERRUPT HANDLING: Hardware Device Needs Attention ─────────────────────────────────────────────────────
    
    Process A: [Running........................] ↓ Disk interrupt! (I/O complete) ↓ Wake Process B (was waiting for disk) ↓ Process B now READY ↓ Scheduler may preempt A ↓ Process B: [Running...]
    
    Interrupt types: ────────────────
    
    - Disk controller: I/O operation complete
    - Network card: Packet arrived
    - Keyboard: Key pressed
    - Mouse: Movement detected
    - Timer: Periodic tick
    
    Flow: ─────
    
    1. Hardware device signals interrupt
    2. CPU suspends current process
    3. Jumps to interrupt handler
    4. Handler services device (e.g., copies network packet)
    5. Handler may wake up blocked processes
    6. If woken process has higher priority, context switch
    7. Otherwise, resume interrupted process
4. EXPLICIT YIELD: Process Voluntarily Gives Up CPU ──────────────────────────────────────────────────
    
    Process A: for (i = 0; i < 1000000; i++) { do_work(); sched_yield(); ← Explicitly yield CPU }
    
    Usage scenarios: ────────────────
    
    - Cooperative multitasking
    - Spin-wait loops (bad practice, but happens)
    - User-space threading libraries
    - Real-time systems
    
    Flow: ─────
    
    1. Process calls sched_yield()
    2. Kernel moves process to end of ready queue
    3. Kernel calls scheduler
    4. Context switch to next process
5. PROCESS/THREAD TERMINATION ────────────────────────────
    
    Process A: main() { ... return 0; ← Program exits }
    
    Flow: ─────
    
    1. Process calls exit() or returns from main
    2. Kernel performs cleanup:
        - Close file descriptors
        - Free memory
        - Send SIGCHLD to parent
        - Mark process as ZOMBIE
    3. Must context switch (no process to run!)
    4. Scheduler picks next READY process
6. PRIORITY CHANGE: Higher Priority Process Becomes Ready ─────────────────────────────────────────────────────────
    
    Low priority Process A: [Running....................] ↓ High priority Process B becomes READY (I/O done) ↓ Preempt A immediately! ↓ High priority Process B: [Running...]
    
    Used in: ────────
    
    - Real-time systems
    - Interactive vs batch processes
    - System processes vs user processes

```

---

CONTEXT SWITCHING COSTS (Detailed Analysis)
============================================
```
DIRECT COSTS:
─────────────

1. CPU Time for Switch Itself
   ───────────────────────────
   Process switch: 3-10 µs
   Thread switch:  0.5-3 µs
   
   This is "dead time" - no useful work done
   Just overhead of saving/restoring state
   
   Impact calculation:
   ──────────────────
   If context switch every 10ms (100 per second):
     Process switches: 100 × 5µs = 500µs = 0.05% overhead
     (Negligible on modern systems)
   
   But if switching every 100µs (10,000 per second):
     Process switches: 10,000 × 5µs = 50ms = 5% overhead!
     (Significant!)


2. TLB Flush Cost (Process switches only)
   ───────────────────────────────────────
   TLB holds ~64-512 cached address translations
   
   After flush:
     Next 50-100 memory accesses → TLB miss
     Each miss: ~100-200 cycles to walk page table
   
   Total cost: ~5,000-20,000 cycles
              ~2-10 µs on modern CPU
   
   This is why thread switches are much faster!


3. Cache Pollution
   ────────────────
   When switching processes:
     - Old process's data in cache becomes useless
     - New process's data not yet in cache
     - Cold cache = many cache misses initially
   
   L1 cache miss: ~4 cycles
   L2 cache miss: ~12 cycles
   L3 cache miss: ~40 cycles
   RAM access:    ~200 cycles
   
   After context switch:
     First ~1000 memory accesses slower
     Cost: ~50,000-200,000 cycles
           ~25-100 µs
   
   Thread switches: Less cache pollution
     (threads share code and global data)


INDIRECT COSTS:
───────────────

4. Pipeline Stalls
   ───────────────
   Modern CPUs have deep pipelines (14-20 stages)
   Context switch = pipeline flush
   
   Cost: ~20-40 wasted cycles per switch
        ~10-20 ns
   
   (Relatively minor on modern CPUs)


5. Branch Predictor Reset
   ───────────────────────
   Branch predictor learns program's behavior
   Context switch to different process:
     - Predictor state becomes irrelevant
     - Must relearn new process's patterns
   
   Cost: Higher branch misprediction rate
         for next ~1000-10000 branches
   
   Each misprediction: ~15-20 cycle penalty
   
   Total cost: Difficult to quantify
              ~0.1-1% performance impact


6. Scheduler Overhead
   ───────────────────
   Scheduler must decide which process/thread runs next
   
   Simple algorithms (Round Robin): ~0.5 µs
   Complex algorithms (CFS, O(1)): ~1-5 µs
   
   With many runnable processes (100+):
     Scheduling decision becomes expensive
     May need to scan multiple queues
     Cost can reach 10-50 µs


TOTAL COST EXAMPLE:
───────────────────

Process Context Switch (worst case):
  Direct switch time:        5 µs
  TLB refill:               8 µs
  Cache warming:           50 µs
  Pipeline/branch:          2 µs
  Scheduling:               3 µs
  ─────────────────────────────
  Total:                   68 µs

For 100 switches/second: 6.8ms = 0.68% overhead
For 1000 switches/second: 68ms = 6.8% overhead
For 10000 switches/second: 680ms = 68% overhead! (Thrashing)


Thread Context Switch (typical case):
  Direct switch time:        2 µs
  NO TLB flush:             0 µs
  Minimal cache impact:     5 µs
  Pipeline/branch:          1 µs
  Scheduling:               2 µs
  ─────────────────────────────
  Total:                   10 µs

For 100 switches/second: 1ms = 0.1% overhead
For 1000 switches/second: 10ms = 1% overhead
For 10000 switches/second: 100ms = 10% overhead


LESSON: Excessive context switching kills performance!
        This is called "thrashing"
        
```

---

## Context Switching in Multi-Core Systems

On multi-core systems, context switching becomes more complex:

```
MULTI-CORE CONTEXT SWITCHING
=============================

System: 4 CPU Cores
───────────────────────

┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐
│ Core 0  │  │ Core 1  │  │ Core 2  │  │ Core 3  │
├─────────┤  ├─────────┤  ├─────────┤  ├─────────┤
│Process A│  │Process B│  │Process C│  │Process D│
└─────────┘  └─────────┘  └─────────┘  └─────────┘

Each core can run a different process simultaneously!
No context switching needed if ≤ 4 processes

But what if we have 100 processes?


SCENARIO 1: Per-Core Run Queues
────────────────────────────────

Each core has its own scheduler and ready queue:

Core 0 Queue: [A, E, I, M, Q]
Core 1 Queue: [B, F, J, N, R]
Core 2 Queue: [C, G, K, O, S]
Core 3 Queue: [D, H, L, P, T]

Advantages:
  ✓ No lock contention (each core independent)
  ✓ Good cache locality (process stays on same core)
  ✓ Simple implementation

Disadvantages:
  ✗ Load imbalance possible
  ✗ One core may be busy while others idle


SCENARIO 2: Global Run Queue
─────────────────────────────

Single shared queue for all cores:

Global Queue: [A, B, C, D, E, F, G, H, ...]
               ↑   ↑   ↑   ↑
               │   │   │   │
           Core0 Core1 Core2 Core3

Advantages:
  ✓ Perfect load balancing
  ✓ No core sits idle if work available

Disadvantages:
  ✗ Lock contention (all cores access same queue)
  ✗ Poor cache locality (process migrates between cores)


SCENARIO 3: Hybrid Approach (Linux CFS)
────────────────────────────────────────

Per-core queues with periodic load balancing:

Initial:
  Core 0: [A, E, I]        ← 3 processes
  Core 1: [B, F]           ← 2 processes  
  Core 2: [C, G]           ← 2 processes
  Core 3: [D, H, J, K, L]  ← 5 processes (overloaded!)

Load balancer runs periodically (every 4ms):
  Detects Core 3 is overloaded
  Migrates process K to Core 1
  Migrates process L to Core 2

After balancing:
  Core 0: [A, E, I]  ← 3 processes
  Core 1: [B, F, K]  ← 3 processes
  Core 2: [C, G, L]  ← 3 processes
  Core 3: [D, H, J]  ← 3 processes (balanced!)


CPU AFFINITY:
─────────────

Processes can have CPU affinity (preferred cores):

Process A:
  - Ran on Core 0 recently
  - L1/L2 cache still has A's data
  - Scheduler prefers to run A on Core 0 again
  - Benefit: Warm cache, better performance

Cache affinity conflict:
  If Process A must run but Core 0 is busy:
    Option 1: Wait for Core 0 (preserve cache)
    Option 2: Run on Core 1 (cold cache, but immediate)
  
  Scheduler balances: latency vs cache performance


CONTEXT SWITCHING WITH CACHE COHERENCE:
────────────────────────────────────────

When Process A migrates from Core 0 to Core 1:

Core 0 L1 Cache: [A's data becomes stale]
Core 1 L1 Cache: [Must fetch A's data]

Cache coherence protocol (MESI) ensures correctness:
  1. Core 1 requests A's data
  2. Cache controller checks other cores
  3. If Core 0 has modified data, it's transferred
  4. Otherwise, fetched from L3 or RAM

Cost of migration:
  - Cache coherence messages: ~100-500 cycles
  - Cold cache on new core: ~10,000-100,000 cycles
  - Total: ~5-50 µs performance impact

This is why CPU affinity matters!
```


```
MULTI-CORE CONTEXT SWITCHING
=============================

System: 4 CPU Cores
───────────────────────

┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐
│ Core 0  │  │ Core 1  │  │ Core 2  │  │ Core 3  │
├─────────┤  ├─────────┤  ├─────────┤  ├─────────┤
│Process A│  │Process B│  │Process C│  │Process D│
└─────────┘  └─────────┘  └─────────┘  └─────────┘

Each core can run a different process simultaneously!
No context switching needed if ≤ 4 processes

But what if we have 100 processes?


SCENARIO 1: Per-Core Run Queues
────────────────────────────────

Each core has its own scheduler and ready queue:

Core 0 Queue: [A, E, I, M, Q]
Core 1 Queue: [B, F, J, N, R]
Core 2 Queue: [C, G, K, O, S]
Core 3 Queue: [D, H, L, P, T]

Advantages:
  ✓ No lock contention (each core independent)
  ✓ Good cache locality (process stays on same core)
  ✓ Simple implementation

Disadvantages:
  ✗ Load imbalance possible
  ✗ One core may be busy while others idle


SCENARIO 2: Global Run Queue
─────────────────────────────

Single shared queue for all cores:

Global Queue: [A, B, C, D, E, F, G, H, ...]
               ↑   ↑   ↑   ↑
               │   │   │   │
           Core0 Core1 Core2 Core3

Advantages:
  ✓ Perfect load balancing
  ✓ No core sits idle if work available

Disadvantages:
  ✗ Lock contention (all cores access same queue)
  ✗ Poor cache locality (process migrates between cores)


SCENARIO 3: Hybrid Approach (Linux CFS)
────────────────────────────────────────

Per-core queues with periodic load balancing:

Initial:
  Core 0: [A, E, I]        ← 3 processes
  Core 1: [B, F]           ← 2 processes  
  Core 2: [C, G]           ← 2 processes
  Core 3: [D, H, J, K, L]  ← 5 processes (overloaded!)

Load balancer runs periodically (every 4ms):
  Detects Core 3 is overloaded
  Migrates process K to Core 1
  Migrates process L to Core 2

After balancing:
  Core 0: [A, E, I]  ← 3 processes
  Core 1: [B, F, K]  ← 3 processes
  Core 2: [C, G, L]  ← 3 processes
  Core 3: [D, H, J]  ← 3 processes (balanced!)


CPU AFFINITY:
─────────────

Processes can have CPU affinity (preferred cores):

Process A:
  - Ran on Core 0 recently
  - L1/L2 cache still has A's data
  - Scheduler prefers to run A on Core 0 again
  - Benefit: Warm cache, better performance

Cache affinity conflict:
  If Process A must run but Core 0 is busy:
    Option 1: Wait for Core 0 (preserve cache)
    Option 2: Run on Core 1 (cold cache, but immediate)
  
  Scheduler balances: latency vs cache performance


CONTEXT SWITCHING WITH CACHE COHERENCE:
────────────────────────────────────────

When Process A migrates from Core 0 to Core 1:

Core 0 L1 Cache: [A's data becomes stale]
Core 1 L1 Cache: [Must fetch A's data]

Cache coherence protocol (MESI) ensures correctness:
  1. Core 1 requests A's data
  2. Cache controller checks other cores
  3. If Core 0 has modified data, it's transferred
  4. Otherwise, fetched from L3 or RAM

Cost of migration:
  - Cache coherence messages: ~100-500 cycles
  - Cold cache on new core: ~10,000-100,000 cycles
  - Total: ~5-50 µs performance impact

This is why CPU affinity matters!

---

## Real-World Example: Video Player

Let's see how context switching enables a smooth video player experience:

```

# VIDEO PLAYER: CONTEXT SWITCHING IN ACTION

Application: VLC Media Player Processes: 1 main process Threads: 7-10 threads
```
VIDEO PLAYER: CONTEXT SWITCHING IN ACTION
==========================================

Application: VLC Media Player
Processes: 1 main process
Threads: 7-10 threads

Thread Architecture:
────────────────────

Thread 1 (Main/UI):       Handle mouse clicks, buttons
Thread 2 (Video Decode):  Decode H.264 video frames
Thread 3 (Audio Decode):  Decode AAC audio
Thread 4 (Video Render):  Send frames to GPU
Thread 5 (Audio Playback): Send audio to sound card
Thread 6 (File Read):     Read video file from disk
Thread 7 (Network):       Download subtitles/metadata

System: 4-core CPU


Timeline (10ms window):
───────────────────────────────────────────────────────

Time: 0-2ms
───────────
Core 0: [Video Decode] - Decoding frame 1247
Core 1: [Audio Decode] - Decoding audio buffer
Core 2: [File Read]    - Reading next 1MB chunk (BLOCKED)
Core 3: [UI Thread]    - Idle (waiting for user input)

File Read blocks waiting for disk...
  → Context switch on Core 2
  → OS schedules Video Render thread


Time: 2-4ms
───────────
Core 0: [Video Decode] - Still working on frame 1247
Core 1: [Audio Playback] ← Context switch (Audio Decode finished)
Core 2: [Video Render] - Sending frame 1246 to GPU
Core 3: [UI Thread]    - Still idle


Time: 4-6ms
───────────
Core 0: [Video Decode] - Frame 1247 complete!
          ↓ Context switch (frame done)
        [Network Thread] - Downloading subtitles

Core 1: [Audio Playback] - Sending samples to sound card
Core 2: [Video Render] - Waiting for GPU (BLOCKED)
          ↓ Context switch
        [File Read] - Disk I/O complete! Reading data

Core 3: [UI Thread] - User clicked pause button!
          ↓ Context switch (needs higher priority thread)
        [Video Decode] - Starting frame 1248


Time: 6-8ms
───────────
Core 0: [Network Thread] - Still downloading
Core 1: [Audio Decode] - Decoding next audio buffer
Core 2: [File Read] - Reading from disk buffer
Core 3: [Video Decode] - Decoding frame 1248


WHY SO MANY CONTEXT SWITCHES?
──────────────────────────────

1. Video Decode finishes frame → Must switch to start next frame
2. Audio buffer fills → Audio thread blocks, CPU to other thread
3. Disk I/O takes 5ms → File thread blocks, CPU to useful work
4. GPU busy rendering → Video render blocks briefly
5. User input → UI thread needs immediate response

Without Context Switching:
──────────────────────────
Video Decode: [████████]..................... waits 10ms for disk
Audio Playback:           [████]............. starves, audio stutters!
UI Response:                    [█]........... 10ms delay = laggy feel

Result: Terrible user experience
  - Audio stutters and pops
  - Video stutters and drops frames
  - UI feels frozen and unresponsive


With Context Switching:
───────────────────────
All threads make progress:
  ✓ Video decoded smoothly (30-60 fps)
  ✓ Audio plays without gaps
  ✓ UI responds instantly (< 50ms)
  ✓ File reading happens in background
  ✓ Network downloads don't block anything

Result: Smooth, responsive experience!


Context Switch Statistics for 1 second:
────────────────────────────────────────
Thread switches: ~5,000-10,000 per second
  (Multiple threads, frequent I/O blocking)

Core 0: ~2,500 switches/sec
Core 1: ~2,500 switches/sec  
Core 2: ~2,000 switches/sec
Core 3: ~3,000 switches/sec

Total CPU time for switching:
  10,000 switches × 2µs = 20ms per second
  = 2% overhead

Trade-off: 2% overhead for smooth multitasking
  Worth it for good user experience!
```

---

## Conclusion

Context switching is the foundational mechanism that enables modern multitasking operating systems. It's what allows your computer to run hundreds of programs simultaneously, stay responsive to your input, and efficiently utilize CPU resources.

**Why we need context switching:**
- Enables multiple programs to share limited CPU cores
- Provides responsive, interactive user experience
- Efficiently utilizes CPU during I/O waits
- Implements fair resource allocation
- Isolates programs from each other

**How it works:**
- **Process context switching**: Saves complete CPU state, switches memory spaces (expensive due to TLB flush and cache effects), restores new process state
- **Thread context switching**: Saves only execution context, no memory switch needed (much faster), threads share the same address space

**Key differences:**
- Process switches: 3-10 µs, involves memory management changes
- Thread switches: 0.5-3 µs, only execution state changes
- Thread switching is 2-5x faster due to shared memory

**Costs and trade-offs:**
- Direct costs: Time spent saving/restoring state
- Indirect costs: TLB refills, cache pollution, pipeline stalls
- Excessive switching causes thrashing (performance collapse)
- Modern systems balance: enough switching for responsiveness, not so much that overhead dominates

Context switching is invisible to applications but essential to the user experience. It's one of the clever illusions that makes your computer appear to do many things at once, when in reality it's rapidly switching between tasks thousands of times per second. Understanding context switching helps explain why threads are preferred for concurrent programming, why excessive multitasking can slow down a system, and how operating systems create the seamless, responsive computing experience we expect today.