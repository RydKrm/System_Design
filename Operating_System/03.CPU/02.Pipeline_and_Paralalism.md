# Deep Dive into CPU Pipelining, Parallelism, and Hyperthreading

## Introduction to Performance Enhancement Techniques

Modern CPUs don't just execute instructions one after another in a simple sequential manner. That would be like having a factory with only one worker who must completely finish building one car before starting the next. Instead, modern processors use sophisticated techniques to maximize the number of instructions they can execute in a given time period. These techniques—pipelining, parallelism, and hyperthreading—transform the CPU from a simple sequential processor into a highly concurrent execution engine capable of processing multiple instructions simultaneously.

The goal of all these techniques is to increase **throughput** (the number of instructions completed per unit time) while managing **latency** (the time it takes to complete a single instruction). Think of it like a highway: we can move more cars by adding lanes (parallelism), organizing traffic flow efficiently (pipelining), or allowing cars to share lanes intelligently (hyperthreading).

---

## CPU Pipeline Architecture

### Understanding Pipelining Fundamentals

Pipelining is inspired by assembly line manufacturing. In a car factory, different stations handle different tasks: one welds the frame, another installs the engine, another paints, and so on. Multiple cars move through the line simultaneously, with each at a different stage of completion. The factory doesn't wait for one car to be completely finished before starting the next.

Similarly, instruction pipelining divides instruction execution into distinct stages, allowing multiple instructions to be in different stages of execution at the same time. While one instruction is being executed, the next can be decoded, and the one after that can be fetched. This overlapping dramatically increases the CPU's instruction throughput.

```
BASIC 5-STAGE PIPELINE CONCEPT
===============================

Sequential Execution (No Pipeline):
Time →
Cycle: 1    2    3    4    5    6    7    8    9    10   11   12
I1:   [F]  [D]  [E]  [M]  [W]
I2:                        [F]  [D]  [E]  [M]  [W]
I3:                                            [F]  [D]  [E]  [M]  [W]

Total Time: 15 cycles for 3 instructions = 5 cycles per instruction


Pipelined Execution:
Time →
Cycle: 1    2    3    4    5    6    7    8    9
I1:   [F]  [D]  [E]  [M]  [W]
I2:        [F]  [D]  [E]  [M]  [W]
I3:             [F]  [D]  [E]  [M]  [W]
I4:                  [F]  [D]  [E]  [M]  [W]
I5:                       [F]  [D]  [E]  [M]

Total Time: 9 cycles for 5 instructions = 1.8 cycles per instruction

Speedup: ~2.8x faster!

Legend:
[F] = Fetch    [D] = Decode    [E] = Execute
[M] = Memory   [W] = Write-back
```

### The Classic Five-Stage Pipeline

Most educational discussions of pipelining use a five-stage model, which captures the essential phases of instruction execution:

**Stage 1: Instruction Fetch (IF)**

In this stage, the CPU retrieves the next instruction from memory. The Program Counter provides the address, which goes through the MMU for translation. The CPU checks the L1 instruction cache first. If the instruction is present (cache hit), it's retrieved in 1-2 clock cycles. If not, the request propagates down the cache hierarchy.

During this stage, the instruction is loaded into the Instruction Register. The Program Counter is also incremented or updated based on the previous instruction (if it was a branch). The fetch stage must maintain high bandwidth because if it falls behind, the entire pipeline stalls.

Modern CPUs can fetch multiple instructions per cycle (called "superscalar" fetch), filling a fetch buffer with upcoming instructions to keep the pipeline fed.

**Stage 2: Instruction Decode (ID)**

The decode stage examines the instruction in the Instruction Register and determines what it means. The decoder identifies:

- The operation to perform (opcode)
- Which registers are needed (source operands)
- Where the result should go (destination)
- Whether memory access is required
- What control signals to generate

The decode stage also reads the necessary registers from the register file. Modern CPUs have register files with multiple read ports, allowing several values to be read simultaneously. If the instruction is "ADD R3, R1, R2", the decoder reads both R1 and R2 in parallel during this stage.

The decoder also checks for hazards—situations where the current instruction depends on results from previous instructions still in the pipeline. This is called dependency checking.

**Stage 3: Execute (EX)**

This is where the actual computation happens. For arithmetic and logical instructions, the ALU performs the operation. For memory access instructions, the ALU calculates the effective memory address. For branch instructions, the condition is evaluated.

The operands (from registers or as immediate values in the instruction) flow into the ALU. The ALU is configured by control signals from the decode stage to perform the specific operation needed. The result is produced along with condition flags (zero, negative, carry, overflow).

For branch instructions, this stage determines whether the branch should be taken. If the branch predictor guessed wrong, the pipeline must be corrected, which we'll discuss later.

**Stage 4: Memory Access (MEM)**

Not all instructions need this stage. It's used for load and store instructions that access the data cache or main memory. The address calculated in the execute stage is used here.

For a load instruction, data is read from the L1 data cache (or from lower cache levels or RAM if necessary) and prepared for writing to a register. For a store instruction, data from a register is written to the cache hierarchy.

The memory stage must handle cache misses, which can take many cycles. Modern processors have sophisticated mechanisms to prevent memory operations from blocking the entire pipeline.

**Stage 5: Write-Back (WB)**

The final stage writes the result back to its destination. For arithmetic operations, the result goes into the destination register. The register file is updated. For load instructions, the data retrieved from memory is written to a register.

Flags are also updated in the flags register if the instruction affects them. Once write-back completes, the instruction is officially "retired" or "committed"—its effects are permanent and visible to the rest of the system.

After write-back, the pipeline slot is free for a new instruction. In steady state, one instruction completes (writes back) every clock cycle, even though each instruction takes five cycles to complete from start to finish.

```
DETAILED PIPELINE STAGE DIAGRAM
================================

                    Pipeline Registers
                          ↓
    ┌──────┐    ┌──────┐    ┌──────┐    ┌──────┐    ┌──────┐
    │      │    │      │    │      │    │      │    │      │
→  →│  IF  │→  →│  ID  │→  →│  EX  │→  →│ MEM  │→  →│  WB  │→ →
    │      │    │      │    │      │    │      │    │      │
    └──────┘    └──────┘    └──────┘    └──────┘    └──────┘
       ↑           ↑           ↑           ↑           ↑
       │           │           │           │           │
    I-Cache    Decode      ALU/        D-Cache     Register
    + PC       Logic       Branch      Access       File
                          Unit                     Update


Pipeline Registers Store Intermediate Results:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

IF/ID Register: Holds fetched instruction
ID/EX Register: Holds decoded instruction + operands
EX/MEM Register: Holds ALU result + memory address
MEM/WB Register: Holds data to write back
```

### Pipeline Registers

Between each pipeline stage are pipeline registers (also called pipeline latches). These are special storage elements that hold the intermediate results as they flow from one stage to the next. They act as buffers and are triggered by the clock signal.

When the clock ticks, all pipeline registers simultaneously:

1. Capture the output from the previous stage
2. Provide their stored values to the next stage

This synchronized movement creates the "flow" through the pipeline. Pipeline registers ensure that each stage works on different instructions simultaneously without interference.

### Pipeline Throughput and Latency

Here's a crucial concept: pipelining doesn't reduce the latency of a single instruction. If an instruction takes 5 cycles to complete without pipelining, it still takes 5 cycles with pipelining. What changes is the throughput.

Without pipelining: Complete 1 instruction every 5 cycles = 0.2 instructions per cycle

With 5-stage pipelining: Complete 1 instruction every cycle (after the initial fill) = 1 instruction per cycle

This is a 5x improvement in throughput! In reality, hazards and stalls reduce this ideal speedup, but the improvement is still dramatic.

The initial "fill time" (the time to fill the pipeline with instructions) is noticeable when the CPU starts executing a new section of code, but once the pipeline is full, instructions complete at a steady rate.

---

## Pipeline Hazards

### Understanding Hazards

If pipelining were perfect, every clock cycle would complete one instruction. However, real programs create situations where the smooth flow of the pipeline is disrupted. These disruptions are called hazards. There are three types of hazards: structural, data, and control hazards.

```
TYPES OF PIPELINE HAZARDS
==========================

1. STRUCTURAL HAZARD: Resource Conflict
   ┌─────────────────────────────────┐
   │ Two instructions need the same  │
   │ hardware resource simultaneously│
   └─────────────────────────────────┘
   
   Example: Single memory port
   I1: [IF] [ID] [EX] [MEM] [WB]
   I2:      [IF] [ID] [EX] [MEM]...
                    ↑
              Both need memory!

2. DATA HAZARD: Dependency
   ┌─────────────────────────────────┐
   │ Instruction needs result from   │
   │ previous instruction not ready  │
   └─────────────────────────────────┘
   
   Example: RAW (Read After Write)
   I1: ADD R1, R2, R3  [IF][ID][EX][MEM][WB]
   I2: SUB R4, R1, R5       [IF][ID] XX XX [EX]
                                    ↑
                        Needs R1, but R1 not ready!

3. CONTROL HAZARD: Branch Uncertainty
   ┌─────────────────────────────────┐
   │ Don't know which instruction    │
   │ to fetch next until branch done │
   └─────────────────────────────────┘
   
   Example: Conditional branch
   I1: BEQ R1, R2, TARGET [IF][ID][EX][MEM][WB]
   I2: ADD ...                 [IF][ID] ?? ?? ??
                                    ↑
                        Wrong instruction if branch taken!
```

### Structural Hazards

Structural hazards occur when hardware resources are insufficient to support all possible combinations of simultaneous operations. The classic example is having a single memory port shared between instruction fetch and data memory access. If one instruction is fetching from memory (IF stage) while another is reading data from memory (MEM stage), they conflict.

**Solution: Harvard Architecture**

Modern processors use separate instruction and data caches (the Harvard architecture modification). The L1 cache is split into L1i (instruction) and L1d (data), eliminating most structural hazards involving memory. Each cache has its own connection to the pipeline stages that need it.

**Solution: Duplicate Resources**

High-performance CPUs include multiple ALUs, multiple load/store units, and multiple ports to the register file. This resource duplication allows multiple instructions to execute simultaneously without conflicts.

### Data Hazards

Data hazards are the most common type. They occur when instructions have dependencies on each other's data. There are three subtypes:

**Read After Write (RAW) - True Dependency**

This is the most common data hazard. An instruction needs to read a value that a previous instruction hasn't finished writing yet.

```
RAW HAZARD EXAMPLE
==================

Instruction Sequence:
I1: ADD R1, R2, R3    ; R1 = R2 + R3
I2: SUB R4, R1, R5    ; R4 = R1 - R5  (needs R1 from I1)

Without hazard handling:
Cycle: 1    2    3    4    5    6    7
I1:   [IF] [ID] [EX] [MEM][WB]
I2:        [IF] [ID] [EX] [MEM][WB]
                ↑
           Reads R1 here, but R1 written in cycle 5!
           Would use OLD value of R1 (WRONG!)

With pipeline stall (bubble):
Cycle: 1    2    3    4    5    6    7    8    9
I1:   [IF] [ID] [EX] [MEM][WB]
I2:        [IF] [ID] [  STALL  ] [EX] [MEM][WB]
                                   ↑
                     Now reads correct R1 value

With data forwarding:
Cycle: 1    2    3    4    5    6    7
I1:   [IF] [ID] [EX] [MEM][WB]
I2:        [IF] [ID] [EX] [MEM][WB]
                       ↑
           Forwarding path from EX/MEM register
           delivers R1 result directly to I2's EX stage
```

**Write After Read (WAR) - Anti-dependency**

An instruction tries to write to a register before a previous instruction has read it. This doesn't occur in simple in-order pipelines because stages always execute in order, but it becomes a problem in out-of-order execution processors.

**Write After Write (WAW) - Output Dependency**

Two instructions write to the same register. The later write must not complete before the earlier write, or the final value would be wrong. Again, this is primarily an issue for out-of-order execution.

### Solutions to Data Hazards

**Pipeline Stalling (Bubbling)**

The simplest solution is to insert "bubbles" (no-operation cycles) into the pipeline. When a hazard is detected during the decode stage, the pipeline is stalled—the dependent instruction waits in the decode stage until the data is available. This is like pausing an assembly line.

Stalling works but wastes clock cycles. It reduces the effective throughput of the pipeline.

**Data Forwarding (Bypassing)**

A much better solution is data forwarding. Instead of waiting for a result to be written back to the register file, the result is "forwarded" directly from one pipeline stage to another through dedicated forwarding paths.

When the execute stage of one instruction produces a result, that result can be immediately sent to the execute stage of the following instruction, bypassing the register file entirely. This eliminates most stalls caused by RAW hazards.

Modern CPUs have extensive forwarding networks with multiplexers that select the correct value from various pipeline stages or the register file, depending on what's needed.

**Compiler Instruction Scheduling**

Compilers can rearrange instructions to minimize hazards. If there's a dependency between instructions, the compiler inserts independent instructions in between to fill the gap. This is called instruction scheduling or software pipelining.

For example:

```
Original code:
  ADD R1, R2, R3    ; R1 = R2 + R3
  SUB R4, R1, R5    ; depends on R1 (hazard!)
  MUL R6, R7, R8    ; independent operation

Reordered by compiler:
  ADD R1, R2, R3    ; R1 = R2 + R3
  MUL R6, R7, R8    ; independent, fills the gap
  SUB R4, R1, R5    ; now R1 is ready (no stall!)
```

### Control Hazards (Branch Hazards)

Control hazards arise from branch instructions (if-statements, loops, function calls). The CPU doesn't know which instruction to fetch next until the branch condition is evaluated, which happens in the execute stage. But by that time, instructions have already been fetched and decoded from the sequential path.

```
BRANCH HAZARD PROBLEM
=====================

Code:
      BEQ  R1, R2, TARGET    ; if R1 == R2, jump to TARGET
      ADD  R3, R4, R5        ; next sequential instruction
      SUB  R6, R7, R8
      ...
TARGET: MUL R9, R10, R11

Pipeline without branch handling:
Cycle: 1    2    3    4    5    6    7
BEQ:  [IF] [ID] [EX] [MEM][WB]
ADD:       [IF] [ID] [EX] [MEM][WB]   ← WRONG if branch taken!
SUB:            [IF] [ID] [EX] [MEM]  ← WRONG if branch taken!
                  ↑
            Branch resolved here (cycle 3)
            Must flush ADD and SUB from pipeline!

Pipeline with 3-cycle branch penalty:
Cycle: 1    2    3    4    5    6    7    8    9
BEQ:  [IF] [ID] [EX] [MEM][WB]
ADD:       [IF] [ID] [--FLUSH--]
SUB:            [IF] [--FLUSH--]
MUL:                 [IF] [ID] [EX] [MEM][WB]
                          ↑
                  3 cycles wasted (branch penalty)
```

If the branch is taken, all the instructions fetched from the sequential path are wrong and must be discarded (flushed from the pipeline). This creates "bubbles" and reduces performance significantly. Since branches occur frequently in programs (typically every 5-7 instructions), branch hazards are a major performance bottleneck.

### Solutions to Control Hazards

**Pipeline Flushing**

The simplest approach: when a branch is taken, flush all incorrectly fetched instructions from the pipeline and restart fetching from the branch target. This works but wastes many cycles (the branch penalty).

**Branch Prediction**

Modern CPUs use sophisticated branch predictors to guess whether a branch will be taken or not taken, and where it will go. The CPU speculatively fetches and executes instructions based on this prediction. If the prediction is correct, there's no penalty. If wrong, the pipeline is flushed and restarted on the correct path.

**Static Branch Prediction**: Simple rules like "backward branches (loops) are taken, forward branches are not taken" or looking at hint bits in the instruction. This achieves about 50-70% accuracy.

**Dynamic Branch Prediction**: Uses hardware to track branch behavior at runtime. The CPU maintains a Branch History Table (BHT) or Branch Target Buffer (BTB) that remembers recent branch outcomes. Modern predictors achieve 90-99% accuracy for many programs.

```
BRANCH PREDICTION MECHANISMS
=============================

1. One-Bit Predictor:
   ┌───────────────────────────┐
   │ Branch Address → [1-bit]  │
   │                   0 = NT  │
   │                   1 = T   │
   └───────────────────────────┘
   Problem: Mispredicts twice on loop exit

2. Two-Bit Saturating Counter:
   ┌─────────────────────────────────┐
   │  States:  00 → 01 → 10 → 11    │
   │        Strong Weak Weak Strong  │
   │          NT    NT   T     T     │
   └─────────────────────────────────┘
   Better: Requires two mispredictions to change

3. Global History with Pattern Recognition:
   ┌──────────────────────────────────────┐
   │ Recent branch outcomes: [TTNTTNT...] │
   │           ↓                           │
   │    Pattern Table: Predicts based     │
   │    on sequences of recent branches   │
   └──────────────────────────────────────┘
   Best: Learns complex patterns

4. Branch Target Buffer (BTB):
   ┌───────────────────────────────────────┐
   │ PC  → [Predicted Target Address]      │
   │       [Prediction Bit]                │
   └───────────────────────────────────────┘
   Stores where branches go, not just if taken
```

**Delayed Branching**

An older technique where the instruction immediately after a branch is always executed, regardless of whether the branch is taken. Compilers fill this "delay slot" with useful instructions. Modern processors don't use this because deeper pipelines make it less effective.

**Speculative Execution**

Modern CPUs don't just predict branches—they fully execute down the predicted path, including memory accesses and subsequent branches. Results are held tentatively. If the prediction is correct, the results are committed (made permanent). If wrong, all speculative work is discarded. This keeps the pipeline full but requires careful management of system state.

---

## Superscalar Architecture and Instruction-Level Parallelism (ILP)

### Beyond Simple Pipelining

A basic pipeline executes one instruction per clock cycle at best. Superscalar processors go further: they can issue and execute multiple instructions per cycle. This is achieved by having multiple execution units working in parallel.

Think of it like having multiple assembly lines in the same factory, all working simultaneously. A superscalar CPU might have 3-6 "issue slots" meaning it can begin executing 3-6 instructions simultaneously in a single clock cycle, provided there are no dependencies between them.

```
SCALAR VS SUPERSCALAR EXECUTION
================================

Scalar (Single-Issue) Pipeline:
Cycle: 1    2    3    4    5    6    7    8
I1:   [IF] [ID] [EX] [MEM][WB]
I2:        [IF] [ID] [EX] [MEM][WB]
I3:             [IF] [ID] [EX] [MEM][WB]

Maximum: 1 instruction per cycle


Superscalar (3-Issue) Pipeline:
Cycle: 1    2    3    4    5
I1:   [IF] [ID] [EX] [MEM][WB]
I2:   [IF] [ID] [EX] [MEM][WB]
I3:   [IF] [ID] [EX] [MEM][WB]
I4:        [IF] [ID] [EX] [MEM][WB]
I5:        [IF] [ID] [EX] [MEM][WB]
I6:        [IF] [ID] [EX] [MEM][WB]

Maximum: 3 instructions per cycle


Superscalar Architecture Diagram:
═══════════════════════════════════

         Instruction Fetch (Wide)
                  ↓
         Decode (3-6 decoders)
                  ↓
         ┌────────┼────────┐
         ↓        ↓        ↓
       ALU1     ALU2     ALU3
         ↓        ↓        ↓
    Load/Store  FP Unit  Branch
         ↓        ↓        ↓
         └────────┼────────┘
                  ↓
           Write-back
```

### Instruction-Level Parallelism (ILP)

ILP refers to the degree to which instructions in a program can be executed simultaneously. It's determined by the dependencies in the code. Consider:

```
High ILP (independent instructions):
  I1: ADD R1, R2, R3     ; R1 = R2 + R3
  I2: MUL R4, R5, R6     ; R4 = R5 * R6  (independent of I1)
  I3: SUB R7, R8, R9     ; R7 = R8 - R9  (independent of I1, I2)
  I4: OR  R10, R11, R12  ; R10 = R11 | R12 (independent)

All four can execute simultaneously!


Low ILP (dependent instructions):
  I1: ADD R1, R2, R3     ; R1 = R2 + R3
  I2: MUL R4, R1, R5     ; R4 = R1 * R5  (depends on I1)
  I3: SUB R6, R4, R7     ; R6 = R4 - R7  (depends on I2)
  I4: OR  R8, R6, R9     ; R8 = R6 | R9  (depends on I3)

Must execute sequentially (chain of dependencies)
```

Superscalar processors analyze instructions to find ILP and issue independent instructions simultaneously. The hardware does this dynamically in high-performance processors.

### Multiple Execution Units

Superscalar CPUs contain multiple execution units specialized for different types of operations:

**Integer ALUs**: Multiple units (2-4 typically) for integer arithmetic and logical operations. This allows simultaneous execution of multiple integer instructions.

**Floating-Point Units (FPUs)**: Specialized for floating-point arithmetic. Often pipelined themselves, with operations like multiplication and addition happening in separate sub-units.

**Load/Store Units**: Handle memory access. Modern CPUs have 2-3 load/store units, allowing multiple memory operations per cycle. They interface with the cache hierarchy.

**Branch Units**: Dedicated hardware for evaluating branch conditions and calculating branch targets. This allows branches to be resolved quickly without tying up the integer ALUs.

**Vector/SIMD Units**: For Single Instruction Multiple Data operations, processing multiple data elements with one instruction. Used for multimedia, scientific computing, and machine learning workloads.

Having these specialized units means that an integer operation, a floating-point operation, a memory load, and a branch can all execute simultaneously if the program provides the necessary ILP.

### Out-of-Order Execution

In-order processors issue instructions in the order they appear in the program. If an instruction stalls (waiting for data), subsequent independent instructions also wait, even though they could execute.

Out-of-order (OOO) execution removes this restriction. The CPU examines a window of upcoming instructions (often 100-200 instructions), identifies independent instructions, and executes them as resources become available, regardless of program order. This maximizes utilization of execution units and hides latencies.

```
OUT-OF-ORDER EXECUTION
======================

Program Order:
  I1: LOAD R1, [addr1]     ; long latency (cache miss)
  I2: ADD  R2, R3, R4      ; independent of I1
  I3: MUL  R5, R6, R7      ; independent of I1
  I4: SUB  R8, R1, R9      ; depends on I1 (must wait)

In-Order Execution:
  I1: [LOAD... waiting for memory ...]
  I2: [STALLED - waiting for I1]
  I3: [STALLED - waiting for I1]
  I4: [STALLED - waiting for I1]
  Execution units idle!

Out-of-Order Execution:
  I1: [LOAD... waiting for memory ...]
  I2: [EXECUTE immediately]    ← doesn't wait!
  I3: [EXECUTE immediately]    ← doesn't wait!
  I4: [WAITING for I1]         ← truly dependent
  I1: [COMPLETE]
  I4: [NOW EXECUTE]
  Much better utilization!


OOO Processor Structure:
═══════════════════════════

         Fetch & Decode
                ↓
         Reservation Stations
         (Instruction Window)
        ┌──────┼──────┐
        ↓      ↓      ↓
     Execute when ready
     (out of program order)
        ↓      ↓      ↓
         Reorder Buffer
         (commits in order)
                ↓
          Write-back
```

### Reorder Buffer (ROB)

Out-of-order execution creates a problem: if instructions complete out of order, how do we handle exceptions and maintain correct program behavior? The answer is the Reorder Buffer.

The ROB is a circular queue that tracks all in-flight instructions in program order. Instructions are dispatched to execution units out of order, but their results are held in the ROB. Only when an instruction reaches the "head" of the ROB (meaning all previous instructions have completed) is it "committed" and its results made permanent.

If an exception occurs, the processor can discard all instructions after the exception point because their results haven't been committed yet. This maintains precise exceptions—the CPU state appears as if instructions executed in order up to the exception point.

### Register Renaming

We mentioned this earlier with registers. Out-of-order execution makes WAR and WAW hazards common. Register renaming solves this elegantly.

The CPU maintains a large pool of physical registers (often 168 or more) hidden from programmers. The architectural registers (R1, R2, etc., visible in the instruction set) are mapped to physical registers dynamically. When an instruction writes to R1, it's actually assigned a free physical register (say P47). Subsequent instructions that read R1 are automatically directed to P47.

This eliminates false dependencies. Two instructions that write to R1 can be assigned different physical registers and execute simultaneously without conflict.

```
REGISTER RENAMING EXAMPLE
==========================

Code with false dependency:
  I1: ADD R1, R2, R3    ; R1 = R2 + R3
  I2: MUL R4, R1, R5    ; R4 = R1 * R5  (true dependency on I1)
  I3: SUB R1, R6, R7    ; R1 = R6 - R7  (WAW hazard with I1!)
  I4: OR  R8, R1, R9    ; R8 = R1 | R9  (depends on I3, not I1)

Without renaming:
  I3 must wait for I1 to complete (false dependency)
  I4 must wait for I3

With renaming (Arch → Physical):
  I1: ADD P10, P2, P3   ; R1→P10, R2→P2, R3→P3
  I2: MUL P11, P10, P5  ; R4→P11, R1→P10 (from I1)
  I3: SUB P12, P6, P7   ; R1→P12 (NEW!), R6→P6, R7→P7
  I4: OR  P13, P12, P9  ; R8→P13, R1→P12 (from I3)

Now I1 and I3 are independent!
Both write to R1 architecturally, but to different physical regs
I1 and I3 can execute simultaneously
```

---

## Simultaneous Multithreading (SMT) / Hyperthreading

### Understanding Thread-Level Parallelism

So far, we've discussed instruction-level parallelism—executing multiple instructions from a single program simultaneously. But programs often don't provide enough ILP to keep all execution units busy. There might be long-latency operations (like memory accesses) that leave execution units idle.

Simultaneous Multithreading (SMT), marketed by Intel as Hyperthreading, addresses this by allowing multiple threads (independent streams of instructions) to share a single physical CPU core. The idea is simple but powerful: if one thread stalls, use the idle execution units to execute instructions from another thread.

```
SINGLE-THREADED VS MULTITHREADED EXECUTION
===========================================

Single Thread on Superscalar Core:
───────────────────────────────────
Cycle:  1    2    3    4    5    6
        ┌────┬────┬────┐
ALU1:   │ T1 │ T1 │ T1 │ T1 │ T1 │ T1 │
        ├────┼────┼────┤
ALU2:   │ T1 │IDLE│IDLE│ T1 │IDLE│ T1 │
        ├────┼────┼────┤
FPU:    │IDLE│ T1 │IDLE│IDLE│ T1 │IDLE│
        ├────┼────┼────┤
L/S:    │ T1 │IDLE│ T1 │ T1 │IDLE│IDLE│
        └────┴────┴────┘
Average utilization: ~40-50%
(Many resources idle due to dependencies and stalls)


Two Threads with SMT:
─────────────────────
Cycle:  1    2    3    4    5    6
        ┌────┬────┬────┐
ALU1:   │ T1 │ T1 │ T2 │ T1 │ T2 │ T1 │
        ├────┼────┼────┤
ALU2:   │ T2 │ T1 │ T1 │ T2 │ T1 │ T2 │
        ├────┼────┼────┤
FPU:    │ T1 │ T2 │ T1 │ T2 │ T2 │ T1 │ 
        ├────┼────┼────┤ 
L/S:    │ T2 │ T1 │ T2 │ T1 │ T1 │ T2 │
        └────┴────┴────┘ Average utilization: ~70-85% (Idle slots filled with the other thread's instructions)

```

### How SMT/Hyperthreading Works

SMT requires relatively modest hardware changes to a superscalar processor:

**Duplicated Architectural State**: Each thread needs its own set of architectural registers (general-purpose registers, program counter, stack pointer, flags). These are duplicated for each hardware thread. On an Intel processor with 2-way SMT, there are two sets of architectural registers.

However, the expensive parts—execution units, caches, MMU, and most of the pipeline logic—are shared between threads. This is why SMT provides improved throughput without doubling the hardware.

**Thread-Aware Fetch and Decode**: The fetch unit can pull instructions from multiple threads. The instruction queue contains instructions tagged with their thread ID. The decode logic tracks which thread each instruction belongs to.

**Shared Execution Units**: All threads share the same ALUs, FPUs, load/store units, etc. The CPU's dispatch logic assigns instructions from any thread to any available execution unit. This is where the magic happens—if Thread 1 has instructions waiting for data, Thread 2's ready instructions can use the otherwise-idle execution units.

**Separate Reorder Buffers or Partitioned ROB**: Each thread typically has its own reorder buffer to maintain correct in-order retirement within each thread. The threads commit independently.

**Shared Caches**: The L1, L2, and L3 caches are shared among threads. This is generally beneficial (threads can share data), but can cause cache contention if threads have poor locality or interfere with each other.

```

SMT CORE ARCHITECTURE
=====================

┌─────────────────────────────────────────────────────┐
│               SMT-ENABLED CPU CORE                  │
│                                                     │
│  ┌──────────────┐         ┌──────────────┐          │
│  │   Thread 0   │         │   Thread 1   │          │
│  │  Arch State  │         │  Arch State  │          │
│  │  ┌────────┐  │         │  ┌────────┐  │          │
│  │  │  PC    │  │         │  │  PC    │  │          │
│  │  │  Regs  │  │         │  │  Regs  │  │          │
│  │  │  Flags │  │         │  │  Flags │  │          │
│  │  └────────┘  │         │  └────────┘  │          │
│  └───────┬──────┘         └──────┬───────┘          │
│          │                        │                 │
│          └────────────┬───────────┘                 │
│                       ↓                             │
│          ┌────────────────────────┐                 │
│          │   Instruction Fetch    │                 │
│          │  (fetches from both)   │                 │
│          └───────────┬────────────┘                 │
│                      ↓                              │
│          ┌────────────────────────┐                 │
│          │   Decode & Rename      │                 │
│          │  (shared resources)    │                 │
│          └───────────┬────────────┘                 │
│                      ↓                              │
│          ┌────────────────────────┐                 │
│          │  Instruction Queue     │                 │
│          │  [T0 T1 T0 T1 T0 T1]  │                  │
│          └───────────┬────────────┘                 │
│                      ↓                              │
│     ┌────────────────┼────────────────┐             │
│     ↓                ↓                ↓             │
│  ┌──────┐       ┌──────┐        ┌──────┐            │
│  │ ALU1 │       │ ALU2 │        │ FPU  │            │
│  └──────┘       └──────┘        └──────┘            │
│     (shared - executes instructions from any thread)│
│                                                     │
│  ┌─────────────────────────────────────────┐        │
│  │         L1 Cache (Shared)               │        │
│  └─────────────────────────────────────────┘        │
└─────────────────────────────────────────────────────┘

```

### Performance Benefits and Limitations

**Performance Gains**: SMT typically provides 15-30% performance improvement for throughput-oriented workloads compared to disabling SMT. This is a significant gain from relatively small hardware additions. The improvement comes from better utilization of execution resources.

For workloads with lots of memory accesses or cache misses (where one thread frequently stalls), SMT can provide even larger improvements because the other thread fills the pipeline during stalls.

**Not True 2x Performance**: It's crucial to understand that a core with 2-way SMT doesn't provide double the performance of a single-threaded core. The threads share execution resources, so if both threads have high ILP and don't stall much, they compete for the same execution units. In the absolute best case, you might see 70-80% additional throughput, but never 100%.

**Cache Contention**: The shared caches can become a bottleneck. If two threads have poor cache locality or their working sets don't fit in the cache, they'll thrash the cache—constantly evicting each other's data. This can actually reduce performance compared to running one thread alone.

**Resource Partitioning**: Some resources (like the reservation stations, reorder buffer entries, and load/store buffers) are partitioned between threads. If these partitions are too small, a thread with lots of in-flight instructions might be bottlenecked.

**Scheduling Considerations**: The operating system must be SMT-aware. It should schedule related threads (like threads from the same process) on the same physical core to benefit from shared caches. It should also balance load across physical cores before using SMT threads to avoid overloading one physical core while leaving others idle.

### Security Considerations

SMT has security implications. Because threads share execution units and caches, side-channel attacks are possible. A malicious thread on one hardware thread can observe cache timing variations caused by a victim thread on the other hardware thread, potentially leaking sensitive information.

Spectre and Meltdown vulnerabilities were partially exacerbated by SMT. Some high-security environments disable SMT entirely to eliminate these cross-thread side channels.

---

## Multi-Core Processors

### From Multi-Threading to Multi-Core

While SMT allows one physical core to handle multiple threads by sharing execution resources, multi-core processors provide true parallelism by including multiple complete CPU cores on the same chip. Each core has its own complete set of execution units, pipeline, and typically L1 and L2 caches. They share L3 cache and memory interfaces.

A modern consumer processor might have 8-16 cores, each with 2-way SMT, providing 16-32 hardware threads total. High-end server processors can have 64 or more cores.

```

MULTI-CORE PROCESSOR ARCHITECTURE
==================================

┌───────────────────────────────────────────────────────┐
│              CPU DIE (Processor Package)              │
│                                                       │
│  ┌──────────────────┐       ┌──────────────────┐      │
│  │    CORE 0        │       │    CORE 1        │      │
│  │  ┌────────────┐  │       │  ┌────────────┐  │      │
│  │  │  Thread 0  │  │       │  │  Thread 0  │  │      │
│  │  │  Thread 1  │  │       │  │  Thread 1  │  │      │
│  │  └────────────┘  │       │  └────────────┘  │      │
│  │                  │       │                  │      │
│  │  ┌────────────┐  │       │  ┌────────────┐  │      │
│  │  │  L1 Cache  │  │       │  │  L1 Cache  │  │      │
│  │  └────────────┘  │       │  └────────────┘  │      │
│  │  ┌────────────┐  │       │  ┌────────────┐  │      │
│  │  │  L2 Cache  │  │       │  │  L2 Cache  │  │      │
│  │  └────────────┘  │       │  └────────────┘  │      │
│  └────────┬─────────┘       └────────┬─────────┘      │
│           │                          │                │
│           └──────────┬───────────────┘                │
│                      ↓                                │
│        ┌──────────────────────────────┐               │
│        │    L3 Cache (Shared)         │               │
│        │         16-64 MB             │               │
│        └──────────────┬───────────────┘               │
│                       ↓                               │
│        ┌──────────────────────────────┐               │
│        │   Memory Controller          │               │
│        └──────────────┬───────────────┘               │
└───────────────────────┼───────────────────────────────┘
                        ↓
                 ┌──────────────┐
                 │  Main Memory │
                 │    (RAM)     │
                 └──────────────┘

```

### Cache Coherence in Multi-Core Systems

When multiple cores each have their own L1 and L2 caches, a crucial problem arises: what happens when two cores cache the same memory location and one modifies it? Without coordination, the cores would have inconsistent views of memory.

Cache coherence protocols solve this. The most common is the MESI protocol (and its variants like MESIF and MOESI). MESI stands for the four states a cache line can be in:

**Modified (M)**: This core has modified the cache line. No other core has a copy. The cache line is "dirty"—it must eventually be written back to main memory.

**Exclusive (E)**: This core has a clean copy of the cache line. No other core has a copy. The line matches main memory.

**Shared (S)**: Multiple cores have copies of this cache line. All copies are clean and match main memory. No core has modified it.

**Invalid (I)**: This cache line is not valid. The core doesn't have a current copy of this memory location.

```

# MESI PROTOCOL STATE TRANSITIONS

State Diagram: ──────────────

```
    ┌─────────────┐
    │  INVALID    │
    └──────┬──────┘
           │
    Read from memory
           ↓
    ┌──────────────┐    Write
    │  EXCLUSIVE   ├───────────┐
    └──────┬───────┘           │
           │                   ↓
    Another core          ┌──────────┐
    reads this line       │ MODIFIED │
           │              └────┬─────┘
           ↓                   │
    ┌──────────────┐          │ Another core
    │   SHARED     │          │ reads
    └──────┬───────┘          │
           │                   ↓
    Write (must invalidate  [Invalidate
    other copies)            others]
           │                   │
           └───────────────────┘
```

Example Scenario: ─────────────────

Initial: Memory location X = 5

Core 0 reads X: Core 0 cache: X = 5 [EXCLUSIVE]

Core 1 reads X: Core 0 cache: X = 5 [SHARED] Core 1 cache: X = 5 [SHARED]

Core 0 writes X = 10: Core 0 cache: X = 10 [MODIFIED] Core 1 cache: X = [INVALID] ← Cache line invalidated Memory: X = 5 (not updated yet)

Core 1 reads X: Core 0 must provide the value (intervention) Core 0 cache: X = 10 [SHARED] Core 1 cache: X = 10 [SHARED] Memory: X = 10 (updated during intervention)

```

The cache coherence protocol is implemented in hardware. When a core wants to write to a cache line in Shared or Exclusive state, it broadcasts an invalidation message. Other cores snoop these messages and invalidate their copies. This happens transparently—programmers don't need to manage it.

The shared L3 cache helps with coherence. The L3 cache tracks which L1/L2 caches have copies of each line, speeding up invalidation and data sharing.

### Inter-Core Communication

Cores communicate through shared memory. When one core writes data that another core needs, the second core must eventually see the updated value. The cache coherence protocol ensures this, but there's latency involved.

Modern processors have special instructions for atomic operations and memory barriers to help programmers synchronize correctly across cores. These instructions ensure that memory operations complete in the expected order and that all cores have a consistent view of memory at synchronization points.

### NUMA (Non-Uniform Memory Access)

In large multi-socket servers with dozens of cores, all memory isn't equidistant from all cores. Each CPU socket has its own memory controller and local RAM. Accessing local memory is fast, but accessing memory attached to another socket (remote memory) requires going through inter-socket links and is slower.

This is called Non-Uniform Memory Access. The operating system and applications must be NUMA-aware, allocating memory close to the cores that use it and minimizing remote memory accesses for optimal performance.

---

## Comparing Parallelism Techniques

Let's summarize the different forms of parallelism we've discussed:

```
PARALLELISM HIERARCHY
=====================

1. Instruction-Level Parallelism (ILP)
   ────────────────────────────────────
   Granularity: Individual instructions
   Hardware: Pipelining, superscalar execution, OOO
   Visibility: Completely transparent to software
   Speedup: ~2-4x over simple in-order
   
   ┌──────┬──────┬──────┬──────┐
   │ ADD  │ MUL  │ SUB  │ LOAD │  ← All execute simultaneously
   └──────┴──────┴──────┴──────┘

2. Thread-Level Parallelism - SMT (Hyperthreading)
   ──────────────────────────────────────────────────
   Granularity: Threads sharing a core
   Hardware: Duplicated arch state, shared execution
   Visibility: OS sees multiple hardware threads
   Speedup: ~1.15-1.3x per core
   
   ┌───────────────────────────┐
   │        One Core           │
   │  Thread A  ─┐             │
   │             ├─► [Shared   │
   │  Thread B  ─┘    Exec]    │
   └───────────────────────────┘

3. Thread-Level Parallelism - Multi-Core
   ────────────────────────────────────────
   Granularity: Threads on separate cores
   Hardware: Complete independent cores
   Visibility: OS schedules across cores
   Speedup: ~Nx for N cores (ideal)
   
   ┌──────┐  ┌──────┐  ┌──────┐  ┌──────┐
   │Core 0│  │Core 1│  │Core 2│  │Core 3│
   │Thr A │  │Thr B │  │Thr C │  │Thr D │
   └──────┘  └──────┘  └──────┘  └──────┘

4. Data-Level Parallelism - SIMD
   ───────────────────────────────
   Granularity: Multiple data elements
   Hardware: Vector/SIMD units
   Visibility: Special instructions (AVX, NEON)
   Speedup: 4-16x for suitable workloads
   
   One instruction operates on vector:
   [A0, A1, A2, A3] + [B0, B1, B2, B3]
   = [A0+B0, A1+B1, A2+B2, A3+B3]
   All additions happen simultaneously
    
```

### When to Use Each Technique

**ILP and Pipelining**: Automatically exploited by hardware. Programmers benefit by writing code with good locality and avoiding unnecessary dependencies. Compilers help by reordering instructions.

**SMT/Hyperthreading**: Useful when you have more threads than physical cores, or when threads have high memory latency. Not beneficial for compute-bound workloads that already saturate execution units. Operating system schedulers use this automatically.

**Multi-Core**: Essential for parallel workloads. Applications must be explicitly written to use multiple threads (via threading libraries or parallel frameworks). Best for embarrassingly parallel problems where threads work independently.

**SIMD**: Requires explicit programming (using intrinsics or compiler auto-vectorization). Excellent for data-parallel operations like image processing, scientific simulations, and machine learning. Can provide dramatic speedups but limited to specific operation patterns.

---

## Real-World Example: Modern Intel/AMD Processor

Let's examine how these concepts come together in a real processor:

```

MODERN HIGH-PERFORMANCE CPU (e.g., Intel Core i9 or AMD Ryzen 9)
=================================================================

Specifications:
───────────────
- 8-16 Performance Cores
- Each core: 2-way SMT (Hyperthreading)
- Total: 16-32 hardware threads
- Pipeline: 14-19 stages (deep pipeline for high frequency)
- Superscalar: 6-8 wide issue
- Out-of-Order execution window: 224+ instructions
- Register renaming: 168-180 physical integer registers

Pipeline Structure (Simplified):
─────────────────────────────────

Frontend:
  [Fetch] → [Pre-decode] → [Decode] → [µOp Cache]
      ↓
  Branch Predictor (99%+ accuracy)
      ↓
Backend:
  [Rename/Allocate] → [Schedule/Dispatch] → [Execute]
                                               ↓
      ┌──────────────────────┬────────────────┼──────┐
      ↓                      ↓                ↓      ↓
   [4x ALU]            [2x Load/Store]    [FPU]  [Vector]
      ↓                      ↓                ↓      ↓
  [Reorder Buffer] → [Retire] → [Commit]

Memory Hierarchy:
─────────────────
L1i Cache: 32 KB per core, 4-way, ~4 cycle latency
L1d Cache: 32-48 KB per core, 8-way, ~5 cycle latency
L2 Cache: 512 KB-1 MB per core, 8-16 way, ~12 cycle latency
L3 Cache: 16-64 MB shared, 16-way, ~40-50 cycle latency
RAM: 16-128 GB, ~200+ cycle latency

Performance Characteristics:
────────────────────────────
Peak IPC: 5-6 instructions per cycle per core
Realistic IPC: 2-4 (depending on workload)
With SMT: 1.2-1.3x improvement in throughput
Branch misprediction penalty: 15-20 cycles
Memory latency: Hidden by OOO execution and prefetching

```

### Performance Scenario

Consider running a complex program:

**Single Thread on One Core**: The deep pipeline and wide issue width allow 2-4 instructions per cycle on average. OOO execution hides memory latencies. When a cache miss occurs, the 224-instruction window provides many other instructions to execute while waiting. Branch prediction keeps the pipeline full 99% of the time.

**Two Threads on One Core (SMT)**: When Thread 1 stalls on a cache miss, Thread 2's instructions fill the execution units. Total throughput increases by ~25%. The shared L1 cache is large enough that contention isn't severe. Both threads benefit from the shared L2 and L3 caches if they're working on related data.

**Multiple Threads Across Cores**: Each thread gets its own core with full resources. The shared L3 cache allows efficient inter-thread communication. The OS schedules threads to maximize cache affinity and minimize cross-core communication. Scales nearly linearly for embarrassingly parallel workloads.

**Mixed Workload**: Background threads (OS services, browser tabs) run on some hardware threads while the main application uses others. SMT ensures the CPU stays productive even when individual threads block on I/O or other events.

---

## Conclusion

Modern CPUs are extraordinarily complex machines that employ multiple layers of parallelism to achieve astounding performance:

**Pipelining** breaks instruction execution into stages, allowing multiple instructions to be in flight simultaneously. The pipeline is the foundation of modern performance, though hazards must be carefully managed through forwarding, branch prediction, and speculative execution.

**Superscalar execution and ILP** exploit instruction-level parallelism by issuing multiple independent instructions simultaneously to multiple execution units. Out-of-order execution maximizes utilization by dynamically scheduling instructions based on data availability rather than program order.

**Simultaneous Multithreading (SMT/Hyperthreading)** allows multiple threads to share a single core's execution resources, improving utilization when individual threads stall. It provides meaningful performance improvements with modest hardware cost.

**Multi-core processors** provide true parallel execution, with each core being a complete, independent processor. This is the primary path to increased performance in an era where single-core frequency scaling has hit physical limits.

Together, these techniques transform the CPU from a simple sequential processor into a highly parallel execution engine. A modern 16-core processor with SMT and 6-wide superscalar execution can theoretically have 192 instructions in flight simultaneously (16 cores × 2 threads × 6 issue width). While real-world workloads rarely achieve this theoretical maximum due to dependencies and resource contention, the performance gains over simple sequential execution are extraordinary.

Understanding these mechanisms helps programmers write more efficient code: minimizing dependencies to expose ILP, writing multi-threaded code to exploit multi-core processors, and structuring data access patterns to work with the cache hierarchy. The hardware provides the performance potential, but software must be designed to unlock it.
```
```