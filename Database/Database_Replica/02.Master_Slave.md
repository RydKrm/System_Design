# Chapter: Understanding Primaryâ€“Replica (Masterâ€“Slave) Replication

Imagine youâ€™re running a busy restaurant.

* The **head chef** is the only one allowed to cook new dishes (the *primary*).
* The **assistant chefs** donâ€™t cook on their own, but they keep watching the head chef carefully, taking notes, and repeating every move so their copies of the dishes match exactly (the *replicas*).
* Customers who just want to *look at the food* (read queries) can walk over to the assistants.
* But if someone wants to *add new orders* (write queries), it always goes to the head chef.

Thatâ€™s **primaryâ€“replica replication** in databases.

---

## Step 1: Why do we even need this?

Databases get stressed under heavy traffic. If everyone sends both **reads** (e.g., â€œshow me product detailsâ€) and **writes** (e.g., â€œadd this to my cartâ€) to a single database, it slows down.

Replication solves this by:

1. **Sharing the load**: Primary handles writes; replicas handle reads.
2. **Safety**: If the primary fails, a replica can be promoted as the new primary.
3. **Backups**: Replicas can safely be used for backups without disturbing the main system.

---

## Step 2: How does replication actually work?

Letâ€™s go back to our chef analogy.

* When the head chef finishes cooking a dish, they write the recipe in a notebook.
* The assistants grab this notebook and replay each step in their own kitchens.

In database terms:

* **Primary writes everything into a log** (like a recipe book).

  * In PostgreSQL, this is called the **WAL** (Write-Ahead Log).
  * In MySQL, itâ€™s called the **binlog**.
* **Replicas read the log and apply the same changes**.

So if you run this SQL on the primary:

```sql
INSERT INTO orders (id, product) VALUES (1, 'Laptop');
```

The primary first writes this event into its log. The replica then reads:
â€œAn order was added with id=1, product=Laptopâ€ â†’ and applies the same change.

---

## Step 3: The timing problem (Replication Lag)

Hereâ€™s the catch:
The replica doesnâ€™t instantly apply the change. Thereâ€™s always a small delay (milliseconds, sometimes seconds under load).

That means if you:

1. Add an order on the primary.
2. Immediately check the replica for that order.
3. You might not see it yet â†’ because the assistant chef hasnâ€™t finished copying the notebook.

This is called **replication lag**.

---

## Step 4: Types of replication timing

There are three common ways databases handle timing between the primary and replicas:

1. **Asynchronous (most common)**

   * Primary doesnâ€™t wait for replicas.
   * Fastest, but you might lose the last few transactions if the primary crashes.

2. **Semi-synchronous**

   * Primary waits until at least one replica confirms it *received* the log (not applied yet).
   * Slightly safer, small delay.

3. **Synchronous**

   * Primary waits until replicas confirm theyâ€™ve *applied* the change.
   * Safest, but slowest.

---

## Step 5: Failover (What if the head chef disappears?)

If the primary crashes:

* One of the replicas is promoted to primary.
* All other replicas are told to follow the new primary.

This way the kitchen keeps running.

Tools like **Patroni** (Postgres) or **Orchestrator** (MySQL) help automate this failover.

---

## Step 6: How apps actually use this

In real systems, you set up two different â€œdoorsâ€ to your database:

* **Write Door** â†’ always goes to the primary.
* **Read Door** â†’ usually goes to replicas.

But thereâ€™s one important rule:
If you just wrote some data and need to see it immediately, you **must read from the primary** (otherwise replication lag might trick you).

Example:

* A user updates their profile picture.
* If you show their profile immediately after, you should fetch from the primary.
* But if another random user checks that profile later, you can safely show it from a replica.

---

## Step 7: Putting it all together (Mental Picture)

```
Clients
   |
   |  Writes (INSERT/UPDATE/DELETE)
   v
Primary --------------> Replicas
   |                       ^
   |  Log of changes       |  Reads (SELECT queries)
   |                       |
   ------------------------- 
```

---

## Step 8: Quick summary

* Primary handles **all writes**.
* Replicas handle **reads**.
* Replicas copy changes by replaying the primaryâ€™s log.
* Thereâ€™s always some **lag**.
* If the primary dies, a replica can be promoted.
* Applications must be smart about when to read from replicas and when to stick to the primary.

---

# Chapter: Building Masterâ€“Replica PostgreSQL with Docker

So far, youâ€™ve read the story. Now, letâ€™s set up a playground where you can *see replication in action*. Weâ€™ll use Docker, because it lets us spin up databases like little Lego blocks â€” no risk of messing with your actual system.

---

## Step 1: The Plan

We want:

* **One master (primary) PostgreSQL** â†’ handles writes.
* **One replica (slave)** â†’ copies data from master, serves reads.
* **Your app (Node.js)** â†’ decides who gets the query.

Both databases will run in containers and talk over a shared Docker network.

---

## Step 2: The Docker Compose File

Hereâ€™s the full `docker-compose.yml` that sets up the cluster:

```yaml
version: "3.9"

services:
  # Master database
  postgres-master:
    image: postgres:16
    container_name: postgres-master
    environment:
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
      POSTGRES_DB: mydb
    ports:
      - "5432:5432"
    volumes:
      - master-data:/var/lib/postgresql/data
      - ./master/init:/docker-entrypoint-initdb.d
    networks:
      - dbnet

  # Replica database
  postgres-replica:
    image: postgres:16
    container_name: postgres-replica
    environment:
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
      POSTGRES_DB: mydb
    ports:
      - "5433:5432"
    depends_on:
      - postgres-master
    command:
      - "postgres"
      - "-c"
      - "primary_conninfo=host=postgres-master port=5432 user=myuser password=mypassword"
      - "-c"
      - "hot_standby=on"
    volumes:
      - replica-data:/var/lib/postgresql/data
    networks:
      - dbnet

networks:
  dbnet:

volumes:
  master-data:
  replica-data:
```

---

## Step 3: Whatâ€™s Happening Here?

* **`postgres-master`**
  Runs a normal PostgreSQL instance, exposed on port `5432`.

* **`postgres-replica`**
  Also runs PostgreSQL but with `primary_conninfo` telling it:

  > â€œFollow the master at `postgres-master:5432` using this username and password.â€

* **Volumes**
  Persist data locally, so even if containers restart, data isnâ€™t lost.

* **Ports**

  * Master: `localhost:5432`
  * Replica: `localhost:5433`

---

## Step 4: Boot It Up

Run:

```bash
docker-compose up -d
```

Youâ€™ll now have two databases running.

---

## Step 5: Test Replication

1. Connect to the master:

```bash
docker exec -it postgres-master psql -U myuser -d mydb
```

Insert a row:

```sql
CREATE TABLE users (id SERIAL PRIMARY KEY, name TEXT);
INSERT INTO users (name) VALUES ('Alice');
```

2. Connect to the replica:

```bash
docker exec -it postgres-replica psql -U myuser -d mydb
```

Query:

```sql
SELECT * FROM users;
```

âœ… You should see `Alice` appear, even though you never wrote to the replica.

Thatâ€™s replication in action.

---

## Step 6: Node.js Router for Reads/Writes

Now we add the application logic. Create `db.js`:

```js
// db.js
const { Pool } = require("pg");

// Write connection (master)
const writePool = new Pool({
  host: "localhost",
  port: 5432,
  user: "myuser",
  password: "mypassword",
  database: "mydb",
});

// Read connection (replica)
const readPool = new Pool({
  host: "localhost",
  port: 5433,
  user: "myuser",
  password: "mypassword",
  database: "mydb",
});

// Query router
async function query(sql, params = [], { readOnly = false } = {}) {
  const pool = readOnly ? readPool : writePool;
  const client = await pool.connect();
  try {
    const result = await client.query(sql, params);
    return result.rows;
  } finally {
    client.release();
  }
}

module.exports = { query };
```

---

## Step 7: Example Usage

```js
// app.js
const express = require("express");
const { query } = require("./db");

const app = express();
app.use(express.json());

// Write: insert user into master
app.post("/users", async (req, res) => {
  const { name } = req.body;
  await query("INSERT INTO users (name) VALUES ($1)", [name]);
  res.send("User created");
});

// Read: fetch users from replica
app.get("/users", async (req, res) => {
  const users = await query("SELECT * FROM users", [], { readOnly: true });
  res.json(users);
});

app.listen(3000, () => console.log("App running on http://localhost:3000"));
```

Run with:

```bash
node app.js
```

* POST `/users` â†’ goes to master.
* GET `/users` â†’ goes to replica.

---

## Step 8: See It in Action

1. `POST /users {"name":"Bob"}`
   â†’ Master inserts Bob.

2. `GET /users`
   â†’ Replica returns `[Alice, Bob]`.

Your app is now truly **splitting reads and writes** across master and replica.

---

ğŸ“– *And thatâ€™s how the story turns real: one master writing the truth, many replicas serving it to readers, and your Node.js app acting as the wise librarian, deciding where each request belongs.*

---

Excellent question, Riyad ğŸ‘ â€” this goes to the **heart of replication**.
Letâ€™s step back and tell the story like a dev author teaching a junior developer:

---

# Chapter: How PostgreSQL Shares Knowledge Between Master and Replica

Imagine our master database as the **scribe** of a kingdom. Every time a citizen (the user) brings new information â€” say, "Alice has joined" â€” the scribe writes it down in the royal scroll.

Now, the kingdom has many libraries (replicas) that must also know this news. But the master doesnâ€™t let every library rewrite the scroll themselves. Instead, it sends them **copies of the scroll updates** so they stay in sync.

In PostgreSQL, that "scroll" is called the **Write-Ahead Log (WAL)**.

---

## Step 1: The Write-Ahead Log (WAL)

* When you **insert, update, or delete** something, PostgreSQL **doesnâ€™t directly change the data on disk first**.
* Instead, it writes a record into the **WAL** â€” a log file that says:

  > â€œAt 10:05 AM, row X was inserted into table users.â€

This guarantees durability. If the database crashes, the WAL can be replayed to recover.

Think of WAL as the **black box flight recorder** of the database.

---

## Step 2: Streaming Replication

When replication is enabled, the master not only writes WAL for itself, but also **streams** these WAL records over TCP to the replicas.

* The replica runs in a special mode called **standby**.
* It continuously receives WAL entries from the master.
* The replica then **replays those WAL changes** on its own data files, applying inserts, updates, and deletes exactly as they happened.

So replicas donâ€™t re-run SQL queries. They simply **replay the changes** at the storage level.

---

## Step 3: How Does It Happen in Real Time?

1. **User writes on Master** â†’ `INSERT INTO users (name) VALUES ('Alice');`
2. Master logs this to WAL.
3. WAL is **sent over the network** to replicas.
4. Replica applies the WAL and now its copy of `users` also has Alice.

To the outside world, it looks like replication is â€œinstant.â€ In reality, itâ€™s just **WAL shipping + replaying**.

---

## Step 4: Synchronous vs Asynchronous

PostgreSQL gives you a choice:

* **Asynchronous replication (default)**

  * Master writes WAL and sends it off.
  * It doesnâ€™t wait for replicas to confirm.
  * Faster, but if the master crashes, the replica might be slightly behind.

* **Synchronous replication**

  * Master waits until at least one replica confirms it has received WAL.
  * Slower, but safer (zero data loss in a crash).

Think of it like this:

* Asynchronous = â€œI sent the letter, theyâ€™ll get it eventually.â€
* Synchronous = â€œI wonâ€™t leave the post office until the receiver signs for it.â€

---

## Step 5: Why Replicas Are Read-Only

Because WAL contains *exactly how the master wrote data*, if a replica started accepting writes of its own, it would create conflicts.
Thatâ€™s why replicas are **read-only**: they just follow orders from WAL, like a mirror.

---

## Step 6: Visual Flow

```
User â†’ Master (writes)
          â†“
       WAL log
          â†“
   Stream WAL over network
          â†“
     Replica (replay WAL, read-only)
```

---

ğŸ’¡ So in short:
PostgreSQL keeps master and replica in sync by **streaming WAL records** â€” little â€œchange notesâ€ â€” from the master to the replicas, which then **replay them** to stay identical.

---