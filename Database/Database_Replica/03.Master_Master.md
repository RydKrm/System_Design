# Master-Master Replication with PostgreSQL and Node.js

### Introduction

Imagine two offices in different cities. Both offices can send and receive letters, and you want to make sure that no matter which office someone writes to, the other office gets the message too. This is essentially the idea behind **master-master replication** in databases. In PostgreSQL, each node (database instance) acts as a master. They can handle **writes**, and changes are synchronized between all nodes.

This is different from the traditional **master-slave** setup, where only the master accepts writes, and slaves are read-only replicas. Master-master allows multiple points of write, which increases availability but introduces complexity like **conflict resolution**.

---

### How Master-Master Works in PostgreSQL

PostgreSQL doesn’t natively support full master-master replication out of the box. Instead, you usually implement it via tools like:

* **Logical Replication** (introduced in PostgreSQL 10+)
* **Third-party tools** like **Bucardo** or **pglogical**

At its core:

1. Each master publishes changes to a **publication**.
2. Each master subscribes to the other master’s publication.
3. When a row is inserted, updated, or deleted on one master, it is replicated to the other.

**Diagrammatically:**

```
Master A  <-- publishes/subscribes -->  Master B
   |                                  |
Writes go here                     Writes go here
```

Conflicts can occur if the same row is updated on both nodes at the same time. A good replication setup defines **conflict resolution rules** (last-update-wins is common).

---

### Node.js Perspective

From the application side, the database nodes can be treated like this:

* **Writes**: Send all `INSERT`, `UPDATE`, `DELETE` operations to **any master**.
* **Reads**: Can be performed from **any master** (or even a read-only replica if you add one later).

You can abstract this in Node.js using a database client pool.

---

### Practical Node.js Example

Let’s assume you have two PostgreSQL nodes running:

* Master A: `postgres://user:pass@localhost:5432/master_a`
* Master B: `postgres://user:pass@localhost:5432/master_b`

We’ll use `pg` library for PostgreSQL.

```js
// db.js
const { Pool } = require('pg');

// Define master nodes
const masterA = new Pool({
  connectionString: 'postgres://user:pass@localhost:5432/master_a',
});

const masterB = new Pool({
  connectionString: 'postgres://user:pass@localhost:5432/master_b',
});

// Simple write router (round-robin example)
let lastUsedMaster = 0;
function getWriteMaster() {
  lastUsedMaster = (lastUsedMaster + 1) % 2;
  return lastUsedMaster === 0 ? masterA : masterB;
}

// Reads can go from either master (simple load-balancing)
function getReadMaster() {
  return Math.random() < 0.5 ? masterA : masterB;
}

module.exports = {
  writeDB: getWriteMaster,
  readDB: getReadMaster,
};
```

---

```js
// app.js
const express = require('express');
const { writeDB, readDB } = require('./db');

const app = express();
app.use(express.json());

// Write route
app.post('/user', async (req, res) => {
  const { name, email } = req.body;
  const client = writeDB();
  try {
    const result = await client.query(
      'INSERT INTO users(name, email) VALUES($1, $2) RETURNING *',
      [name, email]
    );
    res.status(201).json(result.rows[0]);
  } catch (err) {
    console.error('Write error:', err);
    res.status(500).json({ error: 'Database write failed' });
  }
});

// Read route
app.get('/users', async (req, res) => {
  const client = readDB();
  try {
    const result = await client.query('SELECT * FROM users');
    res.status(200).json(result.rows);
  } catch (err) {
    console.error('Read error:', err);
    res.status(500).json({ error: 'Database read failed' });
  }
});

app.listen(3000, () => console.log('Server running on port 3000'));
```

---

### Important Notes

1. **Conflict Handling**: You need a strategy. Either rely on timestamps, version numbers, or a dedicated conflict resolution tool like Bucardo.
2. **Latency**: Replication isn’t instantaneous. Reads might occasionally see stale data.
3. **Transactions**: Make sure your application handles retries if conflicts occur.
4. **Scaling**: Adding more nodes increases complexity; ensure monitoring is in place.

---

### Key Takeaways

* Master-master replication allows multiple nodes to accept writes.
* PostgreSQL achieves this via logical replication or third-party tools.
* Node.js can interact with masters transparently, routing writes and reads efficiently.
* Conflicts and replication latency are the main challenges.

---

Perfect. Let’s go full “dev-author production setup” mode. We’ll create a **master-master PostgreSQL system** in Docker that initializes itself, sets up replication automatically, and gives you a simple failover pattern for Node.js.

---

# Production-Ready Master-Master PostgreSQL with Docker

### 1. Project Structure

```
master-master-prod/
│
├─ docker-compose.yml
├─ init/
│   ├─ master_a_init.sql
│   ├─ master_b_init.sql
│   └─ replication.sh
├─ node-app/
│   ├─ db.js
│   └─ app.js
```

* `init/` – SQL scripts and replication setup.
* `node-app/` – Node.js app that reads/writes to both masters.
* `docker-compose.yml` – defines the containers.

---

### 2. Docker Compose

```yaml
version: "3.9"

services:
  master_a:
    image: postgres:15
    container_name: master_a
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
      POSTGRES_DB: mydb
    ports:
      - "5432:5432"
    volumes:
      - master_a_data:/var/lib/postgresql/data
      - ./init:/docker-entrypoint-initdb.d
    networks:
      - pgnet

  master_b:
    image: postgres:15
    container_name: master_b
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
      POSTGRES_DB: mydb
    ports:
      - "5433:5432"
    volumes:
      - master_b_data:/var/lib/postgresql/data
      - ./init:/docker-entrypoint-initdb.d
    networks:
      - pgnet

networks:
  pgnet:
    driver: bridge

volumes:
  master_a_data:
  master_b_data:
```

**Key point:** anything in `/docker-entrypoint-initdb.d` is executed automatically when the container first starts.

---

### 3. Initialization Scripts

#### `master_a_init.sql`

```sql
CREATE TABLE IF NOT EXISTS users (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    email TEXT UNIQUE NOT NULL,
    updated_at TIMESTAMP DEFAULT now()
);

-- Publication for Master A
CREATE PUBLICATION master_a_pub FOR TABLE users;
```

#### `master_b_init.sql`

```sql
CREATE TABLE IF NOT EXISTS users (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    email TEXT UNIQUE NOT NULL,
    updated_at TIMESTAMP DEFAULT now()
);

-- Publication for Master B
CREATE PUBLICATION master_b_pub FOR TABLE users;
```

---

### 4. Replication Setup Script (`replication.sh`)

This script ensures automatic **subscription creation** and checks for failover.

```bash
#!/bin/bash
set -e

# Wait until PostgreSQL is ready
until pg_isready -h master_a -p 5432 -U user; do
  echo "Waiting for master_a..."
  sleep 2
done

until pg_isready -h master_b -p 5432 -U user; do
  echo "Waiting for master_b..."
  sleep 2
done

# Create subscriptions (ignore errors if they already exist)
psql -U user -d mydb -h master_a -c "
DO \$\$
BEGIN
   IF NOT EXISTS (SELECT 1 FROM pg_subscription WHERE subname='master_b_sub') THEN
       CREATE SUBSCRIPTION master_b_sub
       CONNECTION 'host=master_b port=5432 dbname=mydb user=user password=pass'
       PUBLICATION master_b_pub;
   END IF;
END
\$\$;
"

psql -U user -d mydb -h master_b -c "
DO \$\$
BEGIN
   IF NOT EXISTS (SELECT 1 FROM pg_subscription WHERE subname='master_a_sub') THEN
       CREATE SUBSCRIPTION master_a_sub
       CONNECTION 'host=master_a port=5432 dbname=mydb user=user password=pass'
       PUBLICATION master_a_pub;
   END IF;
END
\$\$;
"

echo "Master-Master replication setup complete."
```

**Tip:** Make this executable:

```bash
chmod +x init/replication.sh
```

You can add it as a **post-start command** in Docker if needed.

---

### 5. Node.js Application

#### `db.js`

```js
const { Pool } = require('pg');

const masterA = new Pool({
  host: 'master_a',
  port: 5432,
  user: 'user',
  password: 'pass',
  database: 'mydb',
});

const masterB = new Pool({
  host: 'master_b',
  port: 5432,
  user: 'user',
  password: 'pass',
  database: 'mydb',
});

// Round-robin write router
let lastUsed = 0;
function getWriteMaster() {
  lastUsed = (lastUsed + 1) % 2;
  return lastUsed === 0 ? masterA : masterB;
}

// Read router with failover
async function getReadMaster() {
  try {
    await masterA.query('SELECT 1');
    return masterA;
  } catch {
    return masterB;
  }
}

module.exports = { getWriteMaster, getReadMaster };
```

#### `app.js`

```js
const express = require('express');
const { getWriteMaster, getReadMaster } = require('./db');

const app = express();
app.use(express.json());

app.post('/user', async (req, res) => {
  const { name, email } = req.body;
  const client = getWriteMaster();
  try {
    const result = await client.query(
      'INSERT INTO users(name, email) VALUES($1, $2) RETURNING *',
      [name, email]
    );
    res.status(201).json(result.rows[0]);
  } catch (err) {
    console.error('Write error:', err);
    res.status(500).json({ error: 'Write failed' });
  }
});

app.get('/users', async (req, res) => {
  const client = await getReadMaster();
  try {
    const result = await client.query('SELECT * FROM users');
    res.status(200).json(result.rows);
  } catch (err) {
    console.error('Read error:', err);
    res.status(500).json({ error: 'Read failed' });
  }
});

app.listen(3000, () => console.log('Node.js app running on port 3000'));
```

---

### 6. Running Everything

1. Spin up Docker containers:

```bash
docker-compose up -d
```

2. Run replication setup (once, after containers start):

```bash
docker exec -it master_a bash -c "/docker-entrypoint-initdb.d/replication.sh"
```

3. Start Node.js app:

```bash
cd node-app
node app.js
```

4. Test writes and reads:

```bash
curl -X POST http://localhost:3000/user -H "Content-Type: application/json" -d '{"name":"Alice","email":"alice@example.com"}'
curl http://localhost:3000/users
```

5. **Failover simulation:** stop one master and Node.js will automatically read from the other.

---

### ✅ Key Production-Ready Features

* Automatic initialization with Docker volumes.
* Replication setup via shell script (`replication.sh`).
* Node.js round-robin writes and read failover.
* Full master-master logical replication.
* Easy to scale: add more tables/publications.

---
