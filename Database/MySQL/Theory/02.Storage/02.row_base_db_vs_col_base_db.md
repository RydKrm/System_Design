# Deep Dive into Row-Based vs Column-Based Databases

Let me explain these two fundamental database storage paradigms in comprehensive detail with clear examples and visualizations.

---

## **Part 1: Row-Based (Row-Oriented) Databases**

### **What is a Row-Based Database?**

A row-based database stores data **row by row** on disk. All columns of a single row are stored together contiguously in physical storage. This is the traditional approach used by most relational databases like MySQL, PostgreSQL, Oracle, and SQL Server.

Think of it like reading a book: you read one complete sentence (row) at a time, from left to right, before moving to the next sentence.

---

### **How Row-Based Storage Works Internally**

#### **Physical Storage Layout**

Imagine we have an EMPLOYEES table:

```
EMPLOYEES Table (Logical View):
+----+----------+-----+------------+--------+----------+
| ID | Name     | Age | Department | Salary | JoinDate |
+----+----------+-----+------------+--------+----------+
| 1  | John     | 30  | IT         | 50000  | 2020-01-15 |
| 2  | Sarah    | 28  | HR         | 45000  | 2021-03-20 |
| 3  | Michael  | 35  | IT         | 60000  | 2019-06-10 |
| 4  | Alice    | 32  | Finance    | 55000  | 2020-08-25 |
| 5  | Bob      | 29  | HR         | 47000  | 2021-11-05 |
+----+----------+-----+------------+--------+----------+
```

**Row-Based Physical Storage on Disk:**

```
DISK PAGES (Row-Oriented):

Page 1 (8KB):
+------------------------------------------------------------------+
| Row 1: [1][John][30][IT][50000][2020-01-15]                    |
| Row 2: [2][Sarah][28][HR][45000][2021-03-20]                   |
| Row 3: [3][Michael][35][IT][60000][2019-06-10]                 |
+------------------------------------------------------------------+

Page 2 (8KB):
+------------------------------------------------------------------+
| Row 4: [4][Alice][32][Finance][55000][2020-08-25]              |
| Row 5: [5][Bob][29][HR][47000][2021-11-05]                     |
| [Free Space]                                                     |
+------------------------------------------------------------------+

Key Point: Each row's ALL columns are stored together sequentially.
When you read Row 1, you get ALL of John's data in one disk read.
```

**Detailed Memory Layout (Byte Level):**

```
Row 1 in Memory/Disk:
+------+----------+-----+------+--------+------------+
| ID   | Name     | Age | Dept | Salary | JoinDate   |
+------+----------+-----+------+--------+------------+
| 0x01 | 'John\0' | 0x1E| 'IT' | 0xC350 | 0x5E1E7B00 |
| 4B   | 20B      | 4B  | 10B  | 4B     | 8B         |
+------+----------+-----+------+--------+------------+
Total: 50 bytes per row (approximately)

Continuous memory block for one complete row!
```

---

### **CRUD Operations in Row-Based Storage**

Let me explain each operation in extreme detail:

---

#### **1. CREATE (INSERT) Operation**

**Query Example:**

```sql
INSERT INTO employees (id, name, age, department, salary, joindate)
VALUES (6, 'Emma', 27, 'IT', 52000, '2022-05-15');
```

**Internal Process - Step by Step:**

STEP 1: Parse and Validate

- Database receives INSERT statement
- Validates data types (age is integer, salary is numeric, etc.)
- Checks constraints (NOT NULL, UNIQUE, FOREIGN KEY)
- Allocates memory for new row (≈50 bytes)

STEP 2: Prepare Row Data in Memory

```
Memory Buffer:
+------+----------+-----+------+--------+------------+
| 0x06 | 'Emma\0' | 0x1B| 'IT' | 0xCB20 | 0x627A8C00 |
+------+----------+-----+------+--------+------------+
```

All column values packed together in row format

STEP 3: Find Storage Location

- Check for page with free space
- Let's say Page 2 has 7.9KB free
- Calculate offset: Start of free space in Page 2

STEP 4: Write to Page
Before Insert - Page 2:

```sql
+------------------------------------------------------------------+
| Row 4: [4][Alice][32][Finance][55000][2020-08-25]                |
| Row 5: [5][Bob][29][HR][47000][2021-11-05]                       |
| [Free Space: 7.9KB]                                              |
+------------------------------------------------------------------+
```

After Insert - Page 2:

```sql
+------------------------------------------------------------------+
| Row 4: [4][Alice][32][Finance][55000][2020-08-25] |
| Row 5: [5][Bob][29][HR][47000][2021-11-05] |
| Row 6: [6][Emma][27][IT][52000][2022-05-15] |
| [Free Space: 7.85KB] |
+------------------------------------------------------------------+
```

STEP 5: Update Metadata

- Update page header (free space reduced by 50 bytes)
- Update row directory (pointer to Row 6)
- Assign Row_ID: Page 2, Slot 3

STEP 6: Update Indexes (if any)
If there's an index on 'name':

- Navigate B-Tree to position for 'Emma'
- Insert entry: 'Emma' → Row_ID(Page 2, Slot 3)

If there's an index on 'salary':

- Navigate B-Tree to position for 52000
- Insert entry: 52000 → Row_ID(Page 2, Slot 3)

STEP 7: Transaction Log
Write to Write-Ahead Log (WAL):
"INSERT Row 6 at Page 2, Slot 3: [6, Emma, 27, IT, 52000, 2022-05-15]"

STEP 8: Commit

- Mark transaction complete
- Flush log to disk (for durability)
- Modified page stays in buffer pool (written to disk later)

```

**Performance Characteristics:**

- **Time Complexity**: O(1) for heap table (just append)
- **IOs Required**:
  - 1 IO to read Page 2 (if not in cache)
  - 1 IO to write WAL
  - Page 2 written to disk later (async)
  - Plus additional IOs for each index update

**Visual Flow:**

```

INSERT Command
↓
[Parse & Validate] → [Prepare Row in Memory]
↓
[Find Page with Space] → [Write Row to Page]
↓
[Update Indexes] → [Write to Transaction Log]
↓
[Commit Transaction] → [Return Success]

````

---

#### **2. READ (SELECT) Operation**

Let's explore different types of SELECT queries:

---

##### **Query Type 1: Single Row Lookup (Primary Key)**

**Query:**

```sql
SELECT * FROM employees WHERE id = 3;
````

**Internal Process:**

```
STEP 1: Query Planning
- Optimizer sees WHERE id = 3
- Checks for index on 'id' column
- Finds PRIMARY KEY index (B-Tree)
- Decision: Use index lookup

STEP 2: Index Navigation (B-Tree on ID)
B-Tree Structure:
                [4]  ← Root
               /   \
           [2]      [5]  ← Internal nodes
          /   \       \
      [1]  [3]     [6]  ← Leaf nodes
       ↓    ↓       ↓
   Row_ID Row_ID Row_ID

Navigate to ID=3:
- Read Root page: Compare 3 with 4 → Go left (1 IO)
- Read Left child: Compare 3 with 2 → Go right (1 IO)
- Read Leaf: Found ID=3 → Row_ID: Page 1, Slot 3 (1 IO)

STEP 3: Fetch Row from Data Page
Row_ID tells us: Page 1, Slot 3
- Read Page 1 into buffer pool (1 IO)

Page 1 in Memory:
+------------------------------------------------------------------+
| Row 1: [1][John][30][IT][50000][2020-01-15]                    |
| Row 2: [2][Sarah][28][HR][45000][2021-03-20]                   |
| Row 3: [3][Michael][35][IT][60000][2019-06-10]  ← FOUND!       |
+------------------------------------------------------------------+

STEP 4: Extract Row 3
Read bytes from Slot 3 position:
+------+-----------+-----+------+--------+------------+
| 0x03 | 'Michael' | 0x23| 'IT' | 0xEA60 | 0x5D2E8A00 |
+------+-----------+-----+------+--------+------------+

STEP 5: Decode and Return
Convert binary to readable format:
ID: 3
Name: 'Michael'
Age: 35
Department: 'IT'
Salary: 60000
JoinDate: '2019-06-10'
```

**Total IOs: 4 (3 for index + 1 for data)**

**Visual Representation:**

```
Query: WHERE id = 3
         ↓
   [Index Lookup]
    B-Tree Navigation
    3 page reads
         ↓
   Found: Page 1, Slot 3
         ↓
   [Read Data Page]
    1 page read
         ↓
   Extract Row 3 bytes
         ↓
   [Return Result]
   Michael, 35, IT, 60000
```

---

##### **Query Type 2: Full Table Scan**

**Query:**

```sql
SELECT * FROM employees WHERE age = 30;
```

**Internal Process (No Index on Age):**

````
STEP 1: Query Planning
- Optimizer sees WHERE age = 30
- Checks for index on 'age' column → NONE
- Decision: Full Table Scan (Sequential Scan)

STEP 2: Sequential Page Reading
Read Page 1 (IO #1):
+------------------------------------------------------------------+
| Row 1: [1][John][30][IT][50000][2020-01-15]      ← age=30 ✓    |
| Row 2: [2][Sarah][28][HR][45000][2021-03-20]     ← age=28 ✗    |
| Row 3: [3][Michael][35][IT][60000][2019-06-10]   ← age=35 ✗    |
+------------------------------------------------------------------+

Check each row:

- Extract age field from each row
- Compare with 30
- Row 1 matches! Buffer this result.

Read Page 2 (IO #2):
+------------------------------------------------------------------+
| Row 4: [4][Alice][32][Finance][55000][2020-08-25] ← age=32 ✗     |
| Row 5: [5][Bob][29][HR][47000][2021-11-05] ← age=29 ✗            |
| Row 6: [6][Emma][27][IT][52000][2022-05-15] ← age=27 ✗           |
+------------------------------------------------------------------+
No matches in Page 2

STEP 3: Return Results
Only Row 1 (John) matched

Total IOs: 2 (read all pages)
Rows Examined: 6 (all rows)
Rows Returned: 1 (only John)

```

**Key Point**: In row-based storage, even though we only need the 'age' column to filter, we must read ALL columns of each row because they're stored together.

**Performance Impact:**

```

If table has 1,000,000 rows across 20,000 pages:

- Must read all 20,000 pages
- Check 1,000,000 rows
- Very slow without index!

````

---

##### **Query Type 3: Aggregate Query**

**Query:**

```sql
SELECT department, AVG(salary) as avg_salary
FROM employees
GROUP BY department;
```

**Internal Process:**

```
STEP 1: Full Table Scan
Must read ALL rows to calculate averages

Read Page 1:
Row 1: department='IT', salary=50000
Row 2: department='HR', salary=45000
Row 3: department='IT', salary=60000

Read Page 2:
Row 4: department='Finance', salary=55000
Row 5: department='HR', salary=47000
Row 6: department='IT', salary=52000

STEP 2: Build Hash Table for Grouping
Memory Structure:
{
  'IT': {sum: 162000, count: 3},        // 50000+60000+52000
  'HR': {sum: 92000, count: 2},         // 45000+47000
  'Finance': {sum: 55000, count: 1}     // 55000
}

STEP 3: Calculate Averages
IT: 162000 / 3 = 54000
HR: 92000 / 2 = 46000
Finance: 55000 / 1 = 55000

STEP 4: Return Results
+------------+------------+
| department | avg_salary |
+------------+------------+
| IT         | 54000      |
| HR         | 46000      |
| Finance    | 55000      |
+------------+------------+

Total IOs: 2 pages
Problem: Had to read ALL columns even though we only needed
'department' and 'salary'!
```

**Inefficiency Visualization:**

```
What we NEED:           What we READ:
[Department] [Salary]   [ID][Name][Age][Department][Salary][JoinDate]
     ↓          ↓           ↑    ↑    ↑       ↑        ↑        ↑
   Used       Used       Waste Waste Waste  Used     Used     Waste

Wasted IO: ~66% of data read was unnecessary!
```

---

#### **3. UPDATE Operation**

**Query:**

```sql
UPDATE employees
SET salary = 65000
WHERE id = 3;
```

**Internal Process:**

```
STEP 1: Find Row (Same as SELECT)
- Use PRIMARY KEY index on id
- Navigate B-Tree: 3 IOs to find Row_ID
- Read data page: Page 1, Slot 3 (1 IO)
- Total: 4 IOs to locate row

STEP 2: Row Before Update (Page 1, Slot 3)
+------+-----------+-----+------+--------+------------+
| 0x03 | 'Michael' | 0x23| 'IT' | 0xEA60 | 0x5D2E8A00 |
|  3   | 'Michael' | 35  | 'IT' | 60000  | 2019-06-10 |
+------+-----------+-----+------+--------+------------+
50 bytes total

STEP 3: Modify Row in Memory
Only change salary field:
+------+-----------+-----+------+--------+------------+
| 0x03 | 'Michael' | 0x23| 'IT' | 0xFDE8 | 0x5D2E8A00 |
|  3   | 'Michael' | 35  | 'IT' | 65000  | 2019-06-10 |
+------+-----------+-----+------+--------+------------+
Still 50 bytes (in-place update possible)

STEP 4: Check if Update Fits
New salary: 65000 (same size as 60000 - both integers)
Decision: In-place update (no row movement needed)

STEP 5: Write Updated Row Back
Page 1 after update:
+------------------------------------------------------------------+
| Row 1: [1][John][30][IT][50000][2020-01-15]                    |
| Row 2: [2][Sarah][28][HR][45000][2021-03-20]                   |
| Row 3: [3][Michael][35][IT][65000][2019-06-10]  ← UPDATED!     |
+------------------------------------------------------------------+

Mark page as "dirty" in buffer pool

STEP 6: Update Indexes
If index on 'salary' exists:
Old Index Entry: 60000 → Row_ID(Page 1, Slot 3)
  ↓ Delete
New Index Entry: 65000 → Row_ID(Page 1, Slot 3)
  ↓ Insert

B-Tree before:
... [55000] [60000] [70000] ...
             ↓
        Row_ID(P1,S3)

B-Tree after:
... [55000] [65000] [70000] ...
             ↓
        Row_ID(P1,S3)

STEP 7: Transaction Log
Write to WAL:
"UPDATE Page 1, Slot 3: salary changed from 60000 to 65000"
Old value logged for potential rollback

STEP 8: Commit
- Transaction marked complete
- Dirty page eventually flushed to disk
```

**Complex Update Scenario (Row Expansion):**

```sql
UPDATE employees
SET name = 'Michael Anderson'
WHERE id = 3;
```

```
Problem: New name is longer!
Old: 'Michael' = 7 chars
New: 'Michael Anderson' = 17 chars
Difference: +10 bytes

STEP 1: Check if row fits in current slot
Current slot: 50 bytes
New size needed: 60 bytes
Page 1 free space: 100 bytes available
Decision: Can fit in current page

STEP 2: Move row to end of page
Page 1 before:
+------------------------------------------------------------------+
| Row 1: [1][John][30][IT][50000][2020-01-15]                    |
| Row 2: [2][Sarah][28][HR][45000][2021-03-20]                   |
| Row 3: [3][Michael][35][IT][65000][2019-06-10]                 |
| [100 bytes free space]                                          |
+------------------------------------------------------------------+

Page 1 after:
+------------------------------------------------------------------+
| Row 1: [1][John][30][IT][50000][2020-01-15]                    |
| Row 2: [2][Sarah][28][HR][45000][2021-03-20]                   |
| [Forwarding Pointer to Row 3']  ← Old Slot 3 location          |
| Row 3': [3][Michael Anderson][35][IT][65000][2019-06-10]       |
| [40 bytes free space]                                           |
+------------------------------------------------------------------+

Now Slot 3 contains a "forwarding pointer" to new location
This causes "row chaining" - slight performance penalty
```

**If page doesn't have space:**

```
Row moves to different page (Page 3)
Old location (Page 1, Slot 3): Forwarding pointer → Page 3, Slot 1
New location (Page 3, Slot 1): Actual row data

Future reads require 2 page accesses:
1. Read Page 1 to get forwarding pointer
2. Read Page 3 to get actual data

This is called "row migration" - bad for performance!
```

---

#### **4. DELETE Operation**

**Query:**

```sql
DELETE FROM employees WHERE id = 5;
```

**Internal Process:**

```
STEP 1: Locate Row
- Use PRIMARY KEY index on id
- Find Row 5: Page 2, Slot 2

STEP 2: Mark Row as Deleted (Soft Delete)
Page 2 before:
+------------------------------------------------------------------+
| Row 4: [4][Alice][32][Finance][55000][2020-08-25]              |
| Row 5: [5][Bob][29][HR][47000][2021-11-05]                     |
| Row 6: [6][Emma][27][IT][52000][2022-05-15]                    |
| [Free Space: 7.85KB]                                            |
+------------------------------------------------------------------+

Page 2 after (logical delete):
+------------------------------------------------------------------+
| Row 4: [4][Alice][32][Finance][55000][2020-08-25]              |
| Row 5: [DELETED_TOMBSTONE]                                      |
| Row 6: [6][Emma][27][IT][52000][2022-05-15]                    |
| [Free Space: 7.90KB]  ← Increased by 50 bytes                  |
+------------------------------------------------------------------+

Tombstone marks space as reusable but doesn't physically remove

STEP 3: Update Index Entries
For each index, remove references:

PRIMARY KEY index on 'id':
Delete: 5 → Row_ID(Page 2, Slot 2)

Index on 'name':
Delete: 'Bob' → Row_ID(Page 2, Slot 2)

Index on 'salary':
Delete: 47000 → Row_ID(Page 2, Slot 2)

STEP 4: Transaction Log
Write to WAL:
"DELETE Page 2, Slot 2: [5, Bob, 29, HR, 47000, 2021-11-05]"
Old values saved for potential rollback

STEP 5: Space Reclamation
The space is now available for future INSERTs
Next INSERT may reuse Slot 2 location

Later, VACUUM process (PostgreSQL) or equivalent:
- Physically removes tombstones
- Compacts page
- Reclaims disk space
```

**Visual Representation of Fragmentation:**

```
After multiple DELETEs:
Page 2:
+------------------------------------------------------------------+
| Row 4: [4][Alice][32][Finance][55000][2020-08-25]                |
| [DELETED_TOMBSTONE] ← Row 5 deleted                              |
| Row 6: [6][Emma][27][IT][52000][2022-05-15]                      |
| [DELETED_TOMBSTONE] ← Another row deleted                        |
| Row 8: [8][Frank][31][IT][53000][2023-01-10]                     |
| [Fragmented free space: 5KB scattered]                           |
+------------------------------------------------------------------+

Problem: "Swiss cheese" effect - wasted space between active rows
Solution: Periodic VACUUM/REORGANIZE to compact pages
```

---

### **Row-Based Storage: Advantages and Disadvantages**

#### **Advantages:**

1. **Excellent for OLTP (Online Transaction Processing):**

```
Typical OLTP query:
SELECT * FROM orders WHERE order_id = 12345;

Performance:
- Single row retrieval
- All columns needed
- Perfect fit for row storage
- Fast: ~4 IOs (index + data)
```

2. **Efficient for row-level operations:**

```
UPDATE users SET email = 'new@email.com' WHERE user_id = 789;

Only need to:
- Locate one row (fast with index)
- Update in place
- Minimal IO
```

3. **Good for random access patterns:**

```
// Application code accessing different rows randomly
getUser(101);  // Read one complete row
getUser(534);  // Read another complete row
getUser(892);  // Read another complete row
```

4. **Write-optimized:**

```
INSERT: Just append row to page (O(1))
No need to update multiple column files
```

#### **Disadvantages:**

1. **Inefficient for analytical queries:**

```
SELECT AVG(salary) FROM employees;

Problem:
- Need only 1 column (salary)
- Must read ALL columns
- Waste: Read 6 columns, use 1
- Inefficient IO usage
```

2. **Poor compression:**

```
Row format limits compression:
Row 1: [1, 'John', 30, 'IT', 50000]
Row 2: [2, 'Jane', 31, 'IT', 51000]
Row 3: [3, 'Jack', 30, 'IT', 50000]

Can't compress repeated values well because
similar data is scattered across rows
```

3. **Cache inefficiency for column-specific queries:**

```
SELECT name FROM employees;

Issue:
Load entire row into CPU cache
CPU cache: [1, 'John', 30, 'IT', 50000, ...]
           ↑    ↑     ↑    ↑      ↑
         Waste  Use  Waste Waste Waste

CPU cache pollution with unwanted data
```

---

## **Part 2: Column-Based (Column-Oriented) Databases**

### **What is a Column-Based Database?**

A column-based database stores data **column by column** on disk. All values for a single column are stored together contiguously. This is used by analytical databases like Amazon Redshift, Google BigQuery, Apache Cassandra (wide-column), ClickHouse, and Snowflake.

Think of it like reading a spreadsheet vertically: you read all values in one column from top to bottom, then move to the next column.

---

### **How Column-Based Storage Works Internally**

#### **Physical Storage Layout**

Same EMPLOYEES table, but stored column-wise:

```
EMPLOYEES Table (Logical View):
+----+----------+-----+------------+--------+----------+
| ID | Name     | Age | Department | Salary | JoinDate |
+----+----------+-----+------------+--------+----------+
| 1  | John     | 30  | IT         | 50000  | 2020-01-15 |
| 2  | Sarah    | 28  | HR         | 45000  | 2021-03-20 |
| 3  | Michael  | 35  | IT         | 60000  | 2019-06-10 |
| 4  | Alice    | 32  | Finance    | 55000  | 2020-08-25 |
| 5  | Bob      | 29  | HR         | 47000  | 2021-11-05 |
+----+----------+-----+------------+--------+----------+
```

**Column-Based Physical Storage on Disk:**

```
SEPARATE FILES/PAGES FOR EACH COLUMN:

ID Column File:
Page 1:
+----------------------------------+
| [1] [2] [3] [4] [5]              |
| 4B  4B  4B  4B  4B               |
+----------------------------------+
All IDs together!

Name Column File:
Page 1:
+----------------------------------+
| ['John']['Sarah']['Michael']     |
| ['Alice']['Bob']                 |
+----------------------------------+
All names together!

Age Column File:
Page 1:
+----------------------------------+
| [30] [28] [35] [32] [29]         |
| 4B   4B   4B   4B   4B           |
+----------------------------------+
All ages together!

Department Column File:
Page 1:
+----------------------------------+
| ['IT']['HR']['IT']['Finance']    |
| ['HR']                           |
+----------------------------------+
All departments together!

Salary Column File:
Page 1:
+----------------------------------+
| [50000][45000][60000][55000]     |
| [47000]                          |
| 4B     4B     4B     4B     4B   |
+----------------------------------+
All salaries together!

JoinDate Column File:
Page 1:
+----------------------------------+
| [2020-01-15][2021-03-20]...      |
+----------------------------------+
All join dates together!
```

**Key Difference Visualization:**

```
ROW-BASED (Reading Row 3):
Page 1: [1,John,30,IT,50000,date1][2,Sarah,28,HR,45000,date2]
        [3,Michael,35,IT,60000,date3]  ← Read entire row
        ↑      ↑   ↑  ↑    ↑       ↑
       All columns read together

COLUMN-BASED (Reading Row 3):
ID File:     [1][2][3][4][5]  ← Read position 3 = value 3
                   ↑
Name File:   [John][Sarah][Michael][Alice][Bob]
                          ↑          Read position 3 = 'Michael'
Age File:    [30][28][35][32][29]
                     ↑             Read position 3 = 35
... and so on for each column

Position/row number is implicit (array index)
```

---

### **Column-Based Storage with Compression**

One of the biggest advantages of columnar storage is **compression**. Let's see how:

```
Department Column (Before Compression):
+----------------------------------+
| ['IT']['HR']['IT']['Finance']   |
| ['HR']['IT']['IT']['HR']        |
| ['IT']['Finance']['IT']...      |
+----------------------------------+
168 bytes (6 chars × 7 values × 4 bytes)

Notice: Lots of repetition!
'IT' appears many times
'HR' appears many times
'Finance' appears fewer times

Department Column (After Dictionary Encoding):
Dictionary:
0 → 'IT'
1 → 'HR'
2 → 'Finance'

Encoded Values:
+----------------------------------+
| [0][1][0][2][1][0][0][1][0][2]  |
| [0]...                          |
+----------------------------------+
11 bytes (just store integers 0-2)

Plus dictionary overhead: 20 bytes

Total: 31 bytes vs 168 bytes
Compression ratio: 81% space saved!

With Run-Length Encoding (RLE):
If column has: IT, IT, IT, HR, HR, IT, IT, IT
Store as: (IT, 3), (HR, 2), (IT, 3)
Even better compression!
```

**Real-World Compression Example:**

```
Salary Column (100,000 employees):
Values: 45000, 45000, 45000, 50000, 50000, 55000, 60000, 60000...

Without compression:
100,000 values × 4 bytes = 400,000 bytes = 400KB

With Dictionary + RLE:
Dictionary: [45000, 50000, 55000, 60000, 65000, 70000]
            (6 unique values × 4 bytes = 24 bytes)

RLE encoding:
(45000, 35000 times)  → 8 bytes
(50000, 40000 times)  → 8 bytes
(55000, 15000 times)  → 8 bytes
(60000, 10000 times)  → 8 bytes

Total: 24 + 32 = 56 bytes vs 400KB
Compression ratio: 99.986% space saved!
```

---

### **CRUD Operations in Column-Based Storage**

Now let's explore how operations work differently in columnar storage:

---

#### **1. CREATE (INSERT) Operation**

**Query:**

```sql
INSERT INTO employees (id, name, age, department, salary, joindate)
VALUES (6, 'Emma', 27, 'IT', 52000, '2022-05-15');
```

**Internal Process:**

````
STEP 1: Parse and Validate (same as row-based)
New row: [6, 'Emma', 27, 'IT', 52000, '2022-05-15']

STEP 2: Decompose Row into Column Values
Split into individual column values:
- ID: 6
- Name: 'Emma'
- Age: 27
- Department: 'IT'
- Salary: 52000
- JoinDate: '2022-05-15'

STEP 3: Append to EACH Column File
This is MUCH more complex than row-based!

ID Column File:
Before: [1][2][3][4][5]
After:  [1][2][3][4][5][6]  ← Append 6
+1 IO (read + write)

Name Column File:
Before: ['John']['Sarah']['Michael']['Alice']['Bob']
After:  ['John']['Sarah']['Michael']['Alice']['Bob']['Emma']
+1 IO

Age Column File:
Before: [30][28][35][32][29]
After:  [30][28][35][32][29][27]
+1 IO ```
Department Column File:
Before: ['IT']['HR']['IT']['Finance']['HR']
After:  ['IT']['HR']['IT']['Finance']['HR']['IT']
+1 IO

BUT! With compression (Dictionary Encoding):
Dictionary: {0: 'IT', 1: 'HR', 2: 'Finance'}
Before: [0][1][0][2][1]
After:  [0][1][0][2][1][0]  ← Just append encoded value
+1 IO

Salary Column File:
Before: [50000][45000][60000][55000][47000]
After:  [50000][45000][60000][55000][47000][52000]
+1 IO

JoinDate Column File:
Before: [date1][date2][date3][date4][date5]
After:  [date1][date2][date3][date4][date5][date6]
+1 IO

STEP 4: Ensure Positional Alignment
CRITICAL: All column files must maintain same row order!

Position 0: ID=1, Name='John',    Age=30, Dept='IT', ...
Position 1: ID=2, Name='Sarah',   Age=28, Dept='HR', ...
Position 2: ID=3, Name='Michael', Age=35, Dept='IT', ...
...
Position 5: ID=6, Name='Emma',    Age=27, Dept='IT', ...

All columns must have 6 values now

STEP 5: Update Metadata
Row count: 5 → 6
Each column file size increased

STEP 6: Transaction Log
"INSERT row at position 5: ID=6, Name='Emma', ..."

Total IOs: 6+ (one per column file + metadata)
Compare to row-based: 1-2 IOs

PERFORMANCE IMPACT:
Row-Based INSERT:  Fast (1 write to 1 page)
Column-Based INSERT: Slow (6 writes to 6 different files)
````

**Optimization Strategies for Column Stores:**

```
Many columnar databases use WRITE BUFFERS:

Memory Buffer (In-Row Format):
+------------------------------------------------------------------+
| [6]['Emma'][27]['IT'][52000]['2022-05-15']                       |
| [7]['Frank'][30]['HR'][48000]['2023-01-10']                      |
| [8]['Grace'][29]['IT'][51000]['2023-02-15']                      |
+------------------------------------------------------------------+
Accumulate inserts in row format (fast)

When buffer is full (e.g., 1000 rows):
Batch conversion to columnar format:
- ID column:    [6][7][8]...
- Name column:  ['Emma']['Frank']['Grace']...
- Age column:   [27][30][29]...
...

Bulk append to column files (more efficient)

This is why column stores often have:
- Eventual consistency
- Batch loading recommended
- Micro-batching architecture
```

**Visual Comparison:**

```
ROW-BASED INSERT (Fast):
                    ↓ Single write
[Row1][Row2][Row3][NewRow]
                    ↑
                 Page 2

COLUMN-BASED INSERT (Slow):
        ↓           ↓           ↓           ↓           ↓
[ID File]  [Name File]  [Age File]  [Dept File]  [Salary File]
    ↓           ↓           ↓           ↓           ↓
 6 separate write operations needed!
```

---

#### **2. READ (SELECT) Operation**

This is where columnar storage **shines**! Let's explore various query types:

---

##### **Query Type 1: Single Column Selection**

**Query:**

```sql
SELECT name FROM employees;
```

**Internal Process (Column-Based):**

```
STEP 1: Query Planning
- Optimizer sees: Only 'name' column needed
- Decision: Read ONLY name column file
- Ignore all other column files!

STEP 2: Read Name Column File
Name Column Page:
+----------------------------------+
| ['John']['Sarah']['Michael']     |
| ['Alice']['Bob']['Emma']         |
+----------------------------------+
1 IO to read entire column

STEP 3: Decompress (if compressed)
If dictionary encoded:
Encoded: [0][1][2][3][4][5]
Dictionary: {0:'John', 1:'Sarah', 2:'Michael', 3:'Alice', 4:'Bob', 5:'Emma'}

Decode to:
['John']['Sarah']['Michael']['Alice']['Bob']['Emma']

STEP 4: Return Results
+----------+
| Name     |
+----------+
| John     |
| Sarah    |
| Michael  |
| Alice    |
| Bob      |
| Emma     |
+----------+

Total IOs: 1 (only name column file)
```

**Compare with Row-Based:**

```
ROW-BASED APPROACH:
Must read ALL columns even though we only need 'name'

Read Page 1:
[1,John,30,IT,50000,date] ← Extract 'John'
[2,Sarah,28,HR,45000,date] ← Extract 'Sarah'
[3,Michael,35,IT,60000,date] ← Extract 'Michael'

Read Page 2:
[4,Alice,32,Finance,55000,date] ← Extract 'Alice'
[5,Bob,29,HR,47000,date] ← Extract 'Bob'
[6,Emma,27,IT,52000,date] ← Extract 'Emma'

Total IOs: 2 pages
Data read: ALL columns (wasted ~83% of IO)

COLUMN-BASED APPROACH:
Total IOs: 1 page (name column only)
Data read: Only needed column (0% waste)

Performance: Column-based is 2x+ faster here!
With 10 columns: Could be 10x faster!
```

---

##### **Query Type 2: Aggregate with Filter**

**Query:**

```sql
SELECT AVG(salary)
FROM employees
WHERE department = 'IT';
```

**Internal Process (Column-Based):**

```
STEP 1: Query Planning
- Need: 'salary' and 'department' columns
- Don't need: ID, name, age, joindate
- Decision: Read only 2 column files

STEP 2: Read Department Column File
Department Column (compressed):
Dictionary: {0:'IT', 1:'HR', 2:'Finance'}
Encoded: [0][1][0][2][1][0]

Position 0: IT ✓
Position 1: HR ✗
Position 2: IT ✓
Position 3: Finance ✗
Position 4: HR ✗
Position 5: IT ✓

Matching positions: [0, 2, 5]
1 IO

STEP 3: Read Salary Column File (Only Needed Positions)
This is where it gets interesting!

Salary Column:
[50000][45000][60000][55000][47000][52000]

OPTIMIZATION: Some column stores use "late materialization"
Only read positions [0, 2, 5]:
- Position 0: 50000
- Position 2: 60000
- Position 5: 52000

1 IO (or partial IO if system supports it)

STEP 4: Calculate Aggregate
Sum: 50000 + 60000 + 52000 = 162000
Count: 3
Average: 162000 / 3 = 54000

STEP 5: Return Result
+------------+
| avg_salary |
+------------+
| 54000      |
+------------+

Total IOs: 2 (department + salary columns only)
Columns read: 2 out of 6 (67% IO saved!)
```

**Row-Based Same Query:**

```
Must read ALL columns for ALL rows:

Page 1:
[1,John,30,IT,50000,date]     ← Use salary=50000
[2,Sarah,28,HR,45000,date]    ← Skip (HR)
[3,Michael,35,IT,60000,date]  ← Use salary=60000

Page 2:
[4,Alice,32,Finance,55000,date] ← Skip (Finance)
[5,Bob,29,HR,47000,date]        ← Skip (HR)
[6,Emma,27,IT,52000,date]       ← Use salary=52000

Total IOs: 2 pages
But read ALL 6 columns × 6 rows = 36 column-row values
Actually used: 2 columns × 3 rows = 6 column-row values
Efficiency: 16.7%

Column-Based Efficiency: 100% (read exactly what's needed)
```

**Visual Comparison:**

```
ROW-BASED (Inefficient):
+----------------------------------------------------------+
| ID | Name | Age | Dept | Salary | Date |                 |
|----|------|-----|------|--------|------|                 |
| ✗  |  ✗   | ✗   |  ✓   |   ✓    |  ✗   | Row 1 (IT)   |
| ✗  |  ✗   | ✗   |  ✗   |   ✗    |  ✗   | Row 2 (HR)   |
| ✗  |  ✗   | ✗   |  ✓   |   ✓    |  ✗   | Row 3 (IT)   |
+----------------------------------------------------------+
Read 18 values, used 4 = 22% efficiency

COLUMN-BASED (Efficient):
Dept Column:  [IT] [HR] [IT] [Finance] [HR] [IT]
              ✓    ✗    ✓      ✗        ✗    ✓
Salary Column: Read only positions [0,2,5]
              [50000]     [60000]     [52000]
              ✓           ✓           ✓
Read 9 values, used 9 = 100% efficiency
```

---

##### **Query Type 3: Complex Analytical Query**

**Query:**

```sql
SELECT department,
       COUNT(*) as emp_count,
       AVG(salary) as avg_salary,
       MIN(age) as min_age,
       MAX(age) as max_age
FROM employees
GROUP BY department;
```

**Internal Process (Column-Based):**

```
STEP 1: Identify Required Columns
- department: for grouping
- salary: for AVG
- age: for MIN/MAX
- Don't need: ID, name, joindate

Read only 3 column files instead of 6!

STEP 2: Read Department Column (Compressed)
Department Column File:
Dictionary: {0:'IT', 1:'HR', 2:'Finance'}
Encoded: [0][1][0][2][1][0]

With Run-Length Encoding awareness:
Position 0: IT
Position 1: HR
Position 2: IT
Position 3: Finance
Position 4: HR
Position 5: IT

Create bitmap/position list for each group:
IT:      positions [0, 2, 5]
HR:      positions [1, 4]
Finance: positions [3]

1 IO

STEP 3: Read Salary Column
Salary Column:
[50000][45000][60000][55000][47000][52000]

Group by department:
IT:      positions [0,2,5] → [50000, 60000, 52000]
HR:      positions [1,4]   → [45000, 47000]
Finance: positions [3]     → [55000]

1 IO

STEP 4: Read Age Column
Age Column:
[30][28][35][32][29][27]

Group by department:
IT:      positions [0,2,5] → [30, 35, 27]
HR:      positions [1,4]   → [28, 29]
Finance: positions [3]     → [32]

1 IO

STEP 5: Compute Aggregates
Processing in memory:

IT Group:
  COUNT: 3 employees
  AVG(salary): (50000+60000+52000)/3 = 54000
  MIN(age): min(30,35,27) = 27
  MAX(age): max(30,35,27) = 35

HR Group:
  COUNT: 2 employees
  AVG(salary): (45000+47000)/2 = 46000
  MIN(age): min(28,29) = 28
  MAX(age): max(28,29) = 29

Finance Group:
  COUNT: 1 employee
  AVG(salary): 55000/1 = 55000
  MIN(age): 32
  MAX(age): 32

STEP 6: Return Results
+------------+-----------+------------+---------+---------+
| department | emp_count | avg_salary | min_age | max_age |
+------------+-----------+------------+---------+---------+
| IT         | 3         | 54000      | 27      | 35      |
| HR         | 2         | 46000      | 28      | 29      |
| Finance    | 1         | 55000      | 32      | 32      |
+------------+-----------+------------+---------+---------+

Total IOs: 3 (dept, salary, age columns only)
Data read: 3 columns × 6 rows = 18 values
Data needed: 3 columns × 6 rows = 18 values
Efficiency: 100%

With compression, actual disk read is even less!
```

**Row-Based Same Query:**

```
Must read entire table:

Page 1:
[1,John,30,IT,50000,date]
[2,Sarah,28,HR,45000,date]
[3,Michael,35,IT,60000,date]

Page 2:
[4,Alice,32,Finance,55000,date]
[5,Bob,29,HR,47000,date]
[6,Emma,27,IT,52000,date]

Total IOs: 2 pages
Data read: 6 columns × 6 rows = 36 values
Data needed: 3 columns × 6 rows = 18 values
Efficiency: 50%

Column-based is 2× more efficient!
With more columns (e.g., 20 columns), could be 6-7× faster!
```

**Advanced Optimization - Vectorized Processing:**

```
Modern column stores use SIMD (Single Instruction Multiple Data):

Age Column: [30][28][35][32][29][27]
                ↓
Load into CPU vector register (process 4 at once):
Register: [30, 28, 35, 32]
          ↓
Find MIN using single CPU instruction:
MIN([30, 28, 35, 32]) = 28 (in 1 cycle!)

Next vector:
Register: [29, 27]
MIN([29, 27]) = 27

Final MIN: min(28, 27) = 27

Compare to row-based (must compare one by one):
min_age = 30
min_age = min(30, 28) = 28
min_age = min(28, 35) = 28
min_age = min(28, 32) = 28
min_age = min(28, 29) = 28
min_age = min(28, 27) = 27

6 comparisons vs 2 vector operations = 3× faster!
```

---

##### **Query Type 4: Point Query (Single Row Lookup)**

**Query:**

```sql
SELECT * FROM employees WHERE id = 3;
```

**Internal Process (Column-Based):**

```
STEP 1: Find Row Position
Need to locate which position has ID=3

Read ID Column:
[1][2][3][4][5][6]
      ↑
   Position 2 has ID=3

1 IO

STEP 2: Read All Columns at Position 2
This is expensive in column stores!

ID Column:
Position 2 → 3
(1 IO - already done)

Name Column:
[John][Sarah][Michael][Alice][Bob][Emma]
              ↑
Position 2 → 'Michael'
(1 IO)

Age Column:
[30][28][35][32][29][27]
        ↑
Position 2 → 35
(1 IO)

Department Column:
[IT][HR][IT][Finance][HR][IT]
        ↑
Position 2 → 'IT'
(1 IO)

Salary Column:
[50000][45000][60000][55000][47000][52000]
              ↑
Position 2 → 60000
(1 IO)

JoinDate Column:
[date1][date2][date3][date4][date5][date6]
              ↑
Position 2 → '2019-06-10'
(1 IO)

STEP 3: Assemble Row
Combine all column values at position 2:
+----+---------+-----+------------+--------+------------+
| ID | Name    | Age | Department | Salary | JoinDate   |
+----+---------+-----+------------+--------+------------+
| 3  | Michael | 35  | IT         | 60000  | 2019-06-10 |
+----+---------+-----+------------+--------+------------+

Total IOs: 6 (one per column)
Very inefficient for single row retrieval!
```

**Row-Based Same Query:**

```
STEP 1: Use index on ID
Navigate B-Tree to find ID=3
Row_ID: Page 1, Slot 3

3 IOs (index navigation)

STEP 2: Read Row
Read Page 1, extract Slot 3:
[3][Michael][35][IT][60000][2019-06-10]

1 IO

Total IOs: 4
Much more efficient than column-based!
```

**Performance Comparison:**

```
POINT QUERY (SELECT * WHERE id = X):
Row-Based:    4 IOs ✓ WINNER
Column-Based: 6 IOs ✗ SLOWER

This is why column stores often use:
- Row caching
- Hybrid storage (hot data in row format)
- Tuple reconstruction optimization
```

**Optimization Strategy - Mini Row Groups:**

```
Some column stores use "row groups":

Row Group 1 (positions 0-999):
  ID Column Segment:    [1...1000]
  Name Column Segment:  [names...]
  Age Column Segment:   [ages...]
  ...

Row Group 2 (positions 1000-1999):
  ID Column Segment:    [1001...2000]
  ...

For point query WHERE id = 1523:
1. Identify Row Group 2 (contains 1523)
2. Read only columns from Row Group 2
3. Find position within group
4. Extract values

Reduces IO by reading smaller segments
```

---

#### **3. UPDATE Operation**

**Query:**

```sql
UPDATE employees
SET salary = 65000
WHERE id = 3;
```

**Internal Process (Column-Based):**

```
STEP 1: Locate Row Position
Read ID Column to find position:
[1][2][3][4][5][6]
      ↑
Position 2 has ID=3

1 IO

STEP 2: Update Salary Column at Position 2
Salary Column Before:
[50000][45000][60000][55000][47000][52000]
              ↑
           Position 2

Salary Column After:
[50000][45000][65000][55000][47000][52000]
              ↑
       Updated to 65000

1 IO (read + write)

STEP 3: Handle Compression
If salary column is compressed:

Before:
Dictionary: {45000, 47000, 50000, 55000, 60000, 52000}
Encoded: [2][0][4][3][1][5]

After:
Dictionary: {45000, 47000, 50000, 55000, 60000, 52000, 65000}
                                                        ↑ new
Encoded: [2][0][6][3][1][5]
             ↑
        Updated reference

Compression overhead: May need to rebuild dictionary
Additional IO for dictionary update

Total IOs: 2-3 (ID column + Salary column + dictionary)
```

**Complex Update (Multiple Columns):**

```sql
UPDATE employees
SET salary = 70000, department = 'Management'
WHERE id = 3;
```

```
STEP 1: Find position (same as before)
Position 2

STEP 2: Update Salary Column
[50000][45000][60000→70000][55000][47000][52000]
1 IO

STEP 3: Update Department Column
Before:
Dictionary: {0:'IT', 1:'HR', 2:'Finance'}
Encoded: [0][1][0][2][1][0]

'Management' is NEW value!
Update dictionary: {0:'IT', 1:'HR', 2:'Finance', 3:'Management'}
Encoded: [0][1][3][2][1][0]
                ↑
            Changed to 3

2 IOs (read dictionary, update column, write dictionary)

Total IOs: 4+ (ID + Salary + Department + metadata)

Compare to row-based: 2-3 IOs
Column-based updates are SLOWER!
```

**Update Optimization - Write Buffer:**

```
Many column stores don't update in place!

Instead, they use Delta Stores:

MAIN STORAGE (Immutable, compressed):
ID:     [1][2][3][4][5][6]
Salary: [50000][45000][60000][55000][47000][52000]

DELTA STORAGE (Mutable, row-format):
+----------+----------+---------------+
| Position | Column   | New Value     |
+----------+----------+---------------+
| 2        | Salary   | 65000         |
+----------+----------+---------------+

Query Process:
1. Read from main storage: Position 2 salary = 60000
2. Check delta storage: Position 2 has update → 65000
3. Return merged result: 65000

Periodically, delta is merged into main storage
(usually during maintenance window)

Benefits:
- Maintains compression
- Faster updates
- Batch processing
```

---

#### **4. DELETE Operation**

**Query:**

```sql
DELETE FROM employees WHERE id = 5;
```

**Internal Process (Column-Based):**

```
STEP 1: Find Row Position
Read ID Column:
[1][2][3][4][5][6]
            ↑
        Position 4

1 IO

STEP 2: Mark as Deleted (Tombstone Approach)
Column stores typically don't delete immediately!

Create deletion bitmap:
Position: [0][1][2][3][4][5]
Deleted:  [0][0][0][0][1][0]
                      ↑
               Position 4 marked deleted

1 IO (update bitmap)

STEP 3: All Columns Still Contain Data
ID Column:     [1][2][3][4][5][6]
Name Column:   [John][Sarah][Michael][Alice][Bob][Emma]
Age Column:    [30][28][35][32][29][27]
...

Data physically remains but marked as deleted

STEP 4: Query Processing with Deletions
Future queries check bitmap:

SELECT * FROM employees;

Process:
- Read all columns
- Filter out position 4 (marked deleted)
- Return positions: 0,1,2,3,5

Result:
+----+---------+-----+
| 1  | John    | 30  |
| 2  | Sarah   | 28  |
| 3  | Michael | 35  |
| 4  | Alice   | 32  | ← Skipped!
| 6  | Emma    | 27  |
+----+---------+-----+

Total IOs for DELETE: 2 (ID column + bitmap)
Actual deletion deferred!
```

**Physical Deletion (Compaction):**

```
Periodically, system runs VACUUM/COMPACT:

Before Compaction:
ID Column:        [1][2][3][4][5][6]
Deletion Bitmap:  [0][0][0][0][1][0]

After Compaction:
ID Column:        [1][2][3][4][6]  ← Position 4 removed!
Deletion Bitmap:  [0][0][0][0][0]  ← Reset

ALL Columns compacted:
Name:      [John][Sarah][Michael][Alice][Emma]
Age:       [30][28][35][32][27]
Department:[IT][HR][IT][Finance][IT]
Salary:    [50000][45000][60000][55000][52000]

Now positions shifted:
Old Position 5 (Emma) → New Position 4

Benefits:
- Reclaim space
- Improve compression
- Faster queries (no bitmap checks)

Cost:
- Rewrite all column files
- Expensive operation (done offline)
```

**Visual Representation:**

```
DELETE Process Timeline:

Time T0 (DELETE issued):
+------------------+
| Deletion Bitmap  |
| [0][0][0][1][0]  | ← Mark position 3
+------------------+
      ↓
   Quick operation (microseconds)

Time T0 - T1 (Queries):
All queries check bitmap before returning results
Slight overhead but manageable

Time T1 (Maintenance window):
+------------------+
| Run Compaction   |
| Remove deleted   |
| Rewrite columns  |
+------------------+
      ↓
   Slow operation (seconds to minutes)

Time T1+ (After compaction):
Clean storage, no bitmap overhead
```

---

### **Performance Comparison: Real-World Examples**

Let's compare realistic scenarios with actual numbers:

---

#### **Scenario 1: E-Commerce Analytics**

**Table**: orders (1 billion rows, 20 columns)

```
Table Schema:
- order_id (8 bytes)
- customer_id (8 bytes)
- order_date (8 bytes)
- order_amount (8 bytes)
- product_id (8 bytes)
- quantity (4 bytes)
- shipping_cost (8 bytes)
- tax (8 bytes)
- discount (8 bytes)
- status (20 bytes)
- payment_method (20 bytes)
- shipping_address (100 bytes)
... (8 more columns, 200 bytes total per row)
```

**Query: Monthly revenue report**

```sql
SELECT
    DATE_TRUNC('month', order_date) as month,
    SUM(order_amount) as total_revenue,
    COUNT(*) as num_orders
FROM orders
WHERE order_date >= '2024-01-01'
GROUP BY month
ORDER BY month;
```

**Row-Based Storage:**

```
Physical Layout:
1 billion rows × 200 bytes = 200 GB raw data

IOs Required:
- Must read entire table (all 20 columns)
- Pages size: 8KB
- Rows per page: 8192 / 200 ≈ 40 rows
- Total pages: 1,000,000,000 / 40 = 25 million pages

Read: 25 million pages × 8KB = 200 GB

Even with year 2024 filter (assume 100M rows):
- Still read: 100M rows × 200 bytes = 20 GB
- Used columns: order_date (8B) + order_amount (8B) = 16B per row
- Wasted: 184 bytes per row = 92% waste!
- Actual needed: 100M × 16B = 1.6 GB
- Read: 20 GB

Query Time: ~5-10 minutes (with good hardware)
```

**Column-Based Storage:**

```
Physical Layout (with compression):
order_date column: 1B rows × 8B = 8 GB
  → With compression (many repeat dates): 800 MB

order_amount column: 1B rows × 8B = 8 GB
  → With compression: 4 GB (more unique values)

Other 18 columns: Not read at all!

IOs Required:
- Read only 2 columns (order_date, order_amount)
- With compression: 800 MB + 4 GB = 4.8 GB
- Year 2024 filter: 480 MB + 400 MB = 880 MB

Query Time: ~10-30 seconds

Speedup: 10-20× faster!
Data scanned: 96% less!
```

---

#### **Scenario 2: Transactional System (OLTP)**

**Table**: user_profiles (10 million rows, 15 columns)

```
Typical Query Pattern:
SELECT * FROM user_profiles WHERE user_id = 12345;
```

**Row-Based Storage:**

```
With B-Tree index on user_id:
- Index lookup: 3-4 IOs (traverse B-Tree)
- Data fetch: 1 IO (read single row)
- Total: 4-5 IOs

Time: 5-10 milliseconds ✓ FAST

Data read: 200 bytes (one complete row)
Data needed: 200 bytes (all columns)
Efficiency: 100%
```

**Column-Based Storage:**

```
Without optimization:
- Find user_id position: 1 IO
- Read 15 column files: 15 IOs
- Total: 16 IOs

Time: 80-100 milliseconds ✗ SLOW

With optimization (row group cache):
- Find and cache row group: 5 IOs
- Total: 5-8 IOs
- Time: 25-40 milliseconds ✗ STILL SLOWER

Conclusion: Row-based is 2-10× faster for point queries!
```

---

#### **Scenario 3: Data Warehouse - Complex Analytics**

**Table**: customer_transactions (10 billion rows, 50 columns)

```
Complex Query:
SELECT
    customer_segment,
    product_category,
    COUNT(*) as transaction_count,
    SUM(amount) as total_amount,
    AVG(amount) as avg_amount,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY amount) as median_amount
FROM customer_transactions
WHERE transaction_date BETWEEN '2024-01-01' AND '2024-12-31'
  AND status = 'completed'
GROUP BY customer_segment, product_category
HAVING COUNT(*) > 1000
ORDER BY total_amount DESC;
```

**Row-Based Storage:**

```
Table size: 10B rows × 500 bytes/row = 5 TB

Query needs 5 columns:
- customer_segment
- product_category
- amount
- transaction_date
- status

Must read: ALL 50 columns for ALL rows
- Read entire year 2024: ~2 billion rows = 1 TB
- Used data: 2B rows × 5 columns × 10 bytes ≈ 100 GB
- Wasted: 900 GB read but not used (90% waste!)

Processing:
- Scan: 1 TB
- Filter: Complex predicates slow
- Sort: Large dataset
- Group: Memory intensive

Query Time: 20-60 minutes
Memory: 50-100 GB needed
```

**Column-Based Storage:**

```
With compression:
customer_segment: 2B rows → 200 MB (low cardinality)
product_category: 2B rows → 500 MB (low cardinality)
amount: 2B rows → 8 GB (high compression)
transaction_date: 2B rows → 1 GB (compression)
status: 2B rows → 100 MB (very low cardinality)

Total read: ~10 GB (with compression)

Processing:
- Scan: Only 10 GB (100× less!)
- Filter: Very fast (bitmap indexes on status)
- Predicate pushdown: Filter before decompression
- Vectorized processing: SIMD operations
- Columnar GROUP BY: Efficient

Query Time: 1-3 minutes

Speedup: 10-20× faster!
Memory: 10-20 GB needed (5× less)
```

---

### **Hybrid Approaches (Best of Both Worlds)**

Modern databases use hybrid strategies:

#### **1. PAX (Partition Attributes Across)**

```
Combines row and column storage within pages:

Traditional Row Store (NSM - N-ary Storage Model):
Page:
[Row1: col1,col2,col3,col4]
[Row2: col1,col2,col3,col4]
[Row3: col1,col2,col3,col4]

PAX (Within-Page Columnar):
Page:
[Col1: row1
```

,row2,row3]
[Col2: row1,row2,row3]
[Col3: row1,row2,row3]
[Col4: row1,row2,row3]

Benefits:

- Better CPU cache utilization
- Vectorized processing within page
- Still maintains row locality across pages
- Good for both OLTP and OLAP

Example:
Page 1 (PAX format):

```sql
+----------------------------------+
| ID Column:             [1][2][3] |
| Name Column:  [John][Sarah][Mike]|
| Age Column:         [30][28][35] |
| Dept Column:        [IT][HR][IT] |
+----------------------------------+
```

```sql
Query: SELECT name, age FROM ...
```

- Load page (1 IO)
- CPU cache: Loads Name and Age mini-columns
- Vectorized scan: Process all names together
- Skip ID and Dept columns entirely

Performance: 2-3× faster than pure row store for scans
Nearly same speed for point queries

#### **2. Columnar with Row Cache**

Strategy used by SAP HANA, MemSQL:

```sql
Main Storage (Column-Oriented, Compressed):
+----------------------------------+
| ID Column    | [compressed] |
| Name Column  | [compressed] |
| Age Column   | [compressed] |
| Dept Column  | [compressed] |
+----------------------------------+

Hot Data Cache (Row-Oriented):
+------------------------------------------------------------------+
| Row 1: [1][John][30][IT] |
| Row 2: [2][Sarah][28][HR] |
| Row 3: [3][Michael][35][IT] |
+------------------------------------------------------------------+
```

Query Decision Logic:

1. Point Query (WHERE id = X):
   - Check row cache first (fast!)
   - If not in cache, read from columns
2. Analytical Query (aggregate/scan):
   - Use column storage (efficient!)
   - Bypass cache

Example Flow:

```sql
Query: SELECT \* FROM users WHERE user_id = 100;
```

Step 1: Check cache
Cache lookup: O(1) hash lookup
Found? Return immediately (1-2 ms)

Not found? Continue to step 2

Step 2: Read from columns

- Position lookup in ID column
- Read all columns at that position
- Reconstruct row
- Add to cache for future use
  Time: 10-20 ms

Query: SELECT AVG(salary) FROM users;

Step 1: Bypass cache (analytical query)
Step 2: Read salary column only

- Compressed read
- Vectorized aggregation
  Time: Fast!

Performance:

- Point queries: 80-90% cache hit → nearly row-store speed
- Analytical: Full column-store benefits

#### **3. Delta Merge Architecture**

Used by SAP HANA, Vertica:

```sql
L0: Delta Store (Row-Oriented, In-Memory)
+------------------------------------------------------------------+
| Recent INSERTs/UPDATEs in row format |
| [6][Emma][27][IT][52000] |
| [7][Frank][30][HR][48000] |
+------------------------------------------------------------------+
Fast writes!
```

```sql
L1: Main Store (Column-Oriented, Compressed, Disk)
+----------------------------------+
| ID: [1][2][3][4][5] |
| Name: [John][Sarah][...] |
| ... |
+----------------------------------+
Efficient reads!
```

Query Merge Logic:

```sql
SELECT \* FROM employees WHERE department = 'IT';
```

Step 1: Query L0 (Delta Store)
Filter: department = 'IT'
Found: [6, Emma, 27, IT, 52000]

Step 2: Query L1 (Main Store)
Read: Department column (compressed)
Filter: positions [0, 2] → IT employees
Read: All columns at positions [0, 2]
Found: [1, John, 30, IT, 50000]
[3, Michael, 35, IT, 60000]

Step 3: Union Results
Merge L0 + L1 results
Return: John, Michael, Emma

Merge Process (Background):

When L0 reaches threshold (e.g., 100MB):

1. Sort L0 data
2. Merge into L1 columns
3. Recompress
4. Clear L0

```sql
Timeline:
+------------------+------------------+------------------+
| L0 fills up | Merge triggered | L0 cleared |
| (5 min) | (30 sec) | (instant) |
+------------------+------------------+------------------+
↓ ↓ ↓
[Fast Writes] [Brief Lock] [Resume Writes]
```

### Performance:

- INSERT/UPDATE: Fast (row format in L0)
- SELECT: Slight overhead (merge two stores)
- Overall: 90% of column-store benefits + 90% of row-store write speed

---

#### **4. Adaptive Columnar/Row Storage**

````

Used by Microsoft SQL Server (Columnstore + Rowstore):

Storage Decision per Table/Partition:

IF (workload = 'analytical' AND data_size > 1GB):
Use Columnstore Index
ELSE IF (workload = 'OLTP' OR data_size < 1GB):
Use Rowstore Index (B-Tree)

Example: Orders Table Partitioning

```sql
Partition 2023 (Historical, Large):
+----------------------------------+
| Columnstore Index |
| Compressed, Read-Only |
| Size: 500 GB → 50 GB compressed |
+----------------------------------+
````

Queries: Analytical (rare updates)
Storage: Column-oriented

````sql
Partition 2024 (Recent, Active):
+----------------------------------+
| Rowstore Index (B-Tree) |
| Standard row format |
| Size: 50 GB |
+----------------------------------+
Queries: Transactional (frequent updates)
Storage: Row-oriented

Partition 2025 (Current, Hot):
+----------------------------------+
| In-Memory Rowstore |
| Ultra-fast access |
| Size: 5 GB |
+----------------------------------+
Queries: Real-time transactions
Storage: Row-oriented, memory-only

Query Processing:
```sql
SELECT SUM(amount) FROM orders
WHERE order_date BETWEEN '2023-01-01' AND '2024-12-31';
````

Execution:

1. Partition 2023: Columnstore scan (fast!)
   Read: amount column only
   Time: 2 seconds
2. Partition 2024: Rowstore scan (slower)
   Read: All columns
   Time: 5 seconds

3. Partition 2025: Memory scan (fastest!)
   Time: 0.1 seconds

4. Union results
   Total time: ~7 seconds

Query Optimizer automatically chooses:

- Which partitions to access
- Which index type to use
- Parallel execution strategy

---

### **Storage Format Comparison Table**

```sql
+------------------+---------------+---------------+------------------+
| Characteristic | Row-Based | Column-Based | Hybrid (PAX) |
+------------------+---------------+---------------+------------------+
| INSERT Speed | ★★★★★ Fast | ★★☆☆☆ Slow | ★★★★☆ Fast |
| UPDATE Speed | ★★★★☆ Fast | ★★☆☆☆ Slow | ★★★☆☆ Medium |
| DELETE Speed | ★★★★☆ Fast | ★★★☆☆ Medium | ★★★☆☆ Medium |
| Point Query | ★★★★★ Fast | ★★☆☆☆ Slow | ★★★★☆ Fast |
| Full Scan | ★★☆☆☆ Slow | ★★★★★ Fast | ★★★★☆ Fast |
| Aggregation | ★★☆☆☆ Slow | ★★★★★ Fast | ★★★★☆ Fast |
| Compression | ★★☆☆☆ Poor | ★★★★★ Excellent| ★★★★☆ Good |
| Storage Size | ★★☆☆☆ Large | ★★★★★ Small | ★★★☆☆ Medium |
| OLTP Workload | ★★★★★ Best | ★★☆☆☆ Poor | ★★★★☆ Good |
| OLAP Workload | ★★☆☆☆ Poor | ★★★★★ Best | ★★★★☆ Good |
+------------------+---------------+---------------+------------------+

```

---

### **Real-World Use Cases and Recommendations**

#### **Use Row-Based Storage When:**

1. OLTP Applications (Online Transaction Processing)
   Example: Banking System

   Typical Queries:

   - Get account details: SELECT \* FROM accounts WHERE account_id = X
   - Update balance: UPDATE accounts SET balance = balance - 100 WHERE ...
   - Insert transaction: INSERT INTO transactions VALUES (...)

   Why Row-Based?

   - Each operation touches ONE complete row
   - Frequent updates/inserts
   - Need ACID guarantees
   - Point queries dominate

   Performance:
   ✓ Millisecond response times
   ✓ High concurrency
   ✓ Immediate consistency

2. Content Management Systems
   Example: WordPress, E-commerce Product Catalog

   Typical Queries:

   - Get product: SELECT \* FROM products WHERE product_id = X
   - Update inventory: UPDATE products SET stock = stock - 1 WHERE ...

   Why Row-Based?

   - Usually need full record
   - Frequent content updates
   - Random access patterns

3. Real-Time Applications
   Example: Gaming, Chat, Social Media Feeds

   Typical Queries:

   - Get user profile: SELECT \* FROM users WHERE user_id = X
   - Post message: INSERT INTO messages VALUES (...)
   - Update status: UPDATE user_status SET status = 'online' WHERE ...

   Why Row-Based?

   - Low latency critical (< 10ms)
   - High write throughput needed
   - Small, focused queries

Databases: MySQL, PostgreSQL, Oracle, SQL Server (rowstore)

#### **Use Column-Based Storage When:**

1. Data Warehousing
   Example: Business Intelligence, Analytics Platform

   Typical Queries:

   - Monthly sales: SELECT month, SUM(revenue) FROM sales GROUP BY month
   - Customer segmentation: SELECT segment, COUNT(\*), AVG(spend) FROM ...
   - Trend analysis: SELECT date, product, SUM(quantity) FROM ...

   Why Column-Based?

   - Queries access few columns across many rows
   - Read-heavy workload (95% reads)
   - Aggregations and scans dominate
   - Data rarely updated (batch loads)

   Performance:
   ✓ 10-100× faster than row stores
   ✓ 5-10× better compression
   ✓ Lower storage costs

   Trade-off:
   ✗ Slow individual inserts
   ✗ Complex updates

   Solution: Batch load data (ETL processes)

2. Log Analysis
   Example: Application Logs, Web Analytics, IoT Sensor Data

   Typical Queries:

   - Error rate: SELECT date, COUNT(\*) FROM logs WHERE level='ERROR' ...
   - Top pages: SELECT page, COUNT(\*) FROM web_logs GROUP BY page ...
   - Sensor trends: SELECT sensor_id, AVG(temperature) FROM readings ...

   Why Column-Based?

   - Massive data volumes (billions of rows)
   - Time-series queries
   - Statistical analysis
   - Mostly append-only

   Performance:
   ✓ Scan billions of rows in seconds
   ✓ 90% compression ratio
   ✓ Minimal storage cost

3. Machine Learning Feature Store
   Example: ML Model Training Data

   Typical Queries:

   - Get features: SELECT feature1, feature2, feature5 FROM training_data
   - Aggregate features: SELECT user_id, AVG(clicks), SUM(purchases) ...

   Why Column-Based?

   - Need specific feature columns
   - Large datasets (millions of rows)
   - Batch processing
   - Read-only after creation

Databases: Amazon Redshift, Google BigQuery, Snowflake,
ClickHouse, Apache Parquet, Apache Arrow

#### **Use Hybrid Storage When:**

1. Mixed Workload Applications
   Example: E-commerce Platform

   Workload Split:

   - 70% Transactional (product browsing, checkout)
   - 30% Analytical (sales reports, inventory analysis)

   Strategy:

   - Hot tables (products, orders): Row-based
   - Historical tables (past_orders): Column-based
   - Reporting tables (aggregates): Column-based

   Example Setup:

   orders_current (last 30 days):
   Storage: Rowstore
   Queries: SELECT \* FROM orders WHERE order_id = X

   orders_archive (> 30 days old):
   Storage: Columnstore
   Queries: SELECT date, SUM(amount) FROM orders_archive GROUP BY date

   Performance:
   ✓ Fast transactions on current data
   ✓ Fast analytics on historical data
   ✓ Balanced resource usage

2. SaaS Analytics Platform
   Example: Customer Analytics Dashboard

   Requirements:

   - Real-time data ingestion (events, clicks, pageviews)
   - Fast dashboard queries
   - Historical trend analysis

   Architecture:

   Hot Tier (Last 24 hours):
   +---------------------------+
   | In-Memory Rowstore |
   | Fast inserts: 100K/sec |
   | Point queries: < 5ms |
   +---------------------------+

   Warm Tier (Last 30 days):
   +---------------------------+
   | Disk Rowstore + Indexes |
   | Mixed workload |
   | Query time: 10-100ms |
   +---------------------------+

   Cold Tier (Historical):
   +---------------------------+
   | Compressed Columnstore |
   | Analytics optimized |
   | Query time: 1-10 seconds |
   +---------------------------+

   Query Example:
   "Show last 24 hours active users" → Hot tier (fast!)
   "Show monthly user growth" → Cold tier (efficient!)

3. Enterprise Data Platform
   Example: Large Corporation (Multiple Use Cases)

   Setup:

   - Transactional DB: Row-based (PostgreSQL)
   - Data Warehouse: Column-based (Snowflake)
   - ETL Pipeline: Nightly sync

   Flow:

   1. Operational data → Row DB (OLTP)
   2. ETL extracts → transforms → loads
   3. Analytics data → Column DB (OLAP)
   4. BI tools query → Column DB

   Benefits:
   ✓ Separation of concerns
   ✓ No impact on production
   ✓ Optimized for each workload

Databases: SAP HANA, MemSQL, Microsoft SQL Server,
Oracle (hybrid features), CockroachD

### **Compression Techniques Deep Dive**

Let's explore how compression works in columnar databases:

#### **1. Dictionary Encoding**

Original Department Column (100,000 rows):
['IT']['HR']['IT']['Finance']['IT']['HR']['IT']...

Analysis:

- Unique values: 5 (IT, HR, Finance, Marketing, Sales)
- String length: 2-15 characters
- Storage: 10 bytes average × 100,000 = 1 MB

Dictionary Encoding:
Dictionary:

```sql
+-----+-----------+
| ID | Value      |
+-----+-----------+
| 0 | IT          |
| 1 | HR          |
| 2 | Finance     |
| 3 | Marketing   |
| 4 | Sales       |
+-----+-----------+
```

Dictionary size: 50 bytes

Encoded Column:
[0][1][0][2][0][1][0][1][0][2][0]...
100,000 values × 1 byte = 100 KB

Total: 100 KB + 50 bytes ≈ 100 KB
Compression ratio: 10:1 (1 MB → 100 KB)

Query Processing:

```sql
SELECT \* FROM employees WHERE department = 'IT';
```

Optimized:

1. Look up 'IT' in dictionary → ID = 0
2. Scan encoded column for value 0
3. Compare integers instead of strings (faster!)
4. Decode result using dictionary

Benefit: 10× less storage + faster comparisons

#### **2. Run-Length Encoding (RLE)**

Original Status Column (sorted by status):
['active']['active']['active']['active']['active']
['active']['active']['inactive']['inactive']['inactive']
['inactive']['pending']['pending']['pending']

Analysis: Many consecutive duplicates

RLE Encoding:

```sql
+----------+-------+
| Value | Count |
+----------+-------+
| active | 7 |
| inactive | 4 |
| pending | 3 |
+----------+-------+
```

Storage:
Without RLE: 14 values × 10 bytes = 140 bytes
With RLE: 3 runs × 12 bytes = 36 bytes
Compression ratio: 4:1

Even Better with Dictionary + RLE:
Dictionary: {0: 'active', 1: 'inactive', 2: 'pending'}
RLE: [(0, 7), (1, 4), (2, 3)]
Storage: 18 bytes
Compression ratio: 8:1

Query Processing:

```sql
SELECT COUNT(\*) FROM users WHERE status = 'active';
```

Optimized:

1. Look up 'active' → ID = 0
2. Find RLE entry: (0, 7)
3. Return count: 7
4. No need to scan actual values!

Ultra-fast aggregation!

#### **3. Bit Packing**

Original Age Column:
[25][30][28][35][22][45][33][29]

Analysis:

- Values: 18-65 (range: 47)
- Normal storage: 4 bytes (32 bits) per value
- Actual needed: 6 bits (can represent 0-63)

Bit Packing:
Pack multiple values into single bytes:

Value: 25 = 011001 (6 bits)
Value: 30 = 011110 (6 bits)
Value: 28 = 011100 (6 bits)

Packed:
Byte 1: 01100101 (partial 25, partial 30)
Byte 2: 11100111 (partial 30, partial 28)
Byte 3: 00... (continue packing)

Storage:
Without packing: 8 values × 4 bytes = 32 bytes
With packing: 8 values × 6 bits = 48 bits = 6 bytes
Compression ratio: 5:1

Query Processing:
SELECT AVG(age) FROM employees;

1. Decompress bit-packed values
2. Use SIMD to process multiple values at once
3. Calculate aggregate

Trade-off: Slight CPU overhead for massive space savings

#### **4. Delta Encoding**

Original Timestamp Column (sorted):
[1609459200] // 2021-01-01 00:00:00
[1609459260] // 2021-01-01 00:01:00 (+60)
[1609459320] // 2021-01-01 00:02:00 (+60)
[1609459380] // 2021-01-01 00:03:00 (+60)
[1609459440] // 2021-01-01 00:04:00 (+60)

Analysis:

- Values: Large 32-bit integers
- Pattern: Small differences between consecutive values

Delta Encoding:
Base value: 1609459200
Deltas: [0][60][60][60][60]

Storage:
Without delta: 5 values × 4 bytes = 20 bytes
With delta: 1 base (4 bytes) + 5 deltas × 1 byte = 9 bytes
Compression ratio: 2:1

Combined with RLE:
Base: 1609459200
RLE: (60, 4 times)
Storage: 4 + 2 = 6 bytes
Compression ratio: 3:1

Perfect for:

- Time series data
- Sequential IDs
- Monotonically increasing values

Query Processing:

```sql
SELECT \* FROM logs WHERE timestamp > 1609459300;
```

1. Calculate delta from base: 1609459300 - 1609459200 = 100
2. Binary search in delta array
3. Reconstruct only needed timestamps
4. Fast range queries!

#### **5. Null Suppression**

Original Salary Column (with many nulls):
[50000][NULL][60000][NULL][NULL][55000][NULL]

Traditional storage:
7 values × 4 bytes = 28 bytes

Null Bitmap Approach:
Bitmap: [1][0][1][0][0][1][0] // 1 = has value, 0 = null
1 byte (can represent 8 positions)

Values: [50000][60000][55000] // Only non-null values
3 values × 4 bytes = 12 bytes

Total: 1 + 12 = 13 bytes
Compression ratio: 2:1

With high null percentage (80% nulls):
100 values, 20 non-null
Bitmap: 100 bits = 13 bytes
Values: 20 × 4 = 80 bytes
Total: 93 bytes vs 400 bytes (traditional)
Compression ratio: 4:1

Query Processing:
SELECT AVG(salary) FROM employees WHERE salary IS NOT NULL;

1. Check bitmap for non-null positions: [0, 2, 5]
2. Read only values at those positions
3. Calculate AVG of [50000, 60000, 55000]
4. Skip NULL checks entirely!

Extremely efficient for sparse data!

---

### **Advanced Query Optimization Examples**

#### **Predicate Pushdown in Columnar Storage**

Query:

```sql
SELECT name, salary
FROM employees
WHERE department = 'IT' AND salary > 50000;
```

Row-Based Execution:

1. Read all rows (all columns)
2. Filter: department = 'IT'
3. Filter: salary > 50000
4. Project: name, salary

Column-Based Execution (Optimized):

Step 1: Predicate Pushdown to Storage Layer
Don't even decompress irrelevant data!

Department Column (compressed):
Dictionary: {0:'IT', 1:'HR', 2:'Finance'}
Encoded: [0][1][0][2][1][0]

Filter at storage: WHERE encoded_value = 0
Result bitmap: [1][0][1][0][0][1]
Positions matching 'IT': [0, 2, 5]

Step 2: Apply Second Predicate
Salary Column:
[50000][45000][60000][55000][47000][52000]

Read ONLY positions [0, 2, 5]:
[50000, 60000, 52000]

Filter: > 50000
Result bitmap: [0][1][1]
Final positions: [2, 5]

Step 3: Late Materialization
Only NOW read name column at positions [2, 5]:
Name Column: [...]['Michael'][...][...][...]['Emma']
Result: ['Michael', 'Emma']

##### Data Read:

- Department column: 6 bytes (compressed)
- Salary column: 12 bytes (3 values)
- Name column: 20 bytes (2 values)
  Total: 38 bytes

##### Compare to Row-Based:

- Would read: 6 rows × 50 bytes = 300 bytes
- Column-based reads: 87% less data!

#### **Vectorized Execution**

Query:
SELECT SUM(salary) FROM employees;

Scalar Execution (Row-Based):
sum = 0
for each row:
sum += row.salary
return sum

CPU Operations: 1,000,000 iterations (for 1M rows)

Vectorized Execution (Column-Based):
Load salary column into memory:
[50000][45000][60000][55000][47000][52000]...

Process in batches of 8 (using SIMD):
Batch 1: [50000, 45000, 60000, 55000, 47000, 52000, 48000, 51000]
| |
+------ Single CPU instruction (AVX-512) ---------------+

Result: 408000 (sum of 8 values in ONE operation!)

Batch 2: [next 8 values]
...

CPU Operations: 1,000,000 / 8 = 125,000 iterations
Speedup: 8× faster!

Real-world with pipelining and prefetching: 10-20× faster

Code Visualization:
// Scalar (slow)
for (int i = 0; i < n; i++) {
sum += salary[i]; // One at a time
}

// Vectorized (fast)
**m512i vec_sum = \_mm512_setzero_si512();
for (int i = 0; i < n; i += 8) {
**m512i vec = \_mm512_load_si512(&salary[i]); // Load 8 values
vec_sum = \_mm512_add_epi64(vec_sum, vec); // Add 8 values at once
}
sum = \_mm512_reduce_add_epi64(vec_sum); // Final reduction

```

---

### **Storage Format Decision Tree**

```

Start: Choosing Storage Format
|
v
[What is your workload?]
|
+-- OLTP (transactions) → ROW-BASED
| - Frequent inserts/updates
| - Point queries (WHERE id = X)
| - Need full rows
| → PostgreSQL, MySQL, Oracle
|
+-- OLAP (analytics) → COLUMN-BASED
| - Large scans
| - Few columns per query
| - Aggregations
| → Redshift, BigQuery, ClickHouse
|
+-- Mixed Workload → HYBRID
|
+-- Can partition data?
| - Hot/Cold separation
| → SQL Server (Row + Column Indexes)
|
+-- Need real-time? - In-memory + Columnar
→ SAP HANA, MemSQL

```

---

### **Summary: Key Takeaways**

**Row-Based Storage:**

- ✅ Excellent for transactional workloads (OLTP)
- ✅ Fast single-row operations (INSERT, UPDATE, DELETE, point SELECT)
- ✅ All columns of a row stored together (locality)
- ❌ Inefficient for analytical queries (reads unnecessary data)
- ❌ Poor compression (mixed data types in rows)
- **Use When:** Banking, e-commerce transactions, CMS, real-time apps

**Column-Based Storage:**

- ✅ Excellent for analytical workloads (OLAP)
- ✅ Fast aggregate queries and large scans
- ✅ Reads only needed columns (I/O efficient)
- ✅ Superior compression (10-100× better)
- ❌ Slow single-row operations
- ❌ Complex updates (touch multiple files)
- **Use When:** Data warehousing, BI, log analysis, ML feature stores

**Hybrid Storage:**

- ✅ Balanced performance for mixed workloads
- ✅ Flexible (different strategies for different data)
- ❌ More complex to manage
- **Use When:** Mixed OLTP/OLAP, SaaS platforms, enterprise systems

The choice between row-based and column-based storage is one of the most fundamental decisions in database design, directly impacting performance, costs, and scalability. Understanding how each stores and processes data allows you to make informed architectural decisions for your specific use case.

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```

```
