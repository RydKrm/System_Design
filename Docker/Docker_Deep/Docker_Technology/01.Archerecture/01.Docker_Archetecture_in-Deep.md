
# Docker Internal Architecture - The Ultimate Deep Dive

## Introduction

This comprehensive guide explores every aspect of Docker's internal architecture, from the Linux kernel primitives to high-level orchestration. You'll learn exactly how Docker works under the hood, how components communicate, and how a simple `docker run` command transforms into a running container.

---

## Part 1: The Foundation - Linux Kernel Features

### Namespaces: Creating Isolation

Docker containers achieve isolation through Linux namespaces. Think of namespaces as creating separate "universes" where processes live independently.

**The Seven Namespace Types:**

1. **PID Namespace** - Process isolation
2. **Network Namespace** - Network stack isolation
3. **Mount Namespace** - Filesystem isolation
4. **UTS Namespace** - Hostname isolation
5. **IPC Namespace** - Inter-process communication isolation
6. **User Namespace** - User ID isolation
7. **Cgroup Namespace** - Control group isolation

**Visual Example:**

```
Host View                  Container View
═══════════               ═══════════════
PID 1: systemd            PID 1: nginx
PID 500: dockerd          PID 2: worker
PID 1000: nginx           (Can't see host processes)
```

### Control Groups (cgroups): Resource Management

Cgroups limit and monitor resource usage. They're like resource police ensuring no container uses more than its allocation.

**Key Controllers:**

- CPU: Limits processing power
- Memory: Limits RAM usage
- Block I/O: Limits disk read/write
- Network: Limits bandwidth

**Example Hierarchy:**

```
/sys/fs/cgroup/
  ├── memory/
  │   └── docker/
  │       └── container_id/
  │           ├── memory.limit_in_bytes
  │           └── cgroup.procs
  └── cpu/
      └── docker/
          └── container_id/
              ├── cpu.shares
              └── cgroup.procs
```

### Union Filesystems: Layer Technology

Union filesystems allow stacking multiple directories (layers) to appear as one unified filesystem.

```
Container View (Merged):
├── /app/myapp.py
├── /usr/lib/python3.9/
└── /bin/bash

Actual Structure:
Layer 3 (R/W): /app/myapp.py
Layer 2 (R/O): /usr/lib/python3.9/
Layer 1 (R/O): /bin/bash
```

---

## Part 2: runc - The Low-Level Runtime

### What is runc?

runc is the component that actually creates containers. It implements the OCI (Open Container Initiative) Runtime Specification and directly interacts with the Linux kernel.

**runc's Job:**

1. Parse OCI runtime specification
2. Create Linux namespaces
3. Set up cgroups
4. Prepare root filesystem
5. Execute container process

**Example OCI Spec (config.json):**

```json
{
  "ociVersion": "1.0.0",
  "process": {
    "args": ["/bin/sh"],
    "env": ["PATH=/usr/bin"],
    "cwd": "/"
  },
  "root": {
    "path": "rootfs"
  },
  "linux": {
    "namespaces": [
      {"type": "pid"},
      {"type": "network"}
    ],
    "resources": {
      "memory": {"limit": 536870912}
    }
  }
}
```

### Container Creation Process

**Step-by-Step:**

1. **Clone System Call**
    
    ```c
    clone(CLONE_NEWPID | CLONE_NEWNS | CLONE_NEWNET)
    ```
    
    Creates new process with isolated namespaces
    
2. **Cgroup Setup**
    
    ```bash
    echo $PID > /sys/fs/cgroup/memory/docker/abc123/cgroup.procs
    echo 536870912 > /sys/fs/cgroup/memory/docker/abc123/memory.limit_in_bytes
    ```
    
3. **Filesystem Preparation**
    
    ```bash
    mount -t overlay overlay \
      -o lowerdir=/layer1:/layer2,upperdir=/upper,workdir=/work \
      /merged
    ```
    
4. **Execute Process**
    
    ```c
    execve("/bin/sh", ["/bin/sh"], env)
    ```
    

---

## Part 3: containerd - The High-Level Runtime

### Architecture Overview

containerd manages the complete container lifecycle and provides a stable API for clients like Docker.

```
┌─────────────────────────────────┐
│        containerd               │
├─────────────────────────────────┤
│  gRPC API Server                │
│    ↓                            │
│  Content Store (images)         │
│  Snapshot Service (filesystems) │
│  Metadata Database (state)      │
│  Task Service (containers)      │
│    ↓                            │
│  containerd-shim (per container)│
│    ↓                            │
│  runc                           │
└─────────────────────────────────┘
```

### Key Components

**1. Content Store** Stores image content addressable by digest:

```
/var/lib/containerd/content/blobs/sha256/
  ├── abc123... (image layer)
  ├── def456... (config)
  └── ghi789... (manifest)
```

**2. Snapshot Service** Manages container filesystems:

```
base (read-only)
  └─> layer1 (read-only)
      └─> layer2 (read-only)
          └─> container (read-write)
```

**3. containerd-shim** Critical daemon that:

- Keeps container's STDIO open
- Reports container exit status
- Acts as parent process
- Allows daemon upgrades without killing containers

**Process Tree:**

```
systemd
  └─> containerd
       ├─> shim-1 -> container-1
       └─> shim-2 -> container-2
```

If containerd restarts, shims keep containers running!

---

## Part 4: Docker Engine - The Complete Platform

### Docker Daemon (dockerd)

The Docker daemon is the central management service.

**Responsibilities:**

```
┌────────────────────────────────┐
│       Docker Daemon            │
├────────────────────────────────┤
│ • REST API Server              │
│ • Image Management             │
│ • Volume Management            │
│ • Network Management           │
│ • Build System (BuildKit)      │
│ • Swarm Mode                   │
└────────────────────────────────┘
```

**Configuration** (`/etc/docker/daemon.json`):

```json
{
  "storage-driver": "overlay2",
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  },
  "default-address-pools": [
    {"base": "172.18.0.0/16", "size": 24}
  ],
  "features": {
    "buildkit": true
  }
}
```

### Docker API

**Communication Methods:**

1. **Unix Socket** (default): `/var/run/docker.sock`
2. **TCP Socket**: `tcp://host:2376` (with TLS)
3. **SSH**: `ssh://user@host`

**API Structure:**

```
/v1.41/
  ├── containers/
  │   ├── create
  │   ├── {id}/start
  │   ├── {id}/stop
  │   └── {id}/logs
  ├── images/
  │   ├── create (pull)
  │   ├── build
  │   └── {id}/json
  ├── networks/
  └── volumes/
```

**Example API Call:**

```bash
curl --unix-socket /var/run/docker.sock \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"Image":"nginx"}' \
  http://localhost/v1.41/containers/create
```

---

## Part 5: Image Management

### Image Storage Structure

```
/var/lib/docker/
  ├── image/overlay2/
  │   ├── imagedb/content/sha256/ (image configs)
  │   ├── layerdb/sha256/ (layer metadata)
  │   └── repositories.json (name->ID mapping)
  └── overlay2/ (actual layer data)
      ├── abc123/
      │   ├── diff/ (layer contents)
      │   ├── link (short ID)
      │   └── lower (parent layers)
      └── def456/
```

### Image Components

**1. Manifest** - Points to config and layers **2. Config** - Image metadata (CMD, ENV, etc.) **3. Layers** - Filesystem changes

**Example Manifest:**

```json
{
  "schemaVersion": 2,
  "config": {
    "digest": "sha256:abc123...",
    "size": 7023
  },
  "layers": [
    {"digest": "sha256:layer1..."},
    {"digest": "sha256:layer2..."}
  ]
}
```

### Pull Image Flow

```
1. docker pull nginx
     ↓
2. Resolve to registry.hub.docker.com
     ↓
3. Authenticate (if required)
     ↓
4. GET /v2/library/nginx/manifests/latest
     ↓
5. Parse manifest, get layer digests
     ↓
6. For each layer:
     GET /v2/library/nginx/blobs/{digest}
     ↓
7. Extract to /var/lib/docker/overlay2/
     ↓
8. Verify checksums
     ↓
9. Update metadata
     ↓
10. Image ready!
```

---

## Part 6: Volume Management

### Volume Types

```
┌────────────────────────────────────┐
│         Volume Types               │
├────────────────────────────────────┤
│ 1. Named Volumes (Docker-managed)  │
│    /var/lib/docker/volumes/        │
│                                    │
│ 2. Bind Mounts (Host directories)  │
│    /any/host/path                  │
│                                    │
│ 3. tmpfs (Memory)                  │
│    RAM-based temporary storage     │
└────────────────────────────────────┘
```

### Volume Architecture

```
Host System                Container
┌──────────────────┐      ┌──────────────────┐
│ /var/lib/docker/ │      │ /data/           │
│  volumes/        │◄────►│  ├── file1.txt   │
│   myvolume/      │      │  └── file2.txt   │
│    _data/        │      └──────────────────┘
│     ├── file1.txt│
│     └── file2.txt│
└──────────────────┘
```

### Mount Process

```
1. docker run -v myvolume:/data nginx
     ↓
2. Check if 'myvolume' exists
     No → Create: /var/lib/docker/volumes/myvolume
     Yes → Use existing
     ↓
3. containerd prepares mount
     Source: /var/lib/docker/volumes/myvolume/_data
     Target: /data (in container)
     ↓
4. runc performs mount
     mount --bind /var/lib/docker/volumes/myvolume/_data \
           /data
     ↓
5. Container sees /data directory
```

---

## Part 7: Network Management

### Network Drivers

```
┌──────────────────────────────────────┐
│      Network Drivers                 │
├──────────────────────────────────────┤
│ bridge    Default, single-host       │
│ host      Uses host network directly │
│ overlay   Multi-host (Swarm)         │
│ macvlan   Assign MAC to container    │
│ none      No networking              │
└──────────────────────────────────────┘
```

### Bridge Network Architecture

```
┌─────────────────────────────────────────┐
│              Host Machine               │
│                                         │
│  ┌────────────────────────────────────┐ │
│  │         docker0 Bridge             │ │
│  │         172.17.0.1/16              │ │
│  │                                    │ │
│  │  ┌────────────┐    ┌────────────┐  │ │
│  │  │Container 1 │    │Container 2 │  │ │
│  │  │172.17.0.2  │    │172.17.0.3  │  │ │
│  │  └─────┬──────┘    └─────┬──────┘  │ │
│  │        │                 │         │ │
│  │        └────────┬────────┘         │ │
│  │                 │                  │ │
│  │         ┌───────▼───────┐          │ │
│  │         │ docker0       │          │ │
│  │         │ (bridge)      │          │ │
│  │         └───────┬───────┘          │ │
│  └─────────────────┼──────────────────┘ │
│                    │                    │
│           ┌────────▼────────┐           │
│           │   iptables      │           │
│           │   (NAT/Filter)  │           │
│           └────────┬────────┘           │
│                    │                    │
│           ┌────────▼────────┐           │
│           │   eth0 (NIC)    │           │
│           │   192.168.1.100 │           │
│           └─────────────────┘           │
└─────────────────────────────────────────┘
```

### Virtual Ethernet (veth) Pairs

```
Container Namespace         Host Namespace
┌───────────────┐          ┌───────────────┐
│   eth0        │◄────────►│   vethXXX     │
│  172.17.0.2   │ veth pair│  (no IP)      │
└───────────────┘          └───────┬───────┘
                                   │
                            ┌──────▼──────┐
                            │   docker0   │
                            │   (bridge)  │
                            └─────────────┘
```

### Port Publishing

When you use `-p 8080:80`:

```
External Request Flow:
──────────────────────

1. Client connects to 192.168.1.100:8080
     ↓
2. Packet arrives at host eth0
     ↓
3. iptables NAT (PREROUTING):
   -A DOCKER -p tcp --dport 8080
   -j DNAT --to-destination 172.17.0.2:80
     ↓
4. Packet destination changed to 172.17.0.2:80
     ↓
5. Routed through docker0 bridge
     ↓
6. Forwarded via veth pair
     ↓
7. Arrives at container eth0:80
     ↓
8. Application (nginx) receives request
```

### DNS Resolution

Docker provides automatic DNS at 127.0.0.11:

```
Container 1: "ping container2"
     ↓
Query 127.0.0.11 for "container2"
     ↓
Docker embedded DNS resolves:
"container2" → 172.17.0.3
     ↓
Connection established
```

---

## Part 8: How Everything Works Together

### Complete System Integration

```
                    User Interface
                          │
                  ┌───────┴────-────┐
                  │   Docker CLI    │
                  └───────┬─────────┘
                          │ HTTP/Unix Socket
                  ┌───────▼──────────┐
                  │  Docker Daemon   │
                  │   (dockerd)      │
                  │                  │
                  │ • API Server     │
                  │ • Image Mgmt     │
                  │ • Volume Mgmt    │
                  │ • Network Mgmt   │
                  └───────┬──────────┘
                          │ gRPC
                  ┌───────▼──────────┐
                  │   containerd     │
                  │                  │
                  │ • Lifecycle      │
                  │ • Images         │
                  │ • Snapshots      │
                  └───────┬──────────┘
                          │
              ┌───────────┴───────────┐
              │                       │
       ┌──────▼─────┐          ┌─────▼──────┐
       │   shim-1   │          │   shim-2   │
       └──────┬─────┘          └─────┬──────┘
              │                      │
       ┌──────▼─────┐          ┌─────▼──────┐
       │   runc-1   │          │   runc-2   │
       └──────┬─────┘          └─────┬──────┘
              │                      │
       ┌──────▼─────┐          ┌─────▼──────┐
       │Container 1 │          │Container 2 │
       └────────────┘          └────────────┘
              │                      │
              └──────────┬───────────┘
                         │
                  ┌──────▼──────┐
                  │Linux Kernel │
                  │             │
                  │• Namespaces │
                  │• Cgroups    │
                  │• OverlayFS  │
                  └─────────────┘
```

### Request Flow Example

**Scenario:** `docker run -d -p 8080:80 nginx`

```
Step 1: CLI Parses Command
───────────────────────────
docker CLI
  • Parses arguments
  • Creates JSON payload

Step 2: API Request
───────────────────
POST /containers/create
{
  "Image": "nginx",
  "HostConfig": {
    "PortBindings": {
      "80/tcp": [{"HostPort": "8080"}]
    }
  }
}

Step 3: Docker Daemon Processes
────────────────────────────────
dockerd
  • Validates request
  • Checks image locally
  • Allocates container ID
  • Assigns IP: 172.17.0.2
  • Sets up iptables rules

Step 4: Call containerd
───────────────────────
gRPC: containers.Create
  • Image: nginx
  • Runtime: runc

Step 5: containerd Actions
───────────────────────────
containerd
  • Gets image layers
  • Creates snapshots
  • Generates OCI spec
  • Launches shim

Step 6: Shim Spawns runc
────────────────────────
containerd-shim
  • Forks process
  • Prepares STDIO
  • Calls runc

Step 7: runc Creates Container
───────────────────────────────
runc
  • Creates namespaces
  • Sets up cgroups
  • Mounts filesystem
  • Configures network
  • Executes process

Step 8: Container Running!
──────────────────────────
nginx process (PID 1 in container)
  • Bound to 0.0.0.0:80
  • Accessible on host:8080
```

---

## Part 9: Dockerfile to Container

### Build Process

**Dockerfile:**

```dockerfile
FROM python:3.9
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY app.py .
CMD ["python", "app.py"]
```

**Build Flow:**

```
1. docker build -t myapp .
     ↓
2. CLI creates build context
   • Tars all files in directory
   • Applies .dockerignore
   • Sends to daemon (e.g., 10 MB)
     ↓
3. Daemon processes Dockerfile
   
   FROM python:3.9
     ↓ Pull if not present
   Base layer: 120 MB
   
   WORKDIR /app
     ↓ Creates directory
   Layer: +1 KB
   
   COPY requirements.txt .
     ↓ Copies from context
   Layer: +512 bytes
   
   RUN pip install -r requirements.txt
     ↓ Creates temp container
     ↓ Executes command
     ↓ Commits changes
   Layer: +30 MB
   
   COPY app.py .
     ↓ Copies from context
   Layer: +2 KB
   
   CMD ["python", "app.py"]
     ↓ Stored in config (not a layer)
     ↓
4. Image created
   • Total: 150 MB
   • 5 layers stacked
   • Tagged: myapp:latest
     ↓
5. Stored in local registry
   /var/lib/docker/image/overlay2/
```

### Layer Caching

```
Build #1:                  Build #2 (after code change):
──────────                 ─────────────────────────────

FROM python:3.9            FROM python:3.9
  → Build (30s)              → CACHED ✓ (0s)

COPY requirements.txt      COPY requirements.txt
  → Build (1s)               → CACHED ✓ (0s)

RUN pip install            RUN pip install
  → Build (60s)              → CACHED ✓ (0s)

COPY app.py                COPY app.py
  → Build (1s)               → REBUILD (1s) ← File changed

Total: 92s                 Total: 1s
```

---

## Part 10: Docker Compose

### Architecture

```
docker-compose.yml
  ├── services:
  │   ├── web
  │   ├── db
  │   └── cache
  ├── networks:
  │   └── backend
  └── volumes:
      └── db-data
         ↓ Parsed by
Docker Compose CLI
  • Validates YAML
  • Resolves dependencies
  • Translates to Docker API calls
         ↓ Sends to
Docker Daemon
  • Creates networks
  • Creates volumes
  • Creates containers in order
```

### Execution Flow

**docker-compose.yml:**

```yaml
services:
  web:
    build: .
    ports:
      - "8080:80"
    depends_on:
      - db
  db:
    image: postgres:13
    volumes:
      - db-data:/var/lib/postgresql/data

volumes:
  db-data:
```

**When you run `docker-compose up`:**

```
1. Parse YAML
     ↓
2. Resolve dependencies
   db (no deps) → web (depends on db)
     ↓
3. Create network
   docker network create myapp_default
     ↓
4. Create volume
   docker volume create myapp_db-data
     ↓
5. Create & start db
   docker run -d \
     --name myapp_db_1 \
     --network myapp_default \
     -v myapp_db-data:/var/lib/postgresql/data \
     postgres:13
     ↓
6. Build web image
   docker build -t myapp_web .
     ↓
7. Create & start web
   docker run -d \
     --name myapp_web_1 \
     --network myapp_default \
     -p 8080:80 \
     myapp_web
     ↓
8. All services running!
   • Web can reach db by hostname "db"
   • Data persists in db-data volume
```

---

## Part 11: Orchestration Layer

### Docker Swarm

```
┌────────────────────────────────────────┐
│          Swarm Architecture            │
├────────────────────────────────────────┤
│                                        │
│     Manager Nodes (Raft Consensus)     │
│  ┌───────┐  ┌───────┐  ┌───────┐       │
│  │Manager│←→│Manager│←→│Manager│       │
│  │   1   │  │   2   │  │   3   │       │
│  └───┬───┘  └───┬───┘  └───┬───┘       │
│      │          │          │           │
│      └──────────┼──────────┘           │
│                 │                      │
│        Task Distribution               │
│                 │                      │
│      ┌──────────┼──────────┐           │
│      ↓          ↓          ↓           │
│  ┌───────┐  ┌───────┐  ┌───────┐       │
│  │Worker │  │Worker │  │Worker │       │
│  │   1   │  │   2   │  │   3   │       │
│  │Tasks: │  │Tasks: │  │Tasks: │       │
│  │1,4,7  │  │2,5,8  │  │3,6,9  │       │
│  └───────┘  └───────┘  └───────┘       │
│                                        │
│    Overlay Network (all nodes)         │
└────────────────────────────────────────┘
```

### Kubernetes Integration

```
┌─────────────────────────────────────────┐
│   Kubernetes + Docker Architecture      │
├─────────────────────────────────────────┤
│                                         │
│  Control Plane                          │
│  ┌────────────────────────────────────┐ │
│  │ API Server │ Scheduler │ etcd      │ │
│  └────────────────────────────────────┘ │
│             │                           │
│  Worker Node│                           │
│  ┌──────────▼──────────────────────────┐│
│  │       kubelet (K8s agent)           ││
│  └──────────┬──────────────────────────┘│
│             │ CRI (Container Runtime   ││
│             │      Interface)          ││
│  ┌──────────▼──────────────────────────┐│
│  │          containerd                 ││
│  └──────────┬──────────────────────────┘│
│             │                           │
│  ┌──────────▼──────────────────────────┐│
│  │           runc                      ││
│  └──────────┬──────────────────────────┘│
│             │                           │
│       Pod (Containers)                  │
│  ┌──────────▼──────────────────────────┐│
│  │ Container 1 │ Container 2 │         ││
│  └─────────────────────────────────────┘│
└─────────────────────────────────────────┘
```

---

## Complete HTTP Request Flow Example

**Scenario:** External HTTP request to containerized nginx

```
1. User Browser
   → http://192.168.1.100:8080
     ↓
2. DNS Resolution
   → 192.168.1.100
     ↓
3. TCP Connection
   → SYN to 192.168.1.100:8080
     ↓
4. Host NIC (eth0)
   → Receives packet
     ↓
5. Netfilter (iptables)
   → PREROUTING chain
   → Rule: -A DOCKER -p tcp --dport 8080
            -j DNAT --to 172.17.0.2:80
   → Destination changed to 172.17.0.2:80
     ↓
6. Routing Decision
   → Destination 172.17.0.0/16
   → Route via docker0 bridge
     ↓
7. docker0 Bridge
   → ARP lookup for 172.17.0.2
   → Find attached veth interface
     ↓
8. veth Pair
   → Host end: vethXXXX
   → Container end: eth0
   → Forward packet
     ↓
9. Container Network Namespace
   → Interface eth0 (172.17.0.2)
   → Port 80 bound by nginx
     ↓
10. nginx Process
    → Accept connection
    → Process HTTP request
    → Generate response
     ↓
11. Response Path (Reverse)
    Container eth0 → veth → docker0
    → iptables NAT (reverse translation)
    → Host eth0 → Internet → User browser
     ↓
12. User Sees Response
    HTTP/1.1 200 OK
    <html>...</html>
```

---

## Key Takeaways

### Component Responsibilities

```
┌────────────┬────────────────────────────────┐
│ Component  │ Primary Responsibility         │
├────────────┼────────────────────────────────┤
│ Docker CLI │ User interface, commands       │
│ dockerd    │ API, orchestration, features   │
│ containerd │ Container lifecycle, images    │
│ shim       │ Daemonless, STDIO handling     │
│ runc       │ Container creation, kernel API │
│ Kernel     │ Namespaces, cgroups, overlayfs │
└────────────┴────────────────────────────────┘
```

### Communication Flow

```
User Command
     ↓ Unix Socket/HTTP
Docker Daemon
     ↓ gRPC
containerd
     ↓ Fork/Exec
containerd-shim
     ↓ Binary Execution
runc
     ↓ System Calls
Linux Kernel
     ↓
Running Container
```

### Why This Architecture?

1. **Modularity** - Each component is independent
2. **Stability** - Daemon can restart without killing containers
3. **Standards** - Follows OCI specifications
4. **Flexibility** - Components can be swapped
5. **Security** - Layered security boundaries
6. **Performance** - Efficient resource usage

---

## Conclusion

Docker's architecture is a masterpiece of software engineering. By understanding how each component works and interacts, you can:

- **Debug effectively** - Know where to look when issues arise
- **Optimize performance** - Understand resource usage
- **Secure deployments** - Apply security at the right layers
- **Make informed decisions** - Choose the right tools and approaches

From a simple `docker run` to complex multi-host orchestration, every action flows through this carefully designed architecture, transforming high-level commands into low-level kernel operations that create the isolated, portable environments we call containers.

The journey from Linux kernel primitives to production-grade container platforms demonstrates the power of layered abstractions, where each layer adds value while maintaining clear boundaries and responsibilities.

---

**End of Docker Internal Architecture Deep Dive**