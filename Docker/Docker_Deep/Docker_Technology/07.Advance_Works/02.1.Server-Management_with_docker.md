# Complete Guide to Managing Multiple Projects with Docker on a Single Server

## Table of Contents

1. [Introduction to Docker Server Management](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#introduction)
2. [Understanding Docker Architecture](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#architecture)
3. [Managing Multiple Projects with Docker Compose](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#docker-compose)
4. [Port Management Strategies](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#port-management)
5. [Docker Networks Deep Dive](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#docker-networks)
6. [SSL Certificate Management](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#ssl-management)
7. [Database Management for Multiple Projects](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#database-management)
8. [Nginx as Reverse Proxy](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#nginx-proxy)
9. [Complete Real-World Example](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#real-world-example)
10. [Best Practices and Production Tips](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#best-practices)

---

## 1. Introduction to Docker Server Management {#introduction}

When you have a single server running multiple projects, each project typically needs its own set of services: a web application, database, cache layer, message queue, and more. Without containerization, managing all these services becomes a nightmare. Imagine trying to run five different Node.js projects, each requiring different versions of dependencies, different databases, and different configurations—all on the same machine without conflicts.

Docker solves this problem by providing **isolation**. Each project lives in its own container ecosystem, completely separated from others, yet all running on the same physical server. Think of Docker containers as individual apartments in a large building—each apartment (container) has its own utilities, but they all share the same building infrastructure (server hardware).

### Why Docker for Multi-Project Servers?

The traditional approach of running everything directly on the server creates several problems:

**Port Conflicts**: If Project A wants to run on port 3000 and Project B also wants port 3000, you have an immediate conflict. With Docker, both can run on port 3000 _inside their containers_, while the host machine maps them to different external ports (like 3001 and 3002).

**Dependency Hell**: Project A might need Node.js 14 with MongoDB 4.4, while Project B needs Node.js 18 with MongoDB 6.0. Installing multiple versions of everything on the bare server is messy and error-prone. Docker lets each project have exactly what it needs, isolated from others.

**Environment Consistency**: What works on your development machine should work on the server. Docker ensures that the environment is identical, eliminating the "it works on my machine" problem.

**Resource Isolation**: Docker can limit how much CPU, memory, and disk each project uses, preventing one project from consuming all resources and crashing others.

### The Docker Server Management Mental Model

Think of your server management with Docker in layers:

```
┌────────────────────────────────────────────────────────────┐
│                     Physical Server                        │
│  ┌──────────────────────────────────────────────────────┐  │
│  │              Docker Engine (Host)                    │  │
│  │  ┌─────────────────┐    ┌─────────────────┐          │  │
│  │  │   Project 1     │    │   Project 2     │          │  │
│  │  │  ┌───────────┐  │    │  ┌───────────┐  │          │  │
│  │  │  │  Web App  │  │    │  │  Web App  │  │          │  │
│  │  │  │(Node:3000)│  │    │  │(Node:3000)│  │          │  │
│  │  │  └───────────┘  │    │  └───────────┘  │          │  │
│  │  │  ┌───────────┐  │    │  ┌───────────┐  │          │  │
│  │  │  │ MongoDB   │  │    │  │PostgreSQL │  │          │  │
│  │  │  │  (27017)  │  │    │  │  (5432)   │  │          │  │
│  │  │  └───────────┘  │    │  └───────────┘  │          │  │
│  │  │  ┌───────────┐  │    │  ┌───────────┐  │          │  │
│  │  │  │   Redis   │  │    │  │ RabbitMQ  │  │          │  │
│  │  │  │  (6379)   │  │    │  │  (5672)   │  │          │  │
│  │  │  └───────────┘  │    │  └───────────┘  │          │  │
│  │  └─────────────────┘    └─────────────────┘          │  │
│  │  ┌──────────────────────────────────────────────┐    │  │
│  │  │         Nginx Reverse Proxy                  │    │  │
│  │  │  (Routes traffic to correct project)         │    │  │
│  │  └──────────────────────────────────────────────┘    │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────┘
```

In this architecture, each project is completely self-contained. The Nginx container sits at the front, receiving all incoming traffic and routing it to the appropriate project based on the domain name or path.

---

## 2. Understanding Docker Architecture {#architecture}

Before diving into multi-project management, you need to understand the fundamental building blocks of Docker. These concepts are like learning the parts of a car before you learn to drive.

### Docker Images: The Blueprint

A Docker image is like a blueprint or a recipe. It contains everything needed to run an application: the operating system, runtime environment, dependencies, and your code. When you write a Dockerfile, you're creating instructions for building this blueprint.

Think of an image as a read-only template. You can't modify an image directly—instead, you build a new image with your changes. Images are built in layers, and Docker caches these layers. If you change only your application code but not the dependencies, Docker reuses the cached dependency layers, making builds much faster.

```dockerfile
# Example Dockerfile showing layered structure
FROM node:18-alpine          # Layer 1: Base OS and Node.js
WORKDIR /app                 # Layer 2: Set working directory
COPY package*.json ./        # Layer 3: Copy dependency files
RUN npm install              # Layer 4: Install dependencies (cached!)
COPY . .                     # Layer 5: Copy application code
CMD ["node", "server.js"]    # Layer 6: Define startup command
```

Each `FROM`, `RUN`, `COPY`, and other instruction creates a new layer. The brilliance of this system is that if you modify only your `server.js` file, Docker rebuilds only Layer 5 and 6, reusing Layers 1-4 from cache.

### Docker Containers: The Running Instance

If an image is a blueprint, a container is the actual constructed building. When you run `docker run`, you create a container from an image. Multiple containers can be created from the same image—just like building multiple houses from the same blueprint.

Containers are **ephemeral** by nature. When you stop and remove a container, everything inside it disappears unless you've specifically stored data in volumes (we'll cover this shortly). This might seem scary at first, but it's actually a powerful feature. It means you can destroy and recreate containers without fear—your data is safely stored elsewhere, and your image ensures the container always starts in a known, clean state.

Here's the lifecycle of a container:

```
┌─────────┐   docker run    ┌─────────┐   docker stop   ┌─────────┐
│  Image  │ ──────────────> │ Running │ ──────────────> │ Stopped │
└─────────┘                 │Container│                 │Container│
                            └─────────┘                 └─────────┘
                                 │                            │
                                 │ docker kill                │
                                 │ (force stop)               │
                                 ▼                            │
                            ┌─────────┐   docker rm           │
                            │ Killed  │◄──────────────────────┘
                            │Container│     (remove)
                            └─────────┘
                                 │
                                 ▼
                            Completely
                             Deleted
```

### Docker Volumes: Persistent Data Storage

Since containers are ephemeral, how do we handle databases or user-uploaded files that must persist even when containers restart? This is where volumes come in.

A Docker volume is a storage location managed by Docker but exists outside any container. Think of it as an external hard drive that you can plug into different containers. When you create a volume and attach it to a container, the container can write data to that volume, and the data remains even after the container is deleted.

There are three types of storage in Docker:

**Volumes** (recommended for most use cases):

```bash
# Create a named volume
docker volume create mongo_data

# Use it in a container
docker run -v mongo_data:/data/db mongo:latest
```

This creates a volume managed by Docker in `/var/lib/docker/volumes/mongo_data`. The `:` syntax maps the volume to a path inside the container (`/data/db` is where MongoDB stores its data).

**Bind Mounts** (useful for development):

```bash
# Mount a host directory directly into the container
docker run -v /home/user/project/data:/data/db mongo:latest
```

This directly mounts a folder from your server into the container. Changes in the container immediately reflect on the host, and vice versa. This is great for development because you can edit code on your host machine and see changes instantly in the container.

**tmpfs Mounts** (temporary in-memory storage):

```bash
docker run --tmpfs /tmp mongo:latest
```

This creates a temporary storage location in RAM. It's fast but volatile—data disappears when the container stops. Useful for temporary files or caches.

### Docker Networks: Container Communication

By default, containers are isolated—they can't talk to each other. Docker networks solve this problem. When you create a Docker network and attach containers to it, those containers can communicate using container names as hostnames.

Imagine you have three containers: a Node.js app, a MongoDB database, and Redis cache. Without a network, your app can't connect to MongoDB. With a network, your app can connect to MongoDB using `mongodb://mongo:27017` where `mongo` is the container name—Docker's internal DNS resolves this automatically.

Docker provides several network types:

**Bridge Network** (default and most common):

```
┌──────────────────────────────────────────────────────┐
│            Docker Bridge Network                     │
│                                                      │
│  ┌────────────┐         ┌────────────┐               │
│  │   App      │◄───────►│  MongoDB   │               │
│  │ Container  │         │ Container  │               │
│  │            │         │            │               │
│  └────────────┘         └────────────┘               │
│       ▲                                              │
│       │ Can communicate using container names        │
│       ▼                                              │
│  ┌────────────┐                                      │
│  │   Redis    │                                      │
│  │ Container  │                                      │
│  └────────────┘                                      │
│                                                      │
└──────────────────────────────────────────────────────┘
          │
          │ Host network interface
          ▼
    External World
```

Containers on the same bridge network can talk to each other but are isolated from containers on different networks. This is perfect for multi-project servers—each project gets its own network.

**Host Network** (container uses host's network directly):

```bash
docker run --network host myapp
```

The container shares the host's network stack. If the container listens on port 3000, it's directly accessible on the host's port 3000. There's no network isolation, and port conflicts can occur. Use sparingly.

**Custom Networks** (explicitly created by you):

```bash
docker network create project1_network
docker run --network project1_network --name app myapp
docker run --network project1_network --name db mongo
```

Now `app` can connect to `db` using `mongodb://db:27017`. The container name becomes the hostname.

---

## 3. Managing Multiple Projects with Docker Compose {#docker-compose}

Docker Compose is a tool that transforms the complex task of managing multiple containers into simple configuration files. Instead of running long `docker run` commands with dozens of flags for each container, you write a YAML file describing your entire project's infrastructure.

### The Power of docker-compose.yml

Think of Docker Compose as an orchestrator for a symphony. Each container is an instrument, and the `docker-compose.yml` file is the musical score that tells each instrument when to play, at what volume, and how to harmonize with others.

Let's start with a simple example and build up to a complex multi-project setup:

```yaml
# docker-compose.yml for a simple Node.js + MongoDB project
version: '3.8'

services:
  app:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - MONGO_URL=mongodb://mongo:27017/mydb
    depends_on:
      - mongo
    networks:
      - backend

  mongo:
    image: mongo:6.0
    volumes:
      - mongo_data:/data/db
    networks:
      - backend

volumes:
  mongo_data:

networks:
  backend:
```

Let's dissect every part of this file to understand what's happening:

**version: '3.8'**: This specifies which version of the Docker Compose file format we're using. Version 3.8 supports all modern features.

**services**: This is where you define all containers for your project. Each service becomes one running container (or multiple if you scale it).

**app service**:

- `build: .` tells Docker Compose to build an image from the Dockerfile in the current directory
- `ports: - "3000:3000"` maps port 3000 from the container to port 3000 on the host. The format is `HOST:CONTAINER`
- `environment` sets environment variables inside the container. Notice `MONGO_URL` uses `mongo` as the hostname—this works because both containers are on the same network
- `depends_on` ensures MongoDB starts before the app. However, this only waits for the container to start, not for MongoDB to be ready to accept connections (we'll handle this properly later)
- `networks` attaches this container to the `backend` network

**mongo service**:

- `image: mongo:6.0` uses an official MongoDB image from Docker Hub instead of building one
- `volumes` mounts the named volume `mongo_data` to `/data/db`, persisting database data
- `networks` attaches to the same `backend` network, enabling communication with the app

**volumes**: Declares named volumes. Docker Compose creates them if they don't exist.

**networks**: Declares custom networks. Docker Compose creates them automatically.

### Project Folder Structure

For managing multiple projects, organize your server like this:

```
/opt/docker-projects/
├── project1/
│   ├── docker-compose.yml
│   ├── .env
│   ├── app/
│   │   ├── Dockerfile
│   │   ├── package.json
│   │   └── src/
│   ├── nginx/
│   │   └── nginx.conf
│   └── data/
├── project2/
│   ├── docker-compose.yml
│   ├── .env
│   ├── api/
│   │   ├── Dockerfile
│   │   └── main.go
│   └── data/
├── project3/
│   ├── docker-compose.yml
│   └── ...
└── nginx-proxy/
    ├── docker-compose.yml
    ├── nginx.conf
    └── ssl/
        ├── cert1.pem
        └── key1.pem
```

Each project is self-contained in its own folder with its own `docker-compose.yml`. The `nginx-proxy` folder contains a reverse proxy that routes traffic to the appropriate project based on the domain name.

### Environment Variables and .env Files

Hardcoding configuration in `docker-compose.yml` is a bad practice. What if you need different database passwords for development and production? Use `.env` files:

```bash
# .env file for project1
COMPOSE_PROJECT_NAME=project1
APP_PORT=3001
NODE_ENV=production
MONGO_USER=admin
MONGO_PASSWORD=securepass123
MONGO_DATABASE=project1_db
```

Now reference these in your compose file:

```yaml
services:
  app:
    ports:
      - "${APP_PORT}:3000"
    environment:
      - NODE_ENV=${NODE_ENV}
      - MONGO_URL=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongo:27017/${MONGO_DATABASE}
```

The `${VARIABLE}` syntax substitutes values from the `.env` file. Docker Compose automatically reads `.env` from the same directory as `docker-compose.yml`.

**Important**: Add `.env` to your `.gitignore`! Never commit secrets to version control. Instead, commit a `.env.example` template:

```bash
# .env.example (commit this)
COMPOSE_PROJECT_NAME=project1
APP_PORT=3001
NODE_ENV=production
MONGO_USER=admin
MONGO_PASSWORD=changeme
MONGO_DATABASE=project1_db
```

Team members copy this to `.env` and fill in real values.

### Understanding COMPOSE_PROJECT_NAME

By default, Docker Compose prefixes all container, network, and volume names with the directory name. If your project is in `/opt/docker-projects/project1`, all containers are named like `project1_app_1`, `project1_mongo_1`.

Setting `COMPOSE_PROJECT_NAME=project1` explicitly ensures consistent naming even if you move the folder. It also prevents naming conflicts between projects.

### Docker Compose Commands

Here are the essential commands you'll use daily:

```bash
# Start all services (creates containers if needed)
docker-compose up -d

# -d runs in detached mode (background)
# Without -d, logs print to terminal

# View running services
docker-compose ps

# Stop all services (containers still exist)
docker-compose stop

# Start stopped services
docker-compose start

# Stop and remove containers, networks (volumes persist)
docker-compose down

# Stop, remove everything including volumes (destructive!)
docker-compose down -v

# View logs from all services
docker-compose logs

# View logs from specific service
docker-compose logs app

# Follow logs (like tail -f)
docker-compose logs -f app

# Rebuild images (after Dockerfile changes)
docker-compose build

# Rebuild and restart
docker-compose up -d --build

# Execute command in running container
docker-compose exec app node scripts/migrate.js

# Run one-off command (creates new container)
docker-compose run app npm test

# Scale a service to multiple instances
docker-compose up -d --scale app=3
```

The difference between `exec` and `run` is crucial: `exec` runs a command in an already-running container, while `run` creates a new container just for that command.

---

## 4. Port Management Strategies {#port-management}

One of the biggest challenges when running multiple projects on a single server is port management. Every service needs a unique port on the host machine, but inside containers, services can use their standard ports without conflicts.

### Understanding Port Mapping

When you map ports in Docker, you're creating a bridge between the host and container. The format is always `HOST_PORT:CONTAINER_PORT`. Here's the critical insight: **the container port can be the same across different containers, but the host port must be unique**.

```
┌─────────────────────────────────────────────────────────┐
│                    Host Server                          │
│                                                         │
│  Port 3001 ──────────────┐                              │
│  Port 3002 ────────┐     │                              │
│  Port 3003 ──┐     │     │                              │
│              │     │     │                              │
│  ┌───────────▼─────▼─────▼─────────────────────────-┐   │
│  │           Docker Network Layer                   │   │
│  │                                                  │   │
│  │  ┌──────────────┐  ┌──────────────┐  ┌────────┐  │   │
│  │  │  Project 1   │  │  Project 2   │  │Project3│  │   │
│  │  │  App         │  │  App         │  │App     │  │   │
│  │  │  Port 3000   │  │  Port 3000   │  │Port3000│  │   │
│  │  └──────────────┘  └──────────────┘  └────────┘  │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
```

All three containers listen on port 3000 internally (no conflict because they're isolated), but they're accessible from outside via ports 3001, 3002, and 3003 respectively.

### Port Allocation Strategy for Multiple Projects

When managing many projects, you need a systematic approach to port allocation. Random port assignments lead to chaos and forgotten services. Here's a recommended strategy:

**Create a port allocation table**:

```
Project      | Service    | Container Port | Host Port | Public?
-------------|------------|----------------|-----------|--------
project1     | app        | 3000          | 3001      | No
project1     | mongo      | 27017         | 27101     | No
project1     | redis      | 6379          | 6301      | No
project2     | app        | 3000          | 3002      | No
project2     | postgres   | 5432          | 5402      | No
project2     | rabbitmq   | 5672          | 5602      | No
project2     | rabbitmq-ui| 15672         | 15602     | Yes
project3     | api        | 8080          | 8003      | No
project3     | mongo      | 27017         | 27103     | No
nginx-proxy  | nginx      | 80            | 80        | Yes
nginx-proxy  | nginx      | 443           | 443       | Yes
```

Notice the pattern:

- Web apps use 3001, 3002, 3003 (base + project number)
- MongoDB ports use 271xx (27100 + project number)
- Redis ports use 63xx (6300 + project number)
- PostgreSQL ports use 54xx (5400 + project number)

This systematic approach makes it easy to remember and manage ports. Document this in a shared file (like `PORTS.md`) in your project repository.

### Internal vs External Ports

Most of your services should **not** expose ports to the host at all. Only expose ports when necessary:

**Expose to host when**:

- You need direct database access for debugging
- You're running development tools that need to connect
- A service must be publicly accessible

**Don't expose when**:

- Services only need to talk to each other (use Docker networks instead)
- It's a production system and the service sits behind a reverse proxy

Here's an example of minimal port exposure:

```yaml
version: '3.8'

services:
  # No ports exposed - only accessible within Docker network
  app:
    build: ./app
    networks:
      - project1
    environment:
      - MONGO_URL=mongodb://mongo:27017/mydb
      - REDIS_URL=redis://redis:6379

  # No ports exposed - app connects via network
  mongo:
    image: mongo:6.0
    networks:
      - project1
    volumes:
      - mongo_data:/data/db

  # No ports exposed - app connects via network
  redis:
    image: redis:7-alpine
    networks:
      - project1

  # Only Nginx exposes ports - it's the public-facing gateway
  nginx:
    image: nginx:alpine
    ports:
      - "3001:80"  # Only one port exposed for the entire project
    networks:
      - project1
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro

volumes:
  mongo_data:

networks:
  project1:
```

In this setup, only Nginx exposes port 3001 to the host. All internal services communicate through the `project1` network. This is much more secure—databases and caches aren't accessible from outside the Docker environment.

### Handling Port Conflicts

What happens if you accidentally assign the same host port to two projects? Docker will fail to start the second container with an error like:

```
Error: Bind for 0.0.0.0:3001 failed: port is already allocated
```

To debug port conflicts:

```bash
# Find what's using a port on the host
sudo netstat -tlnp | grep :3001

# Or using lsof
sudo lsof -i :3001

# List all exposed ports from Docker containers
docker ps --format "table {{.Names}}\t{{.Ports}}"
```

If you find a conflict, update the port in the affected project's `docker-compose.yml` and restart:

```bash
cd /opt/docker-projects/project2
# Edit docker-compose.yml, change port from 3001 to 3002
docker-compose down
docker-compose up -d
```

---

## 5. Docker Networks Deep Dive {#docker-networks}

Docker networks are the nervous system of your multi-project infrastructure. They control how containers communicate with each other and with the outside world. Understanding networks deeply is crucial for building secure, scalable multi-project setups.

### Network Isolation Between Projects

By default, containers from different projects cannot communicate with each other. This is a security feature. If Project 1 gets compromised, the attacker can't automatically access Project 2's database. Each project should have its own isolated network.

Here's the architecture:

```
┌────────────────────────────────────────────────────────────┐
│                    Docker Host                             │
│                                                            │
│  ┌──────────────────────────────────────────────────┐      │
│  │          project1_network (Bridge)               │      │
│  │                                                  │     │
│  │   ┌──────────┐    ┌──────────┐    ┌─────────┐    │     │
│  │   │   App    │◄──►│  Mongo   │◄──►│  Redis  │    │     │
│  │   │ (3000)   │    │ (27017)  │    │ (6379)  │    │     │
│  │   └──────────┘    └──────────┘    └─────────┘    │     │
│  │                                                  │     │
│  └──────────────────────────────────────────────────┘     │
│                                                           │
│  ┌──────────────────────────────────────────────────┐     │
│  │          project2_network (Bridge)               │     │
│  │                                                  │     │
│  │   ┌──────────┐    ┌───────────┐   ┌──────────┐   │     │
│  │   │   API    │◄──►│PostgreSQL │◄──►│RabbitMQ │   │     │
│  │   │ (8080)   │    │  (5432)   │   │ (5672)   │   │     │
│  │   └──────────┘    └───────────┘   └──────────┘   │     │
│  │                                                  │     │
│  └──────────────────────────────────────────────────┘     │
│                                                           │
│  ┌──────────────────────────────────────────────────┐     │
│  │          proxy_network (Bridge)                  │     │
│  │                                                  │     │
│  │                ┌─────────────┐                   │     │
│  │                │    Nginx    │                   │     │
│  │                │   Proxy     │                   │     │
│  │                └──────┬──────┘                   │     │
│  │                       │                          │     │
│  └───────────────────────┼──────────────────────────┘     │
│                          │                                │
│  ┌───────────────────────┼───────────────────────────┐    │
│  │   shared_network      │       (Bridge)            │    │
│  │                       │                           │    │
│  │   ┌───────────────────▼──────────────┐            │    │
│  │   │  Nginx connects to both projects │            │    │
│  │   └──────────────────────────────────┘            │    │
│  └───────────────────────────────────────────── ─────┘    │
└───────────────────────────────────────────────────────────┘
```

Notice how Project 1's containers can only talk to each other within `project1_network`. Project 2's containers are isolated in `project2_network`. The Nginx proxy needs to connect to both projects, so it exists on a shared network that bridges the isolation boundary.

### Creating and Using Custom Networks

Let's walk through creating networks for a real multi-project setup:

```bash
# Create separate networks for each project
docker network create project1_network
docker network create project2_network
docker network create proxy_network
```

Then in each project's `docker-compose.yml`, declare these networks:

**Project 1's docker-compose.yml**:

```yaml
version: '3.8'

services:
  app:
    build: ./app
    networks:
      - project1_internal
      - proxy_network  # Also connected to proxy for external access
    environment:
      - MONGO_URL=mongodb://mongo:27017/project1

  mongo:
    image: mongo:6.0
    networks:
      - project1_internal  # Only internal network - not accessible from proxy
    volumes:
      - project1_mongo:/data/db

  redis:
    image: redis:7-alpine
    networks:
      - project1_internal
    volumes:
      - project1_redis:/data

volumes:
  project1_mongo:
  project1_redis:

networks:
  project1_internal:
    name: project1_network
    external: false  # Managed by this compose file
  proxy_network:
    name: proxy_network
    external: true   # Created separately, shared with proxy
```

**Project 2's docker-compose.yml**:

```yaml
version: '3.8'

services:
  api:
    build: ./api
    networks:
      - project2_internal
      - proxy_network
    environment:
      - DB_URL=postgresql://user:pass@postgres:5432/project2

  postgres:
    image: postgres:15-alpine
    networks:
      - project2_internal
    volumes:
      - project2_postgres:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: project2
      POSTGRES_USER: user
      POSTGRES_PASSWORD: securepass

networks:
  project2_internal:
    name: project2_network
    external: false
  proxy_network:
    name: proxy_network
    external: true
```

The key points here:

1. Each project has an **internal network** (`project1_internal`, `project2_internal`) for communication between its own services
2. Both projects connect their web services to a **shared proxy network** so Nginx can reach them
3. Databases and caches stay on the internal network only—they're not accessible from the proxy network for security
4. `external: true` means the network must already exist (created manually or by another compose file)
5. `external: false` means Docker Compose will create and manage this network

### Network Naming and External Networks

When you use `external: true`, you're telling Docker Compose: "Don't create this network—assume it already exists." This is crucial for the shared proxy network because multiple projects need to connect to it.

The `name:` field specifies the actual Docker network name. Without it, Docker Compose would create a network named something like `project1_proxy_network` (prefixed with the project name), and each project would create its own separate proxy network—defeating the purpose of sharing.

### Docker Network Types Explained

Docker supports several network drivers, each suited for different scenarios:

**Bridge Network** (most common):

- Default network type
- Containers get private IP addresses (like 172.17.0.2, 172.17.0.3)
- Containers communicate via internal DNS using container names
- Isolated from other bridge networks
- Perfect for single-host deployments

```bash
docker network create --driver bridge project1_network
```

**Host Network**:

- Container shares the host's network stack directly
- No network isolation
- No port mapping needed—container's port 3000 is directly accessible on host's port 3000
- Faster performance (no network translation overhead)
- Port conflicts become possible
- Use only when performance is critical and you understand the security implications

```yaml
services:
  app:
    network_mode: "host"  # No ports section needed
```

**Overlay Network** (for multi-host/swarm):

- Spans multiple Docker hosts
- Enables containers on different physical servers to communicate
- Required for Docker Swarm mode
- Not needed for single-server setups

**None Network**:

- Disables all networking
- Container is completely isolated
- Useful for batch processing jobs that don't need network access

```yaml
services:
  processor:
    network_mode: "none"
```

### DNS Resolution in Docker Networks

Docker provides automatic DNS resolution for container names. When you connect containers to the same network, Docker runs an embedded DNS server that resolves container names to their IP addresses.

Example: If you have containers named `app`, `mongo`, and `redis` on the same network, the app can connect using:

```javascript
// In your Node.js application
const mongoUrl = 'mongodb://mongo:27017/mydb';
const redisUrl = 'redis://redis:6379';
```

The names `mongo` and `redis` are the container names (or service names in Docker Compose). Docker's DNS automatically resolves these to the correct internal IP addresses.

**Important**: This only works for user-defined networks (like the ones you create with `docker network create` or define in Compose files). The default bridge network (named `bridge`) does not provide automatic DNS resolution—you'd have to use IP addresses, which change every time containers restart.

### Network Inspection and Debugging

When things don't work, you need to inspect your networks:

```bash
# List all networks
docker network ls

# Inspect a specific network (shows connected containers)
docker network inspect project1_network

# Output shows:
# - Network subnet and gateway
# - All containers connected to this network
# - Their IP addresses
# - Network driver type

# Connect a running container to a network
docker network connect proxy_network project1_app_1

# Disconnect a container
docker network disconnect project1_network project1_app_1

# Test connectivity from one container to another
docker exec project1_app_1 ping mongo
docker exec project1_app_1 curl http://mongo:27017
```

If container A can't reach container B:

1. Check they're on the same network: `docker network inspect network_name`
2. Verify container B is running: `docker ps`
3. Test DNS resolution: `docker exec containerA nslookup containerB`
4. Check for firewall rules blocking internal traffic (rare but possible)

### Advanced Network Configuration

You can fine-tune network settings for specific requirements:

```yaml
networks:
  project1_network:
    driver: bridge
    ipam:  # IP Address Management
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
    driver_opts:
      com.docker.network.bridge.name: br-project1  # Custom bridge name
      com.docker.network.bridge.enable_icc: "true" # Inter-container communication
      com.docker.network.bridge.enable_ip_masquerade: "true"
```

This creates a network with a specific IP range. Containers on this network get IPs like 172.20.0.2, 172.20.0.3, etc. You can even assign static IPs:

```yaml
services:
  app:
    networks:
      project1_network:
        ipv4_address: 172.20.0.10  # Fixed IP for this container
```

Static IPs are useful when services have IP-based security rules or need consistent addresses for external integrations.

---

## 6. SSL Certificate Management {#ssl-management}

In production, your websites must use HTTPS for security, SEO, and user trust. Managing SSL certificates for multiple projects on a single server requires careful planning, especially when automating renewals.

### Understanding SSL/TLS in Docker Context

SSL certificates prove your server's identity to browsers and encrypt traffic between the browser and your server. When you have multiple projects with different domain names (like project1.com, project2.com, api.example.com), you need certificates for each domain.

Here's how HTTPS works with Docker:

```
┌──────────────────────────────────────────────────────────────┐
│                      Internet                                │
│                                                              │
│     ┌──────────────────────────────────────────────┐         │
│     │  Browser requests https://project1.com       │         │
│     └─────────────┬────────────────────────────────┘         │
│                   │ Encrypted traffic (HTTPS)                │
└───────────────────┼──────────────────────────────────────────┘
                    │
            ┌───────▼────────┐
            │  Firewall      │
            │  Port 443      │
            └───────┬────────┘
                    │
┌───────────────────▼──────────────────────────────────────────┐
│                Docker Host Server                            │
│                                                              │
│  ┌──────────────────────────────────────────────────────┐    │
│  │    Nginx Reverse Proxy Container                     │    │
│  │  - Holds SSL certificates                            │    │
│  │  - Terminates HTTPS (decrypts traffic)               │    │
│  │  - Routes to correct project based on domain         │    │
│  │                                                      │    │
│  │  /etc/nginx/ssl/project1.com.crt                     │    │
│  │  /etc/nginx/ssl/project1.com.key                     │    │
│  │  /etc/nginx/ssl/project2.com.crt                     │    │
│  │  /etc/nginx/ssl/project2.com.key                     │    │
│  └────────────┬───────────────┬─────────────────────────┘    │
│               │               │                              │
│       ┌───────▼──────┐  ┌────▼────────┐                      │
│       │  Project 1   │  │  Project 2  │                      │
│       │  Container   │  │  Container  │                      │
│       │  (HTTP only) │  │  (HTTP only)│                      │
│       │  Port 3000   │  │  Port 3000  │                      │
│       └──────────────┘  └─────────────┘                      │
│                                                              │
└──────────────────────────────────────────────────────────────┘
```

The Nginx proxy container handles all SSL/TLS. It receives encrypted HTTPS traffic, decrypts it, then forwards plain HTTP traffic to your application containers. This is called **SSL termination**—the SSL "terminates" at the proxy, and internal traffic is unencrypted.

Why terminate SSL at the proxy instead of in each application?

1. **Simplicity**: Your apps don't need SSL configuration—they just serve HTTP
2. **Centralized certificate management**: All certificates live in one place
3. **Performance**: Nginx is highly optimized for SSL/TLS
4. **Easier debugging**: Internal HTTP traffic can be inspected easily

### Let's Encrypt with Certbot

Let's Encrypt provides free SSL certificates that auto-renew every 90 days. Certbot is a tool that automatically obtains and renews certificates. Here's how to set it up with Docker:

**Option 1: Certbot as a separate container**

Create a docker-compose file specifically for SSL management:

```yaml
# /opt/docker-projects/ssl-manager/docker-compose.yml
version: '3.8'

services:
  certbot:
    image: certbot/certbot:latest
    volumes:
      - certbot_data:/etc/letsencrypt  # Stores certificates
      - certbot_www:/var/www/certbot   # For ACME challenge
      - certbot_logs:/var/log/letsencrypt
    entrypoint: "/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'"

volumes:
  certbot_data:
  certbot_www:
  certbot_logs:
```

This container runs continuously, checking for expiring certificates twice per day and renewing them automatically.

**Option 2: Run Certbot directly on host**

```bash
# Install certbot on the host
sudo apt-get update
sudo apt-get install certbot

# Obtain a certificate for a domain
sudo certbot certonly --standalone \
  -d project1.com \
  -d www.project1.com \
  --non-interactive \
  --agree-tos \
  --email admin@project1.com

# Certificate files are created at:
# /etc/letsencrypt/live/project1.com/fullchain.pem (certificate)
# /etc/letsencrypt/live/project1.com/privkey.pem (private key)
```

The `--standalone` mode temporarily runs a web server on port 80 to prove you control the domain. This means you need to stop Nginx before running certbot, or use the `webroot` mode.

**Option 3: Webroot mode (no downtime)**

If Nginx is already running, use webroot mode:

```bash
sudo certbot certonly --webroot \
  -w /var/www/certbot \
  -d project1.com \
  -d www.project1.com \
  --non-interactive \
  --agree-tos \
  --email admin@project1.com
```

Certbot writes challenge files to `/var/www/certbot`, and Nginx must be configured to serve them:

```nginx
server {
    listen 80;
    server_name project1.com www.project1.com;
    
    # Serve ACME challenge for Let's Encrypt
    location /.well-known/acme-challenge/ {
        root /var/www/certbot;
    }
    
    # Redirect everything else to HTTPS
    location / {
        return 301 https://$host$request_uri;
    }
}
```

### Mounting Certificates into Nginx Container

Once certificates are obtained, mount them into your Nginx proxy container:

```yaml
# nginx-proxy/docker-compose.yml
version: '3.8'

services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./conf.d:/etc/nginx/conf.d:ro
      - /etc/letsencrypt:/etc/letsencrypt:ro  # Mount certificates
      - /var/www/certbot:/var/www/certbot:ro  # Mount webroot for renewals
    networks:
      - proxy_network
    restart: unless-stopped

networks:
  proxy_network:
    external: true
```

Notice `/etc/letsencrypt` is mounted read-only (`:ro`) into the container. Nginx can read certificates but can't modify them.

### Nginx SSL Configuration

Now configure Nginx to use these certificates:

```nginx
# nginx-proxy/conf.d/project1.conf

# HTTP server - redirects to HTTPS
server {
    listen 80;
    server_name project1.com www.project1.com;
    
    # Let's Encrypt challenge location
    location /.well-known/acme-challenge/ {
        root /var/www/certbot;
    }
    
    # Redirect all other traffic to HTTPS
    location / {
        return 301 https://$host$request_uri;
    }
}

# HTTPS server
server {
    listen 443 ssl http2;
    server_name project1.com www.project1.com;
    
    # SSL certificate paths
    ssl_certificate /etc/letsencrypt/live/project1.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/project1.com/privkey.pem;
    
    # SSL optimization and security
    ssl_session_timeout 1d;
    ssl_session_cache shared:SSL:50m;
    ssl_session_tickets off;
    
    # Modern SSL configuration (TLS 1.2 and 1.3 only)
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers 'ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384';
    ssl_prefer_server_ciphers off;
    
    # HSTS - force HTTPS for 1 year
    add_header Strict-Transport-Security "max-age=31536000" always;
    
    # Proxy to application container
    location / {
        proxy_pass http://project1_app:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;
    }
}
```

Key SSL directives explained:

- `ssl_certificate`: The public certificate (safe to share)
- `ssl_certificate_key`: The private key (keep secret!)
- `ssl_protocols`: Only allow modern, secure TLS versions
- `ssl_ciphers`: Encryption algorithms to use
- `add_header Strict-Transport-Security`: Tells browsers to always use HTTPS for this domain

### Automatic Certificate Renewal

Let's Encrypt certificates expire every 90 days. Automate renewal with a cron job:

```bash
# Edit crontab
sudo crontab -e

# Add this line to check for renewals twice daily
0 0,12 * * * certbot renew --quiet --deploy-hook "docker exec nginx-proxy nginx -s reload"
```

The `--deploy-hook` reloads Nginx after renewal so it picks up new certificates without downtime.

If using the certbot container approach, it handles this automatically (see the while loop in the container's entrypoint).

### Wildcard Certificates

If you have many subdomains (api.project1.com, admin.project1.com, blog.project1.com), get a wildcard certificate instead of individual certificates:

```bash
sudo certbot certonly --manual \
  --preferred-challenges=dns \
  -d project1.com \
  -d *.project1.com \
  --email admin@project1.com
```

Certbot will ask you to create a DNS TXT record to prove domain ownership. After adding the record, wait for DNS propagation (can take a few minutes), then press Enter.

Wildcard certificates are powerful but renewal is trickier—you need to update the TXT record each time. Consider using a DNS plugin if your DNS provider is supported:

```bash
# Example for Cloudflare
sudo certbot certonly --dns-cloudflare \
  --dns-cloudflare-credentials ~/.secrets/cloudflare.ini \
  -d project1.com \
  -d *.project1.com
```

This automates DNS challenges, making renewals hands-free.

### Self-Signed Certificates for Development

For local development or internal testing, use self-signed certificates:

```bash
# Create self-signed certificate
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout ./ssl/selfsigned.key \
  -out ./ssl/selfsigned.crt \
  -subj "/C=US/ST=State/L=City/O=Organization/CN=localhost"
```

Mount these into your Nginx container:

```yaml
volumes:
  - ./ssl/selfsigned.crt:/etc/nginx/ssl/selfsigned.crt:ro
  - ./ssl/selfsigned.key:/etc/nginx/ssl/selfsigned.key:ro
```

Update Nginx config:

```nginx
ssl_certificate /etc/nginx/ssl/selfsigned.crt;
ssl_certificate_key /etc/nginx/ssl/selfsigned.key;
```

Browsers will show a warning because the certificate isn't from a trusted authority, but this is fine for development.

---

## 7. Database Management for Multiple Projects {#database-management}

When running multiple projects, each typically needs its own database. You could run a single database instance with multiple databases inside it, but isolating databases per project provides better security, resource management, and migration flexibility.

### Single Instance vs. Multiple Instances

**Approach 1: One database instance, multiple databases**

```yaml
# Shared MongoDB container used by multiple projects
services:
  mongo:
    image: mongo:6.0
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: rootpassword
    volumes:
      - mongo_data:/data/db
    networks:
      - shared_db_network

# Project 1 connects to: mongodb://admin:rootpassword@mongo:27017/project1_db
# Project 2 connects to: mongodb://admin:rootpassword@mongo:27017/project2_db
```

Pros:

- Less memory overhead (one database engine instead of multiple)
- Easier to manage and backup
- Shared connection pooling

Cons:

- Security risk—if one project is compromised, attacker can access all databases
- Resource contention—one project's heavy queries can slow down others
- Harder to version—all projects must use the same database version
- Migration complexity—upgrading database version affects all projects

**Approach 2: Separate database containers per project (recommended)**

```yaml
# Project 1's docker-compose.yml
services:
  mongo:
    image: mongo:6.0
    environment:
      MONGO_INITDB_ROOT_USERNAME: project1_user
      MONGO_INITDB_ROOT_PASSWORD: project1_secure_pass
    volumes:
      - project1_mongo:/data/db
    networks:
      - project1_network  # Only accessible within project1

# Project 2's docker-compose.yml
services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: project2_user
      POSTGRES_PASSWORD: project2_secure_pass
      POSTGRES_DB: project2_db
    volumes:
      - project2_postgres:/var/lib/postgresql/data
    networks:
      - project2_network  # Only accessible within project2
```

Pros:

- Complete isolation—compromising one project doesn't expose others
- Independent versioning—upgrade one project's database without affecting others
- Resource isolation—heavy queries in Project 1 don't impact Project 2
- Easier to move projects between servers

Cons:

- More memory usage (multiple database engines)
- More containers to manage

**Recommendation**: Use separate database containers per project unless you have a specific reason not to (like running 50+ tiny projects where resource efficiency is critical).

### Database-Specific Configurations

Each database type has unique considerations:

#### MongoDB Configuration

```yaml
services:
  mongo:
    image: mongo:6.0
    container_name: project1_mongo
    restart: unless-stopped
    environment:
      # Always set a root password
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
      # Create a specific database on initialization
      MONGO_INITDB_DATABASE: project1_db
    volumes:
      # Persist data
      - mongo_data:/data/db
      # Persist MongoDB logs
      - mongo_logs:/var/log/mongodb
      # Custom MongoDB configuration
      - ./mongo/mongod.conf:/etc/mongod.conf:ro
      # Initialization scripts (create users, indexes, etc.)
      - ./mongo/init-mongo.js:/docker-entrypoint-initdb.d/init-mongo.js:ro
    command: ["--config", "/etc/mongod.conf"]
    networks:
      - project1_internal
    # Resource limits
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
```

Custom mongod.conf example:

```yaml
# mongo/mongod.conf
storage:
  dbPath: /data/db
  journal:
    enabled: true

systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true
  logRotate: reopen

net:
  bindIp: 0.0.0.0
  port: 27017

security:
  authorization: enabled
```

Initialization script example:

```javascript
// mongo/init-mongo.js
db = db.getSiblingDB('project1_db');

db.createUser({
  user: 'project1_app',
  pwd: 'app_password',
  roles: [
    {
      role: 'readWrite',
      db: 'project1_db'
    }
  ]
});

// Create indexes
db.users.createIndex({ email: 1 }, { unique: true });
db.sessions.createIndex({ createdAt: 1 }, { expireAfterSeconds: 86400 });
```

#### PostgreSQL Configuration

```yaml
services:
  postgres:
    image: postgres:15-alpine
    container_name: project2_postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
      # Performance tuning
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 1GB
      POSTGRES_MAX_CONNECTIONS: 100
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
      - ./postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    command: ["postgres", "-c", "config_file=/etc/postgresql/postgresql.conf"]
    networks:
      - project2_internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
```

Custom postgresql.conf:

```conf
# postgres/postgresql.conf
listen_addresses = '*'
max_connections = 100
shared_buffers = 256MB
effective_cache_size = 1GB
maintenance_work_mem = 64MB
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100
random_page_cost = 1.1
effective_io_concurrency = 200
work_mem = 2621kB
min_wal_size = 1GB
max_wal_size = 4GB
```

#### MySQL Configuration

```yaml
services:
  mysql:
    image: mysql:8.0
    container_name: project3_mysql
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${DB_NAME}
      MYSQL_USER: ${DB_USER}
      MYSQL_PASSWORD: ${DB_PASSWORD}
    volumes:
      - mysql_data:/var/lib/mysql
      - ./mysql/my.cnf:/etc/mysql/conf.d/custom.cnf:ro
      - ./mysql/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    command: --default-authentication-plugin=mysql_native_password
    networks:
      - project3_internal
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
```

#### Redis Configuration

```yaml
services:
  redis:
    image: redis:7-alpine
    container_name: project1_redis
    restart: unless-stopped
    volumes:
      - redis_data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    command: redis-server /usr/local/etc/redis/redis.conf
    networks:
      - project1_internal
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
```

Custom redis.conf:

```conf
# redis/redis.conf
bind 0.0.0.0
protected-mode yes
port 6379
requirepass ${REDIS_PASSWORD}

# Persistence
save 900 1
save 300 10
save 60 10000

# Memory management
maxmemory 512mb
maxmemory-policy allkeys-lru
```

### Database Backup Strategies

Losing database data is catastrophic. Implement automated backups:

**MongoDB Backup Script**:

```bash
#!/bin/bash
# backup-mongo.sh

PROJECT="project1"
CONTAINER="${PROJECT}_mongo"
BACKUP_DIR="/opt/backups/mongo/${PROJECT}"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_NAME="${PROJECT}_mongo_${DATE}"

# Create backup directory
mkdir -p ${BACKUP_DIR}

# Dump database
docker exec ${CONTAINER} mongodump \
  --username admin \
  --password ${MONGO_ROOT_PASSWORD} \
  --authenticationDatabase admin \
  --out /tmp/${BACKUP_NAME}

# Copy dump from container to host
docker cp ${CONTAINER}:/tmp/${BACKUP_NAME} ${BACKUP_DIR}/

# Compress backup
tar -czf ${BACKUP_DIR}/${BACKUP_NAME}.tar.gz ${BACKUP_DIR}/${BACKUP_NAME}
rm -rf ${BACKUP_DIR}/${BACKUP_NAME}

# Remove backups older than 30 days
find ${BACKUP_DIR} -name "*.tar.gz" -mtime +30 -delete

echo "MongoDB backup completed: ${BACKUP_NAME}.tar.gz"
```

**PostgreSQL Backup Script**:

```bash
#!/bin/bash
# backup-postgres.sh

PROJECT="project2"
CONTAINER="${PROJECT}_postgres"
BACKUP_DIR="/opt/backups/postgres/${PROJECT}"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_NAME="${PROJECT}_postgres_${DATE}.sql"

mkdir -p ${BACKUP_DIR}

docker exec ${CONTAINER} pg_dump \
  -U ${DB_USER} \
  -d ${DB_NAME} \
  -F c \
  -f /tmp/${BACKUP_NAME}

docker cp ${CONTAINER}:/tmp/${BACKUP_NAME} ${BACKUP_DIR}/

# Compress
gzip ${BACKUP_DIR}/${BACKUP_NAME}

# Cleanup old backups
find ${BACKUP_DIR} -name "*.sql.gz" -mtime +30 -delete

echo "PostgreSQL backup completed: ${BACKUP_NAME}.gz"
```

**Automated Backup with Cron**:

```bash
# Edit crontab
sudo crontab -e

# Run MongoDB backup daily at 2 AM
0 2 * * * /opt/scripts/backup-mongo.sh >> /var/log/backup-mongo.log 2>&1

# Run PostgreSQL backup daily at 3 AM
0 3 * * * /opt/scripts/backup-postgres.sh >> /var/log/backup-postgres.log 2>&1
```

### Database Migrations

When your schema changes, you need to run migrations. Handle this carefully with Docker:

**Method 1: Migration container in docker-compose**:

```yaml
services:
  app:
    build: ./app
    depends_on:
      migrate:
        condition: service_completed_successfully
    networks:
      - project1_internal

  migrate:
    build: ./app
    command: npm run migrate
    environment:
      MONGO_URL: mongodb://mongo:27017/project1_db
    depends_on:
      mongo:
        condition: service_healthy
    networks:
      - project1_internal

  mongo:
    image: mongo:6.0
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - project1_internal
```

The `migrate` service runs first, waits for MongoDB to be healthy, runs migrations, then exits. Only after it completes successfully does the `app` service start.

**Method 2: Run migrations manually before deployment**:

```bash
# After updating code, run migrations
cd /opt/docker-projects/project1
docker-compose run --rm app npm run migrate

# Then restart the app to use new code
docker-compose up -d --build app
```

---

## 8. Nginx as Reverse Proxy {#nginx-proxy}

Nginx is the gateway to your multi-project server. It receives all incoming traffic, examines the domain name or URL path, and routes requests to the correct project container. This is called reverse proxying.

### Why Use a Reverse Proxy?

Without a reverse proxy, each project must use a different port (Project 1 on 3001, Project 2 on 3002, etc.), and users would access them as:

- http://yourserver.com:3001 (ugly and confusing)
- http://yourserver.com:3002

With a reverse proxy:

- https://project1.com → routes to Project 1
- https://project2.com → routes to Project 2
- https://api.example.com → routes to Project 3

The proxy listens on standard ports (80 and 443) and routes traffic internally. Users never see port numbers.

Additional benefits:

- **SSL termination**: Handle HTTPS in one place instead of configuring SSL for each project
- **Load balancing**: Distribute traffic across multiple instances of the same service
- **Caching**: Serve static files and cached responses without hitting the application
- **Security**: Hide internal architecture, implement rate limiting, block malicious requests
- **Logging**: Centralized access logs for all projects

### Basic Nginx Reverse Proxy Setup

Create a dedicated nginx-proxy project:

```yaml
# nginx-proxy/docker-compose.yml
version: '3.8'

services:
  nginx:
    image: nginx:alpine
    container_name: nginx_proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      # Main Nginx configuration
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      # Individual site configurations
      - ./conf.d:/etc/nginx/conf.d:ro
      # SSL certificates
      - /etc/letsencrypt:/etc/letsencrypt:ro
      # Webroot for Let's Encrypt
      - /var/www/certbot:/var/www/certbot:ro
      # Logs
      - nginx_logs:/var/log/nginx
    networks:
      - proxy_network
    depends_on:
      - dockergen

volumes:
  nginx_logs:

networks:
  proxy_network:
    external: true
```

### Main Nginx Configuration

```nginx
# nginx-proxy/nginx.conf
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 2048;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging format
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;

    # Performance optimizations
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 50M;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css text/xml text/javascript 
               application/json application/javascript application/xml+rss;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;

    # Include site-specific configurations
    include /etc/nginx/conf.d/*.conf;
}
```

This main config sets global settings. The actual site configurations go in `/etc/nginx/conf.d/`.

### Site-Specific Configurations

Create one configuration file per project:

```nginx
# nginx-proxy/conf.d/project1.conf

# Upstream defines the backend servers
upstream project1_backend {
    # This should match the container name or service name
    # from project1's docker-compose.yml
    server project1_app:3000;
    
    # If you have multiple instances (scaled), add them:
    # server project1_app_1:3000;
    # server project1_app_2:3000;
    # server project1_app_3:3000;
}

# HTTP server (redirect to HTTPS)
server {
    listen 80;
    server_name project1.com www.project1.com;

    # Let's Encrypt challenge
    location /.well-known/acme-challenge/ {
        root /var/www/certbot;
    }

    # Redirect to HTTPS
    location / {
        return 301 https://$host$request_uri;
    }
}

# HTTPS server
server {
    listen 443 ssl http2;
    server_name project1.com www.project1.com;

    # SSL certificates
    ssl_certificate /etc/letsencrypt/live/project1.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/project1.com/privkey.pem;

    # SSL configuration
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;
    ssl_session_cache shared:SSL:10m;
    ssl_session_timeout 10m;

    # Security headers
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;

    # Logs for this project
    access_log /var/log/nginx/project1_access.log;
    error_log /var/log/nginx/project1_error.log;

    # Proxy to backend
    location / {
        proxy_pass http://project1_backend;
        proxy_http_version 1.1;

        # WebSocket support
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";

        # Pass along client information
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Forwarded-Port $server_port;

        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;

        # Buffering
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
        proxy_busy_buffers_size 8k;
    }

    # Serve static files directly (optional optimization)
    location /static/ {
        alias /var/www/project1/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

The key directive is `proxy_pass http://project1_backend;` which forwards requests to the upstream defined at the top. The `proxy_set_header` directives ensure your application receives correct client information despite sitting behind a proxy.

### Domain-Based Routing

The above configuration routes based on domain names. Nginx checks the `server_name` directive and routes accordingly:

- Request to `project1.com` → proxies to `project1_backend`
- Request to `project2.com` → proxies to `project2_backend` (if configured)

### Path-Based Routing

Alternatively, route based on URL paths if all projects share one domain:

```nginx
# nginx-proxy/conf.d/multi-project.conf

server {
    listen 443 ssl http2;
    server_name example.com www.example.com;

    ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;

    # Project 1 - accessible at example.com/app1/
    location /app1/ {
        proxy_pass http://project1_app:3000/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Project 2 - accessible at example.com/app2/
    location /app2/ {
        proxy_pass http://project2_app:3000/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # API - accessible at example.com/api/
    location /api/ {
        proxy_pass http://project3_api:8080/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    # Default location
    location / {
        return 404;
    }
}
```

Now:

- `https://example.com/app1/users` → routes to Project 1
- `https://example.com/app2/dashboard` → routes to Project 2
- `https://example.com/api/data` → routes to Project 3 API

### Load Balancing

If you scale a service to multiple instances, Nginx can load balance between them:

```yaml
# project1/docker-compose.yml
services:
  app:
    build: ./app
    networks:
      - project1_internal
      - proxy_network
    deploy:
      replicas: 3  # Creates 3 instances
```

Then in Nginx config:

```nginx
upstream project1_backend {
    # Round-robin by default (equal distribution)
    server project1_app_1:3000;
    server project1_app_2:3000;
    server project1_app_3:3000;
    
    # Or use least connections algorithm
    # least_conn;
    
    # Or use IP hash (same client always goes to same server)
    # ip_hash;
}
```

Nginx will automatically distribute requests across all three instances.

### Nginx Auto-Configuration with Docker-Gen

Manually creating Nginx configs for every project is tedious. Use `docker-gen` to automatically generate configurations based on running containers.

Docker-gen watches the Docker socket and generates files from templates whenever containers start/stop:

```yaml
# nginx-proxy/docker-compose.yml (updated)
services:
  nginx:
    image: nginx:alpine
    container_name: nginx_proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - nginx_conf:/etc/nginx/conf.d  # Note: not read-only
      - /etc/letsencrypt:/etc/letsencrypt:ro
      - /var/www/certbot:/var/www/certbot:ro
    networks:
      - proxy_network

  dockergen:
    image: nginxproxy/docker-gen:latest
    container_name: nginx_dockergen
    restart: unless-stopped
    command: -notify-sighup nginx_proxy -watch /etc/docker-gen/templates/nginx.tmpl /etc/nginx/conf.d/default.conf
    volumes:
      - nginx_conf:/etc/nginx/conf.d
      - ./nginx.tmpl:/etc/docker-gen/templates/nginx.tmpl:ro
      - /var/run/docker.sock:/tmp/docker.sock:ro
    networks:
      - proxy_network

volumes:
  nginx_conf:

networks:
  proxy_network:
    external: true
```

Then in each project, add labels to enable auto-configuration:

```yaml
# project1/docker-compose.yml
services:
  app:
    build: ./app
    networks:
      - project1_internal
      - proxy_network
    labels:
      - "nginx.vhost=project1.com"
      - "nginx.port=3000"
```

Docker-gen reads these labels and generates Nginx configuration automatically!

---

## 9. Complete Real-World Example {#real-world-example}

Let's put everything together with a complete, production-ready example of three projects running on one server:

**Server Structure**:

```
/opt/docker-projects/
├── nginx-proxy/
│   ├── docker-compose.yml
│   ├── nginx.conf
│   └── conf.d/
│       ├── ecommerce.conf
│       ├── blog.conf
│       └── analytics.conf
├── ecommerce/
│   ├── docker-compose.yml
│   ├── .env
│   ├── app/
│   │   ├── Dockerfile
│   │   └── (Node.js application)
│   └── data/
├── blog/
│   ├── docker-compose.yml
│   ├── .env
│   ├── wordpress/
│   └── data/
└── analytics/
    ├── docker-compose.yml
    ├── .env
    ├── api/
    │   ├── Dockerfile
    │   └── (Go application)
    └── data/
```

### Project 1: E-commerce (Node.js + MongoDB + Redis)

```yaml
# ecommerce/docker-compose.yml
version: '3.8'

services:
  app:
    build: ./app
    container_name: ecommerce_app
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - MONGO_URL=mongodb://${MONGO_USER}:${MONGO_PASS}@mongo:27017/${MONGO_DB}
      - REDIS_URL=redis://redis:6379
      - JWT_SECRET=${JWT_SECRET}
      - STRIPE_SECRET=${STRIPE_SECRET}
    depends_on:
      mongo:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - ecommerce_internal
      - proxy_network
    volumes:
      - app_uploads:/app/uploads
    labels:
      - "nginx.vhost=shop.example.com"
      - "nginx.port=3000"

  mongo:
    image: mongo:6.0
    container_name: ecommerce_mongo
    restart: unless-stopped
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASS}
      MONGO_INITDB_DATABASE: ${MONGO_DB}
    volumes:
      - mongo_data:/data/db
      - ./mongo/init.js:/docker-entrypoint-initdb.d/init.js:ro
    networks:
      - ecommerce_internal
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 5s
      retries: 3

  redis:
    image: redis:7-alpine
    container_name: ecommerce_redis
    restart: unless-stopped
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    networks:
      - ecommerce_internal
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

volumes:
  mongo_data:
  redis_data:
  app_uploads:

networks:
  ecommerce_internal:
  proxy_network:
    external: true
```

```bash
# ecommerce/.env
COMPOSE_PROJECT_NAME=ecommerce
MONGO_USER=ecommerce_user
MONGO_PASS=secure_mongo_password_here
MONGO_DB=ecommerce_db
REDIS_PASSWORD=secure_redis_password
JWT_SECRET=jwt_secret_key_here
STRIPE_SECRET=sk_live_stripe_key_here
```

### Project 2: Blog (WordPress + MySQL)

```yaml
# blog/docker-compose.yml
version: '3.8'

services:
  wordpress:
    image: wordpress:latest
    container_name: blog_wordpress
    restart: unless-stopped
    environment:
      WORDPRESS_DB_HOST: mysql
      WORDPRESS_DB_USER: ${DB_USER}
      WORDPRESS_DB_PASSWORD: ${DB_PASSWORD}
      WORDPRESS_DB_NAME: ${DB_NAME}
    volumes:
      - wordpress_data:/var/www/html
    depends_on:
      mysql:
        condition: service_healthy
    networks:
      - blog_internal
      - proxy_network
    labels:
      - "nginx.vhost=blog.example.com"
      - "nginx.port=80"

  mysql:
    image: mysql:8.0
    container_name: blog_mysql
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${DB_NAME}
      MYSQL_USER: ${DB_USER}
      MYSQL_PASSWORD: ${DB_PASSWORD}
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - blog_internal
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  wordpress_data:
  mysql_data:

networks:
  blog_internal:
  proxy_network:
    external: true
```

```bash
# blog/.env
COMPOSE_PROJECT_NAME=blog
MYSQL_ROOT_PASSWORD=super_secure_root_password
DB_NAME=wordpress
DB_USER=wp_user
DB_PASSWORD=secure_wp_password
```

### Project 3: Analytics API (Go + PostgreSQL + RabbitMQ)

```yaml
# analytics/docker-compose.yml
version: '3.8'

services:
  api:
    build: ./api
    container_name: analytics_api
    restart: unless-stopped
    environment:
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      - RABBITMQ_URL=amqp://${RABBITMQ_USER}:${RABBITMQ_PASS}@rabbitmq:5672/
    depends_on:
      postgres:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - analytics_internal
      - proxy_network
    labels:
      - "nginx.vhost=api.example.com"
      - "nginx.port=8080"

  postgres:
    image: postgres:15-alpine
    container_name: analytics_postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - analytics_internal
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: analytics_rabbitmq
    restart: unless-stopped
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASS}
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    networks:
      - analytics_internal
    healthcheck:
      test: rabbitmq-diagnostics -q ping
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
  rabbitmq_data:

networks:
  analytics_internal:
  proxy_network:
    external: true
```

```bash
# analytics/.env
COMPOSE_PROJECT_NAME=analytics
DB_USER=analytics_user
DB_PASSWORD=secure_postgres_password
DB_NAME=analytics_db
RABBITMQ_USER=analytics_mq
RABBITMQ_PASS=secure_rabbitmq_password
```

### Nginx Proxy Configuration

```yaml
# nginx-proxy/docker-compose.yml
version: '3.8'

services:
  nginx:
    image: nginx:alpine
    container_name: nginx_proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./conf.d:/etc/nginx/conf.d:ro
      - /etc/letsencrypt:/etc/letsencrypt:ro
      - /var/www/certbot:/var/www/certbot:ro
      - nginx_logs:/var/log/nginx
    networks:
      - proxy_network

volumes:
  nginx_logs:

networks:
  proxy_network:
    name: proxy_network
    driver: bridge
```

```nginx
# nginx-proxy/conf.d/ecommerce.conf
upstream ecommerce_backend {
    server ecommerce_app:3000;
}

server {
    listen 80;
    server_name shop.example.com;
    location /.well-known/acme-challenge/ { root /var/www/certbot; }
    location / { return 301 https://$host$request_uri; }
}

server {
    listen 443 ssl http2;
    server_name shop.example.com;

    ssl_certificate /etc/letsencrypt/live/shop.example.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/shop.example.com/privkey.pem;

    location / {
        proxy_pass http://ecommerce_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

Similar configurations for blog.conf and analytics.conf.

### Deployment Steps

```bash
# 1. Create shared network
docker network create proxy_network

# 2. Start Nginx proxy
cd /opt/docker-projects/nginx-proxy
docker-compose up -d

# 3. Start all projects
cd /opt/docker-projects/ecommerce
docker-compose up -d

cd /opt/docker-projects/blog
docker-compose up -d

cd /opt/docker-projects/analytics
docker-compose up -d

# 4. Verify everything is running
docker ps

# 5. Check logs if there are issues
docker-compose logs -f app  # (in each project directory)
```

---

## 10. Best Practices and Production Tips {#best-practices}

### Security Hardening

**1. Never run containers as root**:

```dockerfile
# In your Dockerfile
FROM node:18-alpine
RUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001
USER nodejs
WORKDIR /app
COPY --chown=nodejs:nodejs . .
CMD ["node", "server.js"]
```

**2. Use secrets management for passwords**:

Don't put passwords in `.env` files in production. Use Docker secrets or environment-specific secret managers:

```yaml
services:
  app:
    environment:
      - DB_PASSWORD_FILE=/run/secrets/db_password
    secrets:
      - db_password

secrets:
  db_password:
    file: ./secrets/db_password.txt
```

**3. Limit container resources**:

```yaml
services:
  app:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
```

**4. Use read-only root filesystems where possible**:

```yaml
services:
  app:
    read_only: true
    tmpfs:
      - /tmp
      - /var/run
```

### Monitoring and Logging

**Centralized logging with Loki or ELK**:

```yaml
services:
  app:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "project,service"
```

**Container monitoring with Prometheus + Grafana**:

```yaml
# monitoring/docker-compose.yml
services:
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - monitoring

  grafana:
    image: grafana/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=secure_password
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - monitoring
      - proxy_network
```

### Backup Strategy

Automate regular backups:

```bash
#!/bin/bash
# /opt/scripts/backup-all.sh

BACKUP_ROOT="/opt/backups"
DATE=$(date +%Y%m%d_%H%M%S)

# Backup each project's databases
for project in ecommerce blog analytics; do
    echo "Backing up $project..."
    cd /opt/docker-projects/$project
    docker-compose exec -T db_container backup_command > ${BACKUP_ROOT}/${project}_${DATE}.sql
done

# Upload to S3 or remote storage
aws s3 sync ${BACKUP_ROOT} s3://your-backup-bucket/docker-backups/

# Clean old local backups
find ${BACKUP_ROOT} -name "*.sql" -mtime +7 -delete
```

### Update Strategy

Zero-downtime updates:

```bash
#!/bin/bash
# update-project.sh

PROJECT=$1
cd /opt/docker-projects/${PROJECT}

# Pull latest code
git pull

# Rebuild images
docker-compose build

# Rolling update (if multiple replicas)
docker-compose up -d --no-deps --scale app=2 app
sleep 30
docker-compose up -d --no-deps --scale app=1 app

# Or simple restart for single instance
# docker-compose up -d --build --force-recreate
```

### Performance Tuning

**Optimize Docker images**:

```dockerfile
# Multi-stage build for smaller images
FROM node:18-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

FROM node:18-alpine
WORKDIR /app
COPY --from=builder /app/node_modules ./node_modules
COPY . .
USER node
CMD ["node", "server.js"]
```

**Use BuildKit for faster builds**:

```bash
export DOCKER_BUILDKIT=1
docker-compose build
```

**Enable Docker's experimental features for better caching**:

```json
// /etc/docker/daemon.json
{
  "experimental": true,
  "features": {
    "buildkit": true
  }
}
```

### Conclusion

Managing multiple Docker projects on a single server is powerful but requires discipline. Key takeaways:

1. **Isolate projects** with separate networks and compose files
2. **Systematize port allocation** to avoid conflicts
3. **Use Nginx as a reverse proxy** for routing and SSL termination
4. **Separate database containers per project** for security and flexibility
5. **Automate backups, monitoring, and updates**
6. **Document everything** in a central repository

With this setup, you can run dozens of projects on one server, each completely isolated yet sharing infrastructure efficiently. The initial setup takes time, but the long-term maintainability and scalability make it worthwhile.