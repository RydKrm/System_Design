# Complete Database & Migration Management Guide with Docker

## Table of Contents

1. [Understanding Database Management with Docker](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#understanding)
2. [Database Architecture in Docker](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#architecture)
3. [Accessing Production Databases Safely](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#accessing-production)
4. [Database Backup Strategies](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#backup-strategies)
5. [Database Restore Procedures](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#restore-procedures)
6. [Migration Management](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#migration-management)
7. [Development Database Workflows](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#development-workflows)
8. [Production Database Operations](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#production-operations)
9. [Database Replication and High Availability](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#replication)
10. [Monitoring and Maintenance](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#monitoring)
11. [Security Best Practices](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#security)
12. [Complete Working Examples](https://claude.ai/chat/aefcc9c9-f1e1-41de-9883-ed6b27dff2da#examples)

---

## 1. Understanding Database Management with Docker {#understanding}

When your databases run in Docker containers, database management becomes both simpler and more complex. Let's understand what changes and what stays the same.

### Traditional vs Docker Database Management

**Traditional Database Server (Without Docker)**:

```
┌────────────────────────────────────────────────────────────┐
│                    Physical Server                         │
│                                                            │
│  ┌────────────────────────────────────────────────────┐    │
│  │  PostgreSQL installed directly on OS               │    │
│  │  - Config: /etc/postgresql/postgresql.conf         │    │
│  │  - Data: /var/lib/postgresql/data                  │    │
│  │  - Logs: /var/log/postgresql                       │    │
│  │  - Access: psql -h localhost -U postgres           │    │
│  │  - Backup: pg_dump > backup.sql                    │    │
│  └────────────────────────────────────────────────────┘    │
│                                                            │
│  Developers need:                                          │
│  - SSH access to server                                    │
│  - Database credentials                                    │
│  - Direct network access                                   │
└────────────────────────────────────────────────────────────┘
```

**Docker Database (Current Setup)**:

```
┌─────────────────────────────────────────────────────────────┐
│                    Docker Host Server                       │
│                                                             │
│  ┌────────────────────────────────────────────────────┐     │
│  │  PostgreSQL Container (project1_postgres)          │     │
│  │                                                    │     │
│  │  Inside Container:                                 │     │
│  │  - Config: /etc/postgresql/postgresql.conf         │    │
│  │  - Data: /var/lib/postgresql/data                  │    │
│  │          (mounted from volume)                     │    │
│  │  - Port: 5432 (internal)                           │    │
│  │                                                    │    │
│  │  ┌──────────────────────────────────────────┐     │    │
│  │  │   Docker Volume                          │     │    │
│  │  │   /var/lib/docker/volumes/               │     │    │
│  │  │   project1_postgres_data/_data/          │     │    │
│  │  │                                          │     │    │
│  │  │   This is where actual data persists     │     │    │
│  │  └──────────────────────────────────────────┘     │    │
│  └───────────────────────────────────────────────────┘    │
│                                                           │
│  Access Methods:                                          │
│  1. docker exec -it project1_postgres psql ...            │
│  2. SSH tunnel: localhost:15432 → container:5432          │
│  3. Exposed port (if configured): host:5432 → container   │
└───────────────────────────────────────────────────────────┘
```

### Key Differences

**What's Different**:

1. **Access**: Can't connect directly to localhost:5432 unless port is exposed
2. **Backup**: Must use `docker exec` or access volumes directly
3. **Commands**: Prefix everything with `docker exec`
4. **Data Location**: Data is in Docker volumes, not standard OS locations
5. **Networking**: Database lives on Docker network, isolated by default

**What's the Same**:

1. **SQL Commands**: All PostgreSQL/MySQL/MongoDB commands work identically
2. **Data Format**: Database files are standard formats
3. **Backup Files**: Dump files are identical to non-Docker backups
4. **Restore Process**: Same SQL commands, just executed through Docker

---

## 2. Database Architecture in Docker {#architecture}

Understanding how databases are structured in your Docker setup is crucial for management.

### Complete Database Stack Visualization

```
┌──────────────────────────────────────────────────────────────────────────┐
│                         Docker Host Server                               │
│                                                                          │
│  ┌─────────────────────────────────────────────────────────────────┐     │
│  │                        Project 1                                │    │
│  │                                                                 │   │
│  │  ┌──────────────┐        ┌──────────────┐                       │   │
│  │  │   Backend    │◄──────►│ PostgreSQL   │                       │   │
│  │  │   (Node.js)  │        │  Container   │                       │   │
│  │  │              │        │              │                       │   │
│  │  │ Port: 5000   │        │ Port: 5432   │                       │   │
│  │  └──────────────┘        └──────┬───────┘                       │   │
│  │         │                       │                               │   │
│  │         │                       │                               │   │
│  │         │                ┌──────▼───────────────────────────┐   │   │
│  │         │                │   Docker Volume                  │   │   │
│  │         │                │   project1_postgres_data         │   │   │
│  │         │                │                                  │   │   │
│  │         │                │   /var/lib/docker/volumes/       │   │   │
│  │         │                │   project1_postgres_data/_data   │   │   │
│  │         │                │                                  │   │   │
│  │         │                │   ├── base/        (tables)      │   │   │
│  │         │                │   ├── global/      (users)       │   │   │
│  │         │                │   ├── pg_wal/      (logs)        │   │   │
│  │         │                │   └── postgresql.conf            │   │   │
│  │         │                └──────────────────────────────────┘   │   │
│  │         │                                                       │   │
│  │         └──────────────► project1_network                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                        │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                        Project 2                                │   │
│  │                                                                 │   │
│  │  ┌──────────────┐        ┌──────────────┐                       │   │
│  │  │   Backend    │◄──────►│ PostgreSQL   │                       │   │
│  │  │   (Golang)   │        │  Container   │                       │   │
│  │  │              │        │              │                       │   │
│  │  │ Port: 8080   │        │ Port: 5432   │                       │   │
│  │  └──────────────┘        └──────┬───────┘                       │   │
│  │         │                       │                               │   │
│  │         │                ┌──────▼───────────────────────────┐   │   │
│  │         │                │   Docker Volume                  │   │   │
│  │         │                │   project2_postgres_data         │   │   │
│  │         │                └──────────────────────────────────┘   │   │
│  │         │                                                       │   │
│  │         └──────────────► project2_network                       │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                        │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                    Management Access                            │   │
│  │                                                                 │   │
│  │  Developer's Machine                                            │   │
│  │         │                                                       │   │
│  │         ├─ SSH Tunnel ──────────► Access any database           │   │
│  │         ├─ docker exec ──────────► Run SQL commands             │   │
│  │         └─ Backup Scripts ───────► Download backups             │   │
│  └─────────────────────────────────────────────────────────────────┘   │
└────────────────────────────────────────────────────────────────────────┘
```

### Data Persistence

Docker containers are ephemeral (temporary), but databases need permanent storage. That's why we use volumes:

```bash
# Without volumes (DATA LOSS!)
docker run postgres:15
# Container stores data internally
# When container stops: DATA IS DELETED ❌

# With volumes (PERSISTENT DATA)
docker run -v postgres_data:/var/lib/postgresql/data postgres:15
# Data stored in Docker volume
# Container can stop, restart, or be deleted
# Data remains safe ✓
```

**Volume Locations**:

```bash
# Find volume location
docker volume inspect project1_postgres_data

# Output shows:
# "Mountpoint": "/var/lib/docker/volumes/project1_postgres_data/_data"

# List all volumes
docker volume ls

# See volume size
docker system df -v
```

---

## 3. Accessing Production Databases Safely {#accessing-production}

Developers often need to access production databases for debugging, data analysis, or emergency fixes. Let's explore safe methods.

### Method 1: Docker Exec (Direct Container Access)

This is the most direct method—execute commands inside the running database container.

**PostgreSQL Access**:

```bash
# SSH into your production server first
ssh deploy@your-server-ip

# Method A: Interactive psql session
docker exec -it project1_postgres psql -U project1_user -d project1_database

# You're now in psql:
project1_database=# \dt              # List tables
project1_database=# \d users         # Describe users table
project1_database=# SELECT * FROM users LIMIT 10;
project1_database=# \q              # Exit

# Method B: Run single query
docker exec -it project1_postgres psql -U project1_user -d project1_database -c "SELECT COUNT(*) FROM users;"

# Method C: Run SQL file
docker exec -i project1_postgres psql -U project1_user -d project1_database < query.sql

# Method D: Get query output to file
docker exec project1_postgres psql -U project1_user -d project1_database -c "SELECT * FROM orders WHERE created_at > NOW() - INTERVAL '7 days';" > recent_orders.csv
```

**MongoDB Access**:

```bash
# Interactive mongosh session
docker exec -it project1_mongo mongosh -u admin -p password project1_db

# MongoDB commands:
> show collections
> db.users.find().limit(10)
> db.users.countDocuments()
> exit

# Run single command
docker exec project1_mongo mongosh -u admin -p password project1_db --eval "db.users.countDocuments()"

# Run JavaScript file
docker exec -i project1_mongo mongosh -u admin -p password project1_db < script.js
```

**MySQL Access**:

```bash
# Interactive mysql session
docker exec -it project3_mysql mysql -u project3_user -p project3_database

# MySQL commands:
mysql> SHOW TABLES;
mysql> DESCRIBE users;
mysql> SELECT * FROM users LIMIT 10;
mysql> EXIT;

# Run single query
docker exec project3_mysql mysql -u project3_user -ppassword project3_database -e "SELECT COUNT(*) FROM users;"
```

**Redis Access**:

```bash
# Interactive redis-cli
docker exec -it project1_redis redis-cli -a your_redis_password

# Redis commands:
127.0.0.1:6379> KEYS *
127.0.0.1:6379> GET user:123
127.0.0.1:6379> TTL session:abc
127.0.0.1:6379> EXIT

# Run single command
docker exec project1_redis redis-cli -a your_redis_password GET user:123
```

### Method 2: SSH Tunnel (Secure Remote Access)

SSH tunnels allow you to access the database from your local machine as if it were running locally—very useful for GUI tools.

**Setting Up SSH Tunnel**:

```bash
# General syntax:
# ssh -L local_port:container_name:container_port user@server

# PostgreSQL tunnel
ssh -L 15432:project1_postgres:5432 deploy@your-server-ip

# Now on your local machine:
psql -h localhost -p 15432 -U project1_user -d project1_database

# Or use GUI tools:
# Host: localhost
# Port: 15432
# User: project1_user
# Password: [your password]
# Database: project1_database
```

**Multiple Tunnels at Once**:

```bash
# Create tunnels for multiple projects
ssh -L 15432:project1_postgres:5432 \
    -L 25432:project2_postgres:5432 \
    -L 16379:project1_redis:6379 \
    deploy@your-server-ip

# Keep terminal open, use databases from other terminal
```

**Persistent Tunnel (Background)**:

```bash
# Run tunnel in background
ssh -f -N -L 15432:project1_postgres:5432 deploy@your-server-ip

# -f: Goes to background
# -N: Don't execute remote commands
# Tunnel runs until you kill the SSH process

# Find and kill background tunnel
ps aux | grep "ssh.*15432"
kill [process_id]
```

**Create Tunnel Alias (Convenience)**:

```bash
# Add to ~/.bashrc or ~/.zshrc
alias db-project1="ssh -L 15432:project1_postgres:5432 deploy@your-server-ip"
alias db-project2="ssh -L 25432:project2_postgres:5432 deploy@your-server-ip"

# Usage:
db-project1
# Tunnel is now active, connect from another terminal
```

### Method 3: Expose Database Port (NOT RECOMMENDED for Production)

**Why Not Recommended?**:

- Exposes database directly to internet
- Security risk even with firewall
- Can be accidentally left open
- Makes database a target for attacks

**If You Must (Development Only)**:

```yaml
# docker-compose.yml (DEVELOPMENT ONLY!)
services:
  postgres:
    image: postgres:15-alpine
    ports:
      - "5432:5432"  # Exposes to host machine
    # DON'T DO THIS IN PRODUCTION
```

**Better Alternative with Firewall**:

```yaml
# Expose on specific IP only
services:
  postgres:
    ports:
      - "127.0.0.1:5432:5432"  # Only accessible from localhost
```

### Method 4: Database GUI Tools with SSH Tunnel

Many GUI tools support SSH tunneling built-in.

**DBeaver Configuration**:

```
1. Connection Settings:
   - Host: localhost
   - Port: 5432
   - Database: project1_database
   - Username: project1_user
   - Password: [password]

2. SSH Tab:
   ✓ Use SSH Tunnel
   - Host: your-server-ip
   - Port: 22
   - Username: deploy
   - Auth: Public Key
   - Private Key: /path/to/github_actions (SSH key)
```

**pgAdmin Configuration**:

```
1. Create SSH Tunnel manually:
   ssh -L 15432:project1_postgres:5432 deploy@your-server-ip

2. In pgAdmin:
   - Host: localhost
   - Port: 15432
   - Database: project1_database
```

**TablePlus / DataGrip / Sequel Pro**:

All support SSH tunneling natively. Look for "SSH" or "SSL/SSH" tab in connection settings.

### Method 5: Read-Only Access for Developers

Create read-only database users for safe production access.

**PostgreSQL Read-Only User**:

```sql
-- Connect as admin
docker exec -it project1_postgres psql -U project1_user -d project1_database

-- Create read-only role
CREATE ROLE readonly_user WITH LOGIN PASSWORD 'readonly_password';

-- Grant connect to database
GRANT CONNECT ON DATABASE project1_database TO readonly_user;

-- Grant usage on schema
GRANT USAGE ON SCHEMA public TO readonly_user;

-- Grant select on all existing tables
GRANT SELECT ON ALL TABLES IN SCHEMA public TO readonly_user;

-- Grant select on future tables
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT SELECT ON TABLES TO readonly_user;

-- Test read-only access
docker exec -it project1_postgres psql -U readonly_user -d project1_database

-- This should work:
SELECT * FROM users LIMIT 10;

-- This should fail:
DELETE FROM users WHERE id = 1;
-- ERROR: permission denied for table users
```

**MongoDB Read-Only User**:

```javascript
// Connect as admin
docker exec -it project1_mongo mongosh -u admin -p adminpass

// Switch to database
use project1_db

// Create read-only user
db.createUser({
  user: "readonly_user",
  pwd: "readonly_password",
  roles: [
    { role: "read", db: "project1_db" }
  ]
})

// Test
docker exec -it project1_mongo mongosh -u readonly_user -p readonly_password project1_db

// This works:
db.users.find().limit(10)

// This fails:
db.users.deleteOne({ _id: ObjectId("...") })
// Error: not authorized
```

---

## 4. Database Backup Strategies {#backup-strategies}

Regular backups are critical. If something goes wrong, backups are your safety net.

### Backup Strategy Overview

```
┌─────────────────────────────────────────────────────────────┐
│                   Backup Strategy Pyramid                   │
│                                                             │
│              ┌──────────────────────────┐                   │
│              │   Automated Daily        │                   │
│              │   Full Backups           │                   │
│              │   (Keep 7 days)          │                   │
│              └──────────────────────────┘                   │
│                       │                                     │
│              ┌────────▼─────────────────────┐               │
│              │   Automated Weekly           │               │
│              │   Full Backups               │               │
│              │   (Keep 4 weeks)             │               │
│              └──────────────────────────────┘               │
│                       │                                     │
│              ┌────────▼─────────────────────────┐           │
│              │   Monthly Backups                │           │
│              │   Archived Long-term             │           │
│              │   (Keep 12 months)               │           │
│              └──────────────────────────────────┘           │
│                       │                                     │
│              ┌────────▼─────────────────────────┐           │
│              │   Off-site Backups               │           │
│              │   (S3, Cloud Storage)            │           │
│              │   Keep indefinitely              │           │
│              └──────────────────────────────────┘           │
└─────────────────────────────────────────────────────────────┘
```

### PostgreSQL Backup Scripts

**Complete PostgreSQL Backup Script**:

```bash
#!/bin/bash
# /home/deploy/scripts/backup-postgres.sh

set -e  # Exit on error

# Configuration
PROJECT="project1"
CONTAINER="${PROJECT}_postgres"
DB_USER="project1_user"
DB_NAME="project1_database"
BACKUP_DIR="/home/deploy/backups/${PROJECT}/postgres"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
DATE=$(date +%Y%m%d)

# Retention settings
DAILY_RETENTION=7      # Keep 7 days of daily backups
WEEKLY_RETENTION=28    # Keep 4 weeks of weekly backups
MONTHLY_RETENTION=365  # Keep 12 months of monthly backups

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

log() { echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1"; }
warn() { echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING:${NC} $1"; }
error() { echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ERROR:${NC} $1"; }

# Create backup directories
mkdir -p "${BACKUP_DIR}/daily"
mkdir -p "${BACKUP_DIR}/weekly"
mkdir -p "${BACKUP_DIR}/monthly"

log "Starting PostgreSQL backup for ${PROJECT}"

# Check if container is running
if ! docker ps --format '{{.Names}}' | grep -q "^${CONTAINER}$"; then
    error "Container ${CONTAINER} is not running!"
    exit 1
fi

# Check database connectivity
if ! docker exec ${CONTAINER} pg_isready -U ${DB_USER} > /dev/null 2>&1; then
    error "Database is not ready!"
    exit 1
fi

# Create daily backup
log "Creating daily backup..."
DAILY_BACKUP="${BACKUP_DIR}/daily/${PROJECT}_${TIMESTAMP}.sql"

docker exec ${CONTAINER} pg_dump -U ${DB_USER} -d ${DB_NAME} \
    --format=plain \
    --no-owner \
    --no-acl \
    --compress=9 \
    > "${DAILY_BACKUP}"

if [ $? -eq 0 ]; then
    # Compress backup
    gzip "${DAILY_BACKUP}"
    DAILY_BACKUP="${DAILY_BACKUP}.gz"
    
    BACKUP_SIZE=$(du -h "${DAILY_BACKUP}" | cut -f1)
    log "✓ Daily backup created: ${DAILY_BACKUP} (${BACKUP_SIZE})"
else
    error "Daily backup failed!"
    exit 1
fi

# Create weekly backup (every Sunday)
DAY_OF_WEEK=$(date +%u)
if [ "${DAY_OF_WEEK}" -eq 7 ]; then
    log "Creating weekly backup..."
    cp "${DAILY_BACKUP}" "${BACKUP_DIR}/weekly/${PROJECT}_weekly_${DATE}.sql.gz"
    log "✓ Weekly backup created"
fi

# Create monthly backup (first day of month)
DAY_OF_MONTH=$(date +%d)
if [ "${DAY_OF_MONTH}" -eq 01 ]; then
    log "Creating monthly backup..."
    cp "${DAILY_BACKUP}" "${BACKUP_DIR}/monthly/${PROJECT}_monthly_${DATE}.sql.gz"
    log "✓ Monthly backup created"
fi

# Cleanup old backups
log "Cleaning up old backups..."

# Remove daily backups older than retention period
find "${BACKUP_DIR}/daily" -name "*.sql.gz" -mtime +${DAILY_RETENTION} -delete
log "✓ Removed daily backups older than ${DAILY_RETENTION} days"

# Remove weekly backups older than retention period
find "${BACKUP_DIR}/weekly" -name "*.sql.gz" -mtime +${WEEKLY_RETENTION} -delete
log "✓ Removed weekly backups older than ${WEEKLY_RETENTION} days"

# Remove monthly backups older than retention period
find "${BACKUP_DIR}/monthly" -name "*.sql.gz" -mtime +${MONTHLY_RETENTION} -delete
log "✓ Removed monthly backups older than ${MONTHLY_RETENTION} days"

# Backup statistics
DAILY_COUNT=$(find "${BACKUP_DIR}/daily" -name "*.sql.gz" | wc -l)
WEEKLY_COUNT=$(find "${BACKUP_DIR}/weekly" -name "*.sql.gz" | wc -l)
MONTHLY_COUNT=$(find "${BACKUP_DIR}/monthly" -name "*.sql.gz" | wc -l)
TOTAL_SIZE=$(du -sh "${BACKUP_DIR}" | cut -f1)

log "Backup statistics:"
log "  Daily backups: ${DAILY_COUNT}"
log "  Weekly backups: ${WEEKLY_COUNT}"
log "  Monthly backups: ${MONTHLY_COUNT}"
log "  Total size: ${TOTAL_SIZE}"

# Upload to S3 (optional)
if [ -n "${AWS_S3_BUCKET}" ]; then
    log "Uploading backup to S3..."
    aws s3 cp "${DAILY_BACKUP}" "s3://${AWS_S3_BUCKET}/backups/${PROJECT}/postgres/daily/" \
        --storage-class STANDARD_IA
    
    if [ $? -eq 0 ]; then
        log "✓ Backup uploaded to S3"
    else
        warn "Failed to upload to S3"
    fi
fi

log "PostgreSQL backup completed successfully!"
```

**Make it executable and schedule**:

```bash
# Make executable
chmod +x /home/deploy/scripts/backup-postgres.sh

# Test it
bash /home/deploy/scripts/backup-postgres.sh

# Schedule with cron (daily at 2 AM)
crontab -e

# Add:
0 2 * * * /home/deploy/scripts/backup-postgres.sh >> /home/deploy/logs/backup-postgres.log 2>&1
```

### MongoDB Backup Scripts

```bash
#!/bin/bash
# /home/deploy/scripts/backup-mongodb.sh

set -e

PROJECT="project1"
CONTAINER="${PROJECT}_mongo"
DB_NAME="project1_db"
BACKUP_DIR="/home/deploy/backups/${PROJECT}/mongodb"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# MongoDB credentials (from .env)
source /home/deploy/projects/${PROJECT}/.env

log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"; }

mkdir -p "${BACKUP_DIR}"

log "Starting MongoDB backup for ${PROJECT}"

# Create temporary directory inside container
TEMP_DIR="/tmp/mongodb_backup_${TIMESTAMP}"

# Run mongodump inside container
docker exec ${CONTAINER} mongodump \
    --username=${MONGO_USER} \
    --password=${MONGO_PASSWORD} \
    --authenticationDatabase=admin \
    --db=${DB_NAME} \
    --out=${TEMP_DIR}

# Copy backup from container to host
BACKUP_PATH="${BACKUP_DIR}/${PROJECT}_${TIMESTAMP}"
docker cp ${CONTAINER}:${TEMP_DIR}/${DB_NAME} "${BACKUP_PATH}"

# Compress backup
tar -czf "${BACKUP_PATH}.tar.gz" -C "${BACKUP_DIR}" "$(basename ${BACKUP_PATH})"
rm -rf "${BACKUP_PATH}"

# Clean up inside container
docker exec ${CONTAINER} rm -rf ${TEMP_DIR}

BACKUP_SIZE=$(du -h "${BACKUP_PATH}.tar.gz" | cut -f1)
log "✓ MongoDB backup created: ${BACKUP_PATH}.tar.gz (${BACKUP_SIZE})"

# Cleanup old backups (keep 7 days)
find "${BACKUP_DIR}" -name "*.tar.gz" -mtime +7 -delete

log "MongoDB backup completed!"
```

### MySQL Backup Scripts

```bash
#!/bin/bash
# /home/deploy/scripts/backup-mysql.sh

set -e

PROJECT="project3"
CONTAINER="${PROJECT}_mysql"
DB_NAME="project3_database"
BACKUP_DIR="/home/deploy/backups/${PROJECT}/mysql"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

source /home/deploy/projects/${PROJECT}/.env

mkdir -p "${BACKUP_DIR}"

log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"; }

log "Starting MySQL backup for ${PROJECT}"

BACKUP_FILE="${BACKUP_DIR}/${PROJECT}_${TIMESTAMP}.sql"

# Create backup using mysqldump
docker exec ${CONTAINER} mysqldump \
    -u${MYSQL_USER} \
    -p${MYSQL_PASSWORD} \
    --single-transaction \
    --quick \
    --lock-tables=false \
    ${DB_NAME} > "${BACKUP_FILE}"

# Compress
gzip "${BACKUP_FILE}"

BACKUP_SIZE=$(du -h "${BACKUP_FILE}.gz" | cut -f1)
log "✓ MySQL backup created: ${BACKUP_FILE}.gz (${BACKUP_SIZE})"

# Cleanup old backups
find "${BACKUP_DIR}" -name "*.sql.gz" -mtime +7 -delete

log "MySQL backup completed!"
```

### Redis Backup Scripts

Redis has two persistence methods: RDB (snapshots) and AOF (append-only file).

```bash
#!/bin/bash
# /home/deploy/scripts/backup-redis.sh

set -e

PROJECT="project1"
CONTAINER="${PROJECT}_redis"
BACKUP_DIR="/home/deploy/backups/${PROJECT}/redis"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

source /home/deploy/projects/${PROJECT}/.env

mkdir -p "${BACKUP_DIR}"

log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"; }

log "Starting Redis backup for ${PROJECT}"

# Trigger Redis save
docker exec ${CONTAINER} redis-cli -a ${REDIS_PASSWORD} BGSAVE

# Wait for save to complete
sleep 5

while docker exec ${CONTAINER} redis-cli -a ${REDIS_PASSWORD} LASTSAVE | grep -q "ERR"; do
    log "Waiting for Redis save to complete..."
    sleep 2
done

# Copy RDB file from container
docker cp ${CONTAINER}:/data/dump.rdb "${BACKUP_DIR}/dump_${TIMESTAMP}.rdb"

# Compress
gzip "${BACKUP_DIR}/dump_${TIMESTAMP}.rdb"

log "✓ Redis backup created: ${BACKUP_DIR}/dump_${TIMESTAMP}.rdb.gz"

# Cleanup old backups
find "${BACKUP_DIR}" -name "dump_*.rdb.gz" -mtime +7 -delete

log "Redis backup completed!"
```

### Combined Backup Script (All Databases)

```bash
#!/bin/bash
# /home/deploy/scripts/backup-all-databases.sh

set -e

LOG_FILE="/home/deploy/logs/backup-all-$(date +%Y%m%d).log"

log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a "${LOG_FILE}"
}

log "===== Starting Automated Database Backups ====="

# Backup Project 1 (PostgreSQL + Redis)
log "Backing up Project 1..."
/home/deploy/scripts/backup-postgres.sh project1
/home/deploy/scripts/backup-redis.sh project1

# Backup Project 2 (PostgreSQL)
log "Backing up Project 2..."
/home/deploy/scripts/backup-postgres.sh project2

# Send notification
if [ $? -eq 0 ]; then
    log "✓ All backups completed successfully"
    
    # Optional: Send success notification
    curl -X POST "${SLACK_WEBHOOK_URL}" \
        -H 'Content-Type: application/json' \
        -d "{\"text\":\"✅ Database backups completed successfully\"}"
else
    log "✗ Backup failed!"
    
    # Optional: Send failure notification
    curl -X POST "${SLACK_WEBHOOK_URL}" \
        -H 'Content-Type: application/json' \
        -d "{\"text\":\"❌ Database backup FAILED! Check logs.\"}"
fi

log "===== Backup Process Complete ====="
```

**Schedule all backups**:

```bash
# Edit crontab
crontab -e

# Add these lines:
# Daily backup at 2 AM
0 2 * * * /home/deploy/scripts/backup-all-databases.sh

# Weekly verification backup at 3 AM on Sundays
0 3 * * 0 /home/deploy/scripts/verify-backups.sh
```

### Off-site Backup to AWS S3

```bash
#!/bin/bash
# /home/deploy/scripts/sync-backups-to-s3.sh

set -e

BACKUP_ROOT="/home/deploy/backups"
S3_BUCKET="s3://your-company-backups"
AWS_REGION="us-east-1"

log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"; }

log "Syncing backups to S3..."

# Install AWS CLI if not present
if ! command -v aws &> /dev/null; then
    log "Installing AWS CLI..."
    curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    unzip awscliv2.zip
    sudo ./aws/install
fi

# Configure AWS credentials (do this once)
# aws configure set aws_access_key_id YOUR_ACCESS_KEY
# aws configure set aws_secret_access_key YOUR_SECRET_KEY
# aws configure set default.region ${AWS_REGION}

# Sync backups to S3
aws s3 sync "${BACKUP_ROOT}" "${S3_BUCKET}/$(hostname)/backups/" \
    --storage-class STANDARD_IA \
    --exclude "*.log" \
    --exclude "tmp/*"

if [ $? -eq 0 ]; then
    log "✓ Backups synced to S3 successfully"
else
    log "✗ S3 sync failed!"
    exit 1
fi

# List recent backups
log "Recent backups in S3:"
aws s3 ls "${S3_BUCKET}/$(hostname)/backups/" --recursive --human-readable | tail -10

log "S3 sync completed!"
```

**Schedule S3 sync**:

```bash
# Run daily after backups complete (3 AM)
0 3 * * * /home/deploy/scripts/sync-backups-to-s3.sh >> /home/deploy/logs/s3-sync.log 2>&1
```

---

## 5. Database Restore Procedures {#restore-procedures}

Knowing how to restore backups is just as important as creating them. Let's practice restore procedures.

### PostgreSQL Restore

**Basic Restore from SQL Dump**:

```bash
#!/bin/bash
# /home/deploy/scripts/restore-postgres.sh

# Usage: ./restore-postgres.sh project1 /path/to/backup.sql.gz

PROJECT=$1
BACKUP_FILE=$2

if [ -z "$PROJECT" ] || [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <project_name> <backup_file>"
    exit 1
fi

CONTAINER="${PROJECT}_postgres"
DB_USER="${PROJECT}_user"
DB_NAME="${PROJECT}_database"

log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"; }

log "Starting PostgreSQL restore for ${PROJECT}"

# Verify backup file exists
if [ ! -f "$BACKUP_FILE" ]; then
    echo "Error: Backup file not found: $BACKUP_FILE"
    exit 1
fi

# Warning
echo "⚠️  WARNING: This will OVERWRITE the database: ${DB_NAME}"
read -p "Are you sure you want to continue? (type 'yes' to proceed): " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
    echo "Restore cancelled."
    exit 0
fi

# Create backup of current state before restore
log "Creating backup of current database before restore..."
SAFETY_BACKUP="/tmp/pre-restore-backup-$(date +%Y%m%d_%H%M%S).sql.gz"
docker exec ${CONTAINER} pg_dump -U ${DB_USER} -d ${DB_NAME} | gzip > "${SAFETY_BACKUP}"
log "✓ Safety backup created: ${SAFETY_BACKUP}"

# Drop existing connections
log "Terminating existing connections..."
docker exec ${CONTAINER} psql -U ${DB_USER} -d postgres -c \
    "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '${DB_NAME}' AND pid <> pg_backend_pid();"

# Drop and recreate database
log "Dropping and recreating database..."
docker exec ${CONTAINER} psql -U ${DB_USER} -d postgres -c "DROP DATABASE IF EXISTS ${DB_NAME};"
docker exec ${CONTAINER} psql -U ${DB_USER} -d postgres -c "CREATE DATABASE ${DB_NAME};"

# Restore from backup
log "Restoring from backup..."
if [[ "$BACKUP_FILE" == *.gz ]]; then
    # Compressed backup
    gunzip -c "$BACKUP_FILE" | docker exec -i ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME}
else
    # Uncompressed backup
    docker exec -i ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME} < "$BACKUP_FILE"
fi

if [ $? -eq 0 ]; then
    log "✓ Database restored successfully!"
    
    # Verify restore
    log "Verifying restore..."
    TABLE_COUNT=$(docker exec ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME} -t -c \
        "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='public';")
    log "Tables restored: ${TABLE_COUNT}"
    
    USER_COUNT=$(docker exec ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME} -t -c \
        "SELECT COUNT(*) FROM users;" 2>/dev/null || echo "N/A")
    log "User records: ${USER_COUNT}"
else
    log "✗ Restore failed!"
    log "Rolling back to safety backup..."
    gunzip -c "${SAFETY_BACKUP}" | docker exec -i ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME}
    exit 1
fi

log "Restore completed!"
log "Safety backup is at: ${SAFETY_BACKUP}"
log "Keep this file until you verify the restored database is working correctly."
```

**Partial Restore (Single Table)**:

```bash
# Restore just one table from backup

# Extract single table from backup
pg_restore -U project1_user -d project1_database -t users backup.dump

# Or if using SQL dump:
# Extract table data
grep -A 10000 "CREATE TABLE users" backup.sql | grep -B 10000 "CREATE TABLE next_table" > users_only.sql

# Restore just that table
docker exec -i project1_postgres psql -U project1_user -d project1_database < users_only.sql
```

### MongoDB Restore

```bash
#!/bin/bash
# /home/deploy/scripts/restore-mongodb.sh

PROJECT=$1
BACKUP_FILE=$2

if [ -z "$PROJECT" ] || [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <project_name> <backup_file.tar.gz>"
    exit 1
fi

CONTAINER="${PROJECT}_mongo"
DB_NAME="${PROJECT}_db"

source /home/deploy/projects/${PROJECT}/.env

log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"; }

log "Starting MongoDB restore for ${PROJECT}"

# Verify backup file
if [ ! -f "$BACKUP_FILE" ]; then
    echo "Error: Backup file not found: $BACKUP_FILE"
    exit 1
fi

# Warning
echo "⚠️  WARNING: This will OVERWRITE the database: ${DB_NAME}"
read -p "Type 'yes' to proceed: " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
    echo "Restore cancelled."
    exit 0
fi

# Extract backup
TEMP_DIR="/tmp/mongodb_restore_$$"
mkdir -p "${TEMP_DIR}"
tar -xzf "${BACKUP_FILE}" -C "${TEMP_DIR}"

# Copy to container
docker cp "${TEMP_DIR}" ${CONTAINER}:/tmp/restore

# Restore database
log "Restoring MongoDB..."
docker exec ${CONTAINER} mongorestore \
    --username=${MONGO_USER} \
    --password=${MONGO_PASSWORD} \
    --authenticationDatabase=admin \
    --db=${DB_NAME} \
    --drop \
    /tmp/restore/

# Cleanup
rm -rf "${TEMP_DIR}"
docker exec ${CONTAINER} rm -rf /tmp/restore

if [ $? -eq 0 ]; then
    log "✓ MongoDB restored successfully!"
    
    # Verify
    COLLECTION_COUNT=$(docker exec ${CONTAINER} mongosh -u ${MONGO_USER} -p ${MONGO_PASSWORD} \
        --authenticationDatabase=admin ${DB_NAME} --quiet \
        --eval "db.getCollectionNames().length")
    log "Collections restored: ${COLLECTION_COUNT}"
else
    log "✗ Restore failed!"
    exit 1
fi

log "Restore completed!"
```

### MySQL Restore

```bash
#!/bin/bash
# /home/deploy/scripts/restore-mysql.sh

PROJECT=$1
BACKUP_FILE=$2

CONTAINER="${PROJECT}_mysql"
DB_NAME="${PROJECT}_database"

source /home/deploy/projects/${PROJECT}/.env

log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"; }

log "Starting MySQL restore for ${PROJECT}"

# Warning
echo "⚠️  WARNING: This will OVERWRITE the database: ${DB_NAME}"
read -p "Type 'yes' to proceed: " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
    exit 0
fi

log "Restoring MySQL..."

if [[ "$BACKUP_FILE" == *.gz ]]; then
    gunzip -c "$BACKUP_FILE" | docker exec -i ${CONTAINER} mysql \
        -u${MYSQL_USER} -p${MYSQL_PASSWORD} ${DB_NAME}
else
    docker exec -i ${CONTAINER} mysql \
        -u${MYSQL_USER} -p${MYSQL_PASSWORD} ${DB_NAME} < "$BACKUP_FILE"
fi

if [ $? -eq 0 ]; then
    log "✓ MySQL restored successfully!"
else
    log "✗ Restore failed!"
    exit 1
fi
```

### Redis Restore

```bash
#!/bin/bash
# /home/deploy/scripts/restore-redis.sh

PROJECT=$1
BACKUP_FILE=$2  # .rdb or .rdb.gz file

CONTAINER="${PROJECT}_redis"

log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"; }

log "Starting Redis restore for ${PROJECT}"

# Stop Redis container
log "Stopping Redis..."
docker-compose -f /home/deploy/projects/${PROJECT}/docker-compose.yml stop redis

# Extract if compressed
if [[ "$BACKUP_FILE" == *.gz ]]; then
    TEMP_FILE="/tmp/restore_dump.rdb"
    gunzip -c "$BACKUP_FILE" > "$TEMP_FILE"
    BACKUP_FILE="$TEMP_FILE"
fi

# Copy RDB file to volume
log "Copying backup to Redis data directory..."
docker cp "$BACKUP_FILE" ${CONTAINER}:/data/dump.rdb

# Start Redis
log "Starting Redis..."
docker-compose -f /home/deploy/projects/${PROJECT}/docker-compose.yml start redis

# Wait for Redis to load data
sleep 5

# Verify
KEY_COUNT=$(docker exec ${CONTAINER} redis-cli -a ${REDIS_PASSWORD} DBSIZE)
log "✓ Redis restored. Keys: ${KEY_COUNT}"

# Cleanup
[ -f "$TEMP_FILE" ] && rm "$TEMP_FILE"

log "Restore completed!"
```

### Testing Restore in Development

**Always test restores in development first!**

```bash
#!/bin/bash
# /home/deploy/scripts/test-restore.sh

# This script tests a restore on a temporary database

BACKUP_FILE=$1

if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file>"
    exit 1
fi

log() { echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1"; }

log "Creating temporary PostgreSQL container for restore testing..."

# Start temporary database
docker run -d \
    --name test_restore_postgres \
    -e POSTGRES_PASSWORD=testpass \
    -e POSTGRES_DB=testdb \
    postgres:15-alpine

# Wait for it to be ready
sleep 5

# Restore backup
log "Restoring backup to test database..."
if [[ "$BACKUP_FILE" == *.gz ]]; then
    gunzip -c "$BACKUP_FILE" | docker exec -i test_restore_postgres psql -U postgres -d testdb
else
    docker exec -i test_restore_postgres psql -U postgres -d testdb < "$BACKUP_FILE"
fi

if [ $? -eq 0 ]; then
    log "✓ Restore test successful!"
    
    # Show some stats
    log "Database statistics:"
    docker exec test_restore_postgres psql -U postgres -d testdb -c \
        "SELECT schemaname, tablename, n_live_tup as rows FROM pg_stat_user_tables ORDER BY n_live_tup DESC LIMIT 10;"
else
    log "✗ Restore test failed!"
fi

# Cleanup
log "Cleaning up test container..."
docker stop test_restore_postgres
docker rm test_restore_postgres

log "Test complete!"
```

---

## 6. Migration Management {#migration-management}

Database migrations are changes to your database schema (creating tables, adding columns, etc.). Managing migrations properly prevents data loss and deployment issues.

### Migration Tools Overview

**Popular Migration Tools**:

- **Node.js/PostgreSQL**: Sequelize, TypeORM, Knex.js, node-pg-migrate
- **Node.js/MongoDB**: Mongoose, migrate-mongo
- **Go/PostgreSQL**: golang-migrate, goose
- **Python**: Alembic, Django migrations

### Setting Up Migrations with Node.js (Sequelize)

**Install Sequelize CLI**:

```bash
# In your backend directory
npm install --save-dev sequelize-cli
npm install --save sequelize pg pg-hstore

# Initialize Sequelize
npx sequelize-cli init
```

**This creates**:

```
backend/
├── config/
│   └── config.json          # Database configuration
├── migrations/              # Migration files go here
├── models/                  # Database models
└── seeders/                 # Test data
```

**Configure database connection**:

```javascript
// backend/config/config.js
require('dotenv').config();

module.exports = {
  development: {
    url: process.env.DATABASE_URL,
    dialect: 'postgres',
  },
  production: {
    url: process.env.DATABASE_URL,
    dialect: 'postgres',
    dialectOptions: {
      ssl: {
        require: false,
        rejectUnauthorized: false
      }
    },
    logging: false
  }
};
```

**Create migration**:

```bash
# Create a migration to add users table
npx sequelize-cli migration:generate --name create-users-table
```

**Edit migration file**:

```javascript
// migrations/20241222120000-create-users-table.js
'use strict';

module.exports = {
  async up(queryInterface, Sequelize) {
    await queryInterface.createTable('users', {
      id: {
        type: Sequelize.INTEGER,
        primaryKey: true,
        autoIncrement: true
      },
      username: {
        type: Sequelize.STRING(50),
        allowNull: false,
        unique: true
      },
      email: {
        type: Sequelize.STRING(255),
        allowNull: false,
        unique: true
      },
      password_hash: {
        type: Sequelize.STRING(255),
        allowNull: false
      },
      created_at: {
        type: Sequelize.DATE,
        allowNull: false,
        defaultValue: Sequelize.literal('CURRENT_TIMESTAMP')
      },
      updated_at: {
        type: Sequelize.DATE,
        allowNull: false,
        defaultValue: Sequelize.literal('CURRENT_TIMESTAMP')
      }
    });

    // Add indexes
    await queryInterface.addIndex('users', ['email']);
    await queryInterface.addIndex('users', ['username']);
  },

  async down(queryInterface, Sequelize) {
    await queryInterface.dropTable('users');
  }
};
```

**Run migration locally**:

```bash
# Run migrations
npx sequelize-cli db:migrate

# Check status
npx sequelize-cli db:migrate:status

# Rollback last migration
npx sequelize-cli db:migrate:undo

# Rollback all migrations
npx sequelize-cli db:migrate:undo:all
```

### Running Migrations in Docker

**Method 1: Migration script in package.json**:

```json
{
  "scripts": {
    "migrate": "sequelize-cli db:migrate",
    "migrate:undo": "sequelize-cli db:migrate:undo",
    "migrate:status": "sequelize-cli db:migrate:status"
  }
}
```

**Run in container**:

```bash
# Run migrations inside running container
docker exec project1_backend npm run migrate

# Or during deployment
docker-compose run --rm backend npm run migrate
```

**Method 2: Automatic migrations on startup**:

```dockerfile
# backend/Dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .

# Create startup script
COPY docker-entrypoint.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/docker-entrypoint.sh

ENTRYPOINT ["docker-entrypoint.sh"]
CMD ["node", "src/server.js"]
```

```bash
#!/bin/bash
# backend/docker-entrypoint.sh

set -e

echo "Waiting for database..."
while ! nc -z postgres 5432; do
  sleep 1
done

echo "Running database migrations..."
npm run migrate

echo "Starting application..."
exec "$@"
```

**Method 3: Separate migration service in docker-compose**:

```yaml
# docker-compose.yml
services:
  migrate:
    build: ./backend
    command: npm run migrate
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://user:pass@postgres:5432/dbname
    networks:
      - project1_internal

  backend:
    build: ./backend
    depends_on:
      migrate:
        condition: service_completed_successfully
    networks:
      - project1_internal
```

### Migration Best Practices

**1. Always write reversible migrations (up and down)**:

```javascript
module.exports = {
  async up(queryInterface, Sequelize) {
    await queryInterface.addColumn('users', 'phone', {
      type: Sequelize.STRING(20),
      allowNull: true
    });
  },

  async down(queryInterface, Sequelize) {
    await queryInterface.removeColumn('users', 'phone');
  }
};
```

**2. Never modify existing migrations**:

```
❌ DON'T: Edit 20241220-create-users.js after it's deployed
✓ DO: Create new migration 20241222-add-phone-to-users.js
```

**3. Test migrations on development first**:

```bash
# Development workflow
1. Create migration locally
2. Test up: npx sequelize-cli db:migrate
3. Test down: npx sequelize-cli db:migrate:undo
4. Test up again to ensure clean state
5. Commit migration file
6. Deploy to production
```

**4. Backup before running migrations in production**:

```bash
#!/bin/bash
# migration-wrapper.sh

# Backup first
/home/deploy/scripts/backup-postgres.sh project1

# Run migration
docker exec project1_backend npm run migrate

if [ $? -eq 0 ]; then
    echo "✓ Migration successful"
else
    echo "✗ Migration failed! Backup available for restore."
    exit 1
fi
```

### Golang Migrations (golang-migrate)

**Install migrate tool**:

```bash
# Download migrate binary
curl -L https://github.com/golang-migrate/migrate/releases/download/v4.16.2/migrate.linux-amd64.tar.gz | tar xvz
sudo mv migrate /usr/local/bin/
```

**Create migration files**:

```bash
# Create migration
migrate create -ext sql -dir migrations -seq create_users_table

# This creates:
# migrations/000001_create_users_table.up.sql
# migrations/000001_create_users_table.down.sql
```

**Write migration**:

```sql
-- migrations/000001_create_users_table.up.sql
CREATE TABLE IF NOT EXISTS users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_username ON users(username);
```

```sql
-- migrations/000001_create_users_table.down.sql
DROP TABLE IF EXISTS users;
```

**Run migrations**:

```bash
# Run migrations
migrate -path migrations -database "postgresql://user:pass@localhost:5432/dbname?sslmode=disable" up

# Check version
migrate -path migrations -database "postgresql://user:pass@localhost:5432/dbname?sslmode=disable" version

# Rollback
migrate -path migrations -database "postgresql://user:pass@localhost:5432/dbname?sslmode=disable" down 1
```

**In Docker**:

```dockerfile
# backend/Dockerfile (Go)
FROM golang:1.21-alpine AS builder

WORKDIR /app

# Install migrate
RUN apk add --no-cache curl && \
    curl -L https://github.com/golang-migrate/migrate/releases/download/v4.16.2/migrate.linux-amd64.tar.gz | tar xvz && \
    mv migrate /usr/local/bin/

# ... rest of Dockerfile

FROM alpine:latest
COPY --from=builder /usr/local/bin/migrate /usr/local/bin/
COPY --from=builder /app/main .
COPY migrations ./migrations

# Entrypoint script
COPY docker-entrypoint.sh /
RUN chmod +x /docker-entrypoint.sh

ENTRYPOINT ["/docker-entrypoint.sh"]
CMD ["./main"]
```

```bash
#!/bin/sh
# docker-entrypoint.sh

set -e

echo "Running database migrations..."
migrate -path /migrations -database "${DATABASE_URL}" up

echo "Starting application..."
exec "$@"
```

### MongoDB Migrations (migrate-mongo)

```bash
# Install
npm install -g migrate-mongo

# Initialize
migrate-mongo init

# Create migration
migrate-mongo create add-email-index-to-users
```

```javascript
// migrations/20241222120000-add-email-index-to-users.js
module.exports = {
  async up(db, client) {
    await db.collection('users').createIndex({ email: 1 }, { unique: true });
  },

  async down(db, client) {
    await db.collection('users').dropIndex('email_1');
  }
};
```

```bash
# Run migrations
migrate-mongo up

# Rollback
migrate-mongo down
```

---

## 7. Development Database Workflows {#development-workflows}

Developers need local databases that mirror production structure but use test data.

### Local Development Setup

**Option 1: Run databases locally with Docker**:

```yaml
# docker-compose.dev.yml
version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    container_name: dev_postgres
    ports:
      - "5432:5432"  # Exposed for local tools
    environment:
      POSTGRES_USER: devuser
      POSTGRES_PASSWORD: devpass
      POSTGRES_DB: devdb
    volumes:
      - dev_postgres_data:/var/lib/postgresql/data
      - ./backend/migrations:/docker-entrypoint-initdb.d
    networks:
      - dev_network

  redis:
    image: redis:7-alpine
    container_name: dev_redis
    ports:
      - "6379:6379"
    networks:
      - dev_network

volumes:
  dev_postgres_data:

networks:
  dev_network:
```

```bash
# Start dev environment
docker-compose -f docker-compose.dev.yml up -d

# Now connect from host:
psql -h localhost -p 5432 -U devuser -d devdb
```

**Option 2: Use production database snapshot**:

```bash
#!/bin/bash
# sync-production-to-dev.sh

# Download latest backup from production
scp deploy@your-server:/home/deploy/backups/project1/postgres/daily/latest.sql.gz ./

# Restore to local dev database
gunzip -c latest.sql.gz | docker exec -i dev_postgres psql -U devuser -d devdb

# Anonymize sensitive data
docker exec dev_postgres psql -U devuser -d devdb -c "
  UPDATE users SET 
    email = CONCAT('user', id, '@example.com'),
    phone = NULL,
    password_hash = '\$2b\$10\$dummy_hash_for_dev';
"

echo "✓ Production data synced to development (anonymized)"
```

### Database Seeding

**Create seed data for development**:

```javascript
// seeders/20241222120000-demo-users.js
'use strict';

module.exports = {
  async up(queryInterface, Sequelize) {
    await queryInterface.bulkInsert('users', [
      {
        username: 'alice',
        email: 'alice@example.com',
        password_hash: '$2b$10$...',  // Bcrypt hash of "password123"
        created_at: new Date(),
        updated_at: new Date()
      },
      {
        username: 'bob',
        email: 'bob@example.com',
        password_hash: '$2b$10$...',
        created_at: new Date(),
        updated_at: new Date()
      }
    ], {});
  },

  async down(queryInterface, Sequelize) {
    await queryInterface.bulkDelete('users', null, {});
  }
};
```

```bash
# Run seeders
npx sequelize-cli db:seed:all

# Undo seeders
npx sequelize-cli db:seed:undo:all
```

### Development Database Reset Script

```bash
#!/bin/bash
# reset-dev-db.sh

set -e

echo "⚠️  This will DELETE all data in the development database!"
read -p "Type 'yes' to continue: " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
    exit 0
fi

echo "Stopping containers..."
docker-compose -f docker-compose.dev.yml down

echo "Deleting database volume..."
docker volume rm dev_postgres_data

echo "Starting containers..."
docker-compose -f docker-compose.dev.yml up -d

echo "Waiting for database..."
sleep 5

echo "Running migrations..."
docker exec dev_backend npm run migrate

echo "Seeding database..."
docker exec dev_backend npm run seed

echo "✓ Development database reset complete!"
```

---

## 8. Production Database Operations {#production-operations}

### Safe Data Modifications in Production

**Never** directly modify production data without backups and a plan.

**Safe Workflow**:

```bash
# 1. Create backup
/home/deploy/scripts/backup-postgres.sh project1

# 2. Test query on a copy first
docker exec project1_postgres psql -U project1_user -d project1_database -c "
  BEGIN;
  UPDATE users SET email_verified = true WHERE email LIKE '%@company.com';
  SELECT COUNT(*) FROM users WHERE email_verified = true;
  -- Check if count looks correct
  ROLLBACK;  -- Don't commit yet!
"

# 3. If test looks good, run with commit
docker exec project1_postgres psql -U project1_user -d project1_database -c "
  BEGIN;
  UPDATE users SET email_verified = true WHERE email LIKE '%@company.com';
  COMMIT;
"

# 4. Verify result
docker exec project1_postgres psql -U project1_user -d project1_database -c "
  SELECT COUNT(*), email_verified FROM users GROUP BY email_verified;
"
```

### Adding Columns to Large Tables

**Problem**: Adding a column with `NOT NULL` and no default value locks the table.

**Solution**: Add in steps:

```sql
-- Step 1: Add column as nullable
ALTER TABLE users ADD COLUMN phone VARCHAR(20);

-- Step 2: Populate data (in batches if large table)
UPDATE users SET phone = '000-000-0000' WHERE phone IS NULL;

-- Step 3: Make NOT NULL
ALTER TABLE users ALTER COLUMN phone SET NOT NULL;
```

**For very large tables**, use batching:

```javascript
// backend/scripts/backfill-phone-numbers.js
const { Pool } = require('pg');
const pool = new Pool({ connectionString: process.env.DATABASE_URL });

async function backfillPhones() {
  let processed = 0;
  const batchSize = 1000;

  while (true) {
    const result = await pool.query(`
      UPDATE users 
      SET phone = '000-000-0000' 
      WHERE id IN (
        SELECT id FROM users 
        WHERE phone IS NULL 
        LIMIT $1
      )
      RETURNING id
    `, [batchSize]);

    processed += result.rowCount;
    console.log(`Processed ${processed} rows...`);

    if (result.rowCount < batchSize) break;
    
    // Small delay to avoid overloading database
    await new Promise(resolve => setTimeout(resolve, 100));
  }

  console.log(`✓ Backfill complete. Total rows: ${processed}`);
}

backfillPhones().then(() => process.exit(0));
```

```bash
# Run backfill script
docker exec project1_backend node scripts/backfill-phone-numbers.js
```

### Database Performance Monitoring

```bash
#!/bin/bash
# check-database-performance.sh

CONTAINER="project1_postgres"
DB_USER="project1_user"
DB_NAME="project1_database"

echo "=== Database Performance Stats ==="

# Connection count
echo -e "\n--- Active Connections ---"
docker exec $CONTAINER psql -U $DB_USER -d $DB_NAME -c "
  SELECT COUNT(*), state 
  FROM pg_stat_activity 
  WHERE datname = '$DB_NAME' 
  GROUP BY state;
"

# Slow queries
echo -e "\n--- Slow Queries (> 1 second) ---"
docker exec $CONTAINER psql -U $DB_USER -d $DB_NAME -c "
  SELECT pid, now() - query_start as duration, query 
  FROM pg_stat_activity 
  WHERE state = 'active' 
    AND now() - query_start > interval '1 second'
  ORDER BY duration DESC;
"

# Table sizes
echo -e "\n--- Largest Tables ---"
docker exec $CONTAINER psql -U $DB_USER -d $DB_NAME -c "
  SELECT 
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
  FROM pg_tables 
  WHERE schemaname = 'public'
  ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC 
  LIMIT 10;
"

# Index usage
echo -e "\n--- Unused Indexes ---"
docker exec $CONTAINER psql -U $DB_USER -d $DB_NAME -c "
  SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan
  FROM pg_stat_user_indexes
  WHERE idx_scan = 0
  ORDER BY pg_relation_size(indexrelid) DESC
  LIMIT 10;
"
```

---

## 9. Database Replication and High Availability {#replication}

For critical production systems, set up database replication for redundancy.

### PostgreSQL Streaming Replication

**Architecture**:

```
┌───────────────────────────────────────────────────────┐
│                   Primary Server                      │
│  ┌──────────────────────────────────────────────────┐ │
│  │  PostgreSQL Primary (Read/Write)                 │ │
│  │  - Accepts all writes                            │ │
│  │  - Streams WAL to replicas                       │ │
│  └────────────────┬─────────────────────────────────┘ │
└───────────────────┼───────────────────────────────────┘
                    │
                    │ Streaming Replication
                    │
        ┌───────────┼───────────┐
        │           │           │
        ▼           ▼           ▼
┌──────────────┐ ┌──────────────┐ ┌──────────────┐
│  Replica 1   │ │  Replica 2   │ │  Replica 3   │
│  (Read-only) │ │  (Read-only) │ │  (Read-only) │
│              │ │              │ │              │
│  Hot Standby │ │  Hot Standby │ │  Hot Standby │
└──────────────┘ └──────────────┘ └──────────────┘
```

**Primary Setup**:

```yaml
# docker-compose.primary.yml
services:
  postgres-primary:
    image: postgres:15-alpine
    container_name: postgres_primary
    environment:
      POSTGRES_USER: replicator
      POSTGRES_PASSWORD: repl_password
      POSTGRES_DB: production_db
    volumes:
      - postgres_primary_data:/var/lib/postgresql/data
      - ./postgresql.primary.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
```

```conf
# postgresql.primary.conf
listen_addresses = '*'
wal_level = replica
max_wal_senders = 3
wal_keep_size = 1GB
hot_standby = on
```

**Replica Setup**:

```yaml
# docker-compose.replica.yml
services:
  postgres-replica:
    image: postgres:15-alpine
    container_name: postgres_replica
    environment:
      POSTGRES_USER: replicator
      POSTGRES_PASSWORD: repl_password
    volumes:
      - postgres_replica_data:/var/lib/postgresql/data
      - ./recovery.conf:/var/lib/postgresql/data/recovery.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
```

This is an advanced topic—consult PostgreSQL documentation for full setup.

---

## 10. Monitoring and Maintenance {#monitoring}

### Database Health Checks

```bash
#!/bin/bash
# database-health-check.sh

check_postgres() {
    local container=$1
    local user=$2
    local db=$3
    
    # Check if container is running
    if ! docker ps --format '{{.Names}}' | grep -q "^${container}$"; then
        echo "❌ Container ${container} is not running"
        return 1
    fi
    
    # Check database connectivity
    if docker exec ${container} pg_isready -U ${user} > /dev/null 2>&1; then
        echo "✓ ${container}: Database is ready"
    else
        echo "❌ ${container}: Database is not ready"
        return 1
    fi
    
    # Check for blocking queries
    local blocking=$(docker exec ${container} psql -U ${user} -d ${db} -t -c \
        "SELECT COUNT(*) FROM pg_stat_activity WHERE wait_event_type = 'Lock';")
    
    if [ "$blocking" -gt 0 ]; then
        echo "⚠️  ${container}: ${blocking} queries waiting on locks"
    fi
    
    # Check replication lag (if replica)
    # ...
}

# Check all databases
check_postgres "project1_postgres" "project1_user" "project1_database"
check_postgres "project2_postgres" "project2_user" "project2_database"
```

### Automated Vacuum and Analyze

```bash
#!/bin/bash
# vacuum-databases.sh

# PostgreSQL needs regular VACUUM to reclaim space and update statistics

docker exec project1_postgres psql -U project1_user -d project1_database -c "VACUUM ANALYZE;"
docker exec project2_postgres psql -U project2_user -d project2_database -c "VACUUM ANALYZE;"

echo "✓ Database maintenance complete"
```

```bash
# Schedule weekly
# crontab -e
0 3 * * 0 /home/deploy/scripts/vacuum-databases.sh >> /home/deploy/logs/vacuum.log 2>&1
```

---

## 11. Security Best Practices {#security}

### Database Security Checklist

**1. Strong passwords**:

```bash
# Generate secure password
openssl rand -base64 32

# Store in environment variables, never in code
```

**2. Least privilege principle**:

```sql
-- Don't give application user superuser access
-- Create limited user
CREATE USER app_user WITH PASSWORD 'password';
GRANT CONNECT ON DATABASE mydb TO app_user;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_user;
```

**3. Network isolation**:

```yaml
# Database should NOT be on proxy_network
services:
  postgres:
    networks:
      - project1_internal  # Internal only!
```

**4. Encrypted backups**:

```bash
# Encrypt backup before storing
pg_dump -U user dbname | gzip | gpg --encrypt --recipient admin@company.com > backup.sql.gz.gpg

# Decrypt
gpg --decrypt backup.sql.gz.gpg | gunzip | psql -U user dbname
```

**5. Audit logging**:

```conf
# postgresql.conf
log_statement = 'mod'  # Log all modifications
log_connections = on
log_disconnections = on
```

---

## 12. Complete Working Examples {#examples}

### Complete Production Database Management Script

```bash
#!/bin/bash
# /home/deploy/scripts/db-manager.sh

set -e

PROJECT="project1"
CONTAINER="${PROJECT}_postgres"
DB_USER="${PROJECT}_user"
DB_NAME="${PROJECT}_database"

usage() {
    cat << EOF
Database Manager for ${PROJECT}

Usage: $0 <command> [options]

Commands:
    backup              Create database backup
    restore <file>      Restore from backup file
    migrate             Run pending migrations
    shell               Open database shell
    stats               Show database statistics
    vacuum              Run VACUUM ANALYZE
    health              Check database health
    
Examples:
    $0 backup
    $0 restore /path/to/backup.sql.gz
    $0 migrate
    $0 shell
EOF
    exit 1
}

backup() {
    echo "Creating backup..."
    /home/deploy/scripts/backup-postgres.sh ${PROJECT}
}

restore() {
    local backup_file=$1
    if [ -z "$backup_file" ]; then
        echo "Error: Backup file required"
        usage
    fi
    /home/deploy/scripts/restore-postgres.sh ${PROJECT} ${backup_file}
}

migrate() {
    echo "Running migrations..."
    cd /home/deploy/projects/${PROJECT}
    docker-compose exec backend npm run migrate
}

shell() {
    docker exec -it ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME}
}

stats() {
    docker exec ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME} -c "
        SELECT 
            schemaname,
            tablename,
            pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
            n_live_tup as rows
        FROM pg_tables 
        JOIN pg_stat_user_tables USING (schemaname, tablename)
        WHERE schemaname = 'public'
        ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
    "
}

vacuum() {
    echo "Running VACUUM ANALYZE..."
    docker exec ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME} -c "VACUUM ANALYZE;"
    echo "✓ Complete"
}

health() {
    if docker exec ${CONTAINER} pg_isready -U ${DB_USER} > /dev/null 2>&1; then
        echo "✓ Database is healthy"
        
        # Show connections
        connections=$(docker exec ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME} -t -c \
            "SELECT COUNT(*) FROM pg_stat_activity WHERE datname = '${DB_NAME}';")
        echo "  Active connections: ${connections}"
        
        # Show database size
        db_size=$(docker exec ${CONTAINER} psql -U ${DB_USER} -d ${DB_NAME} -t -c \
            "SELECT pg_size_pretty(pg_database_size('${DB_NAME}'));")
        echo "  Database size: ${db_size}"
    else
        echo "❌ Database is unhealthy"
        exit 1
    fi
}

case "${1:-}" in
    backup) backup ;;
    restore) restore "$2" ;;
    migrate) migrate ;;
    shell) shell ;;
    stats) stats ;;
    vacuum) vacuum ;;
    health) health ;;
    *) usage ;;
esac
```

**Make it executable**:

```bash
chmod +x /home/deploy/scripts/db-manager.sh

# Create alias for convenience
echo "alias db1='bash /home/deploy/scripts/db-manager.sh project1'" >> ~/.bashrc
source ~/.bashrc

# Usage:
db1 backup
db1 stats
db1 shell
```

---

## Summary

You now have complete database management capabilities with Docker:

✅ **Access production databases** safely via SSH tunnels or docker exec  
✅ **Automated backups** with retention policies and S3 sync  
✅ **Restore procedures** with safety checks and rollback  
✅ **Migration management** for schema changes  
✅ **Development workflows** for testing and data seeding  
✅ **Performance monitoring** and maintenance  
✅ **Security best practices** for protecting data  
✅ **Complete scripts** ready to use in production

### Key Takeaways

1. **Always backup before modifications**
2. **Test migrations in development first**
3. **Use read-only users for developers**
4. **Automate backups and test restores regularly**
5. **Monitor database health continuously**
6. **Keep databases isolated from public networks**
7. **Document all database procedures**

Your databases are the heart of your application—treat them with care! 💝