# The Complete Guide to Advanced Channel Concepts in Go

## Table of Contents

1. [Introduction - Beyond Basic Channels](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#introduction)
2. [Channel Buffering](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#channel-buffering)
3. [Channel Synchronization](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#channel-synchronization)
4. [Channel Directions](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#channel-directions)
5. [Select Statement](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#select)
6. [Timeouts](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#timeouts)
7. [Non-Blocking Channel Operations](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#non-blocking)
8. [Closing Channels](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#closing-channels)
9. [Range over Channels](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#range-channels)
10. [Advanced Patterns and Real-World Examples](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#advanced-patterns)

---

## Introduction - Beyond Basic Channels {#introduction}

Imagine you're organizing logistics for a busy shipping company. You have trucks (goroutines) picking up packages (data) and delivering them to destinations (other goroutines). The most basic approach is direct hand-off: a delivery truck waits at the sender's location until a receiving truck arrives, and they transfer the package directly. This works, but it's inefficient - trucks spend a lot of time waiting.

Now imagine adding a small warehouse (buffer) at each transfer point. The delivery truck can drop off packages at the warehouse and immediately leave to pick up more packages. The receiving truck can come later and pick up from the warehouse. If the warehouse is full, the delivery truck waits. If the warehouse is empty, the receiving truck waits. This is channel buffering - it adds elasticity to the system.

But channels are much more than just data pipes. They're sophisticated synchronization primitives that enable complex coordination patterns:

- **Synchronization**: Using channels to coordinate when things happen (not just what data is transferred)
- **Directionality**: Restricting channels to send-only or receive-only for safety
- **Selection**: Waiting on multiple channels simultaneously, responding to whichever is ready first
- **Timeouts**: Giving up on channel operations after a deadline
- **Non-blocking**: Attempting channel operations without waiting
- **Closing**: Signaling that no more data will be sent
- **Ranging**: Iterating over all values in a channel until it closes

Each of these concepts solves specific real-world coordination problems. Understanding them deeply transforms how you think about concurrent programming. You stop thinking in terms of locks and shared memory, and start thinking in terms of communication and coordination. This is the Go way.

---

## Channel Buffering {#channel-buffering}

Channel buffering is perhaps the most important concept to master because it fundamentally changes the behavior of channels. Let's explore this deeply.

### Unbuffered Channels: The Default

When you create a channel without specifying a capacity, you get an unbuffered channel:

```go
ch := make(chan int)  // Unbuffered
```

An unbuffered channel has zero capacity. It cannot store any values. This means every send operation must wait for a corresponding receive operation, and every receive operation must wait for a corresponding send. This is called a **rendezvous** - sender and receiver must meet at the channel.

**Mental Model**: Think of an unbuffered channel as two people directly handing an object to each other. Neither can leave until the handoff completes. If the sender arrives first, they wait holding the object until the receiver arrives. If the receiver arrives first, they wait with empty hands until the sender arrives.

**Execution Timeline**:

```
Goroutine A (Sender):          Goroutine B (Receiver):
─────────────────────────────────────────────────────────
T=0ms:  ch <- 42
        [BLOCKED]              
        Waiting for            
        receiver...            
                               T=10ms: value := <-ch
                                       [Receives 42]
T=10ms: Send completes
        Continues execution    Continues with value=42
```

Notice that the sender is blocked for 10 milliseconds until the receiver arrives. The receiver gets the value immediately (there's no waiting because the sender is already waiting). They synchronize at T=10ms.

**Key Characteristics**:

**Synchronization Guarantee**: When a send on an unbuffered channel completes, you have an absolute guarantee that some goroutine has received that value. Not "will receive" or "might receive" - has already received. This happens-before relationship is very powerful for coordination.

**No Data Storage**: The channel itself stores no data. The data flows directly from sender's memory to receiver's memory (or through a small intermediate location, but conceptually it's direct transfer).

**Blocking is the Norm**: Both sends and receives typically block. This is by design - unbuffered channels are for synchronization more than data transfer.

### Buffered Channels: Adding Elasticity

A buffered channel has capacity to store values:

```go
ch := make(chan int, 3)  // Buffered, capacity 3
```

This channel can hold up to 3 integers. Sends succeed immediately as long as the buffer isn't full. Receives succeed immediately as long as the buffer isn't empty.

**Mental Model**: Think of a buffered channel as a small warehouse with a fixed number of slots. The delivery truck (sender) can drop off a package in an empty slot and leave immediately. The pickup truck (receiver) can take a package from a full slot and leave immediately. Only when all slots are full does the delivery truck wait. Only when all slots are empty does the pickup truck wait.

**Execution Timeline**:

```
Goroutine A (Sender):          Buffer State:       Goroutine B (Receiver):
──────────────────────────────────────────────────────────────────────────
T=0ms:  ch <- 42               [42, _, _]
        Completes immediately! 
        (buffer not full)
        
T=1ms:  ch <- 100              [42, 100, _]
        Completes immediately!
        
T=2ms:  ch <- 200              [42, 100, 200]
        Completes immediately!
        
T=3ms:  ch <- 300              [42, 100, 200]
        [BLOCKED]              (buffer full)
        Waiting for            
        buffer space...        
                                                   T=10ms: val := <-ch
                                                           Gets 42
                               [100, 200, 300]             (buffer has space)
T=10ms: Send of 300 completes
        Continues execution                        Continues with val=42
```

Notice that the sender completed three sends without blocking. The fourth send blocked because the buffer was full. When the receiver took one value, it created space, allowing the sender to complete.

**Key Characteristics**:

**Asynchronous Communication**: Sender and receiver don't need to rendezvous. They can operate at different rates as long as the buffer absorbs the difference.

**Decoupling in Time**: The sender can send now, and the receiver can receive later. The buffer holds the data in between.

**Backpressure**: When the buffer fills, the sender is forced to slow down. This provides automatic flow control - fast senders are throttled to match slow receivers.

### Buffer Size Selection: The Art and Science

Choosing the right buffer size is critical. Too small, and you lose the benefits of buffering. Too large, and you waste memory and potentially mask design issues.

**Size 0 (Unbuffered)**: Use when you want strict synchronization. Every send must be matched immediately with a receive. Examples:

- Request-response pattern: send request, wait for response
- Handshake protocol: both sides must acknowledge
- Critical coordination: one goroutine must wait for another

**Size 1**: The most common buffer size. Provides just enough decoupling to avoid immediate blocking while keeping memory usage minimal. Examples:

- Signal channels: sending a notification that receiver might check slightly later
- Simple producer-consumer: producer makes one item while consumer processes previous item
- Event channels: one event can be pending

**Size N (Small Fixed)**: When you know the typical burst size or number of concurrent senders/receivers. Examples:

- N concurrent workers sending results: buffer size N ensures no worker blocks on send
- Request queue: buffer holds N pending requests
- Rate limiter: buffer holds N tokens

**Large Buffers (Hundreds/Thousands)**: Use cautiously. A large buffer can hide problems:

- If your buffer is constantly full, your consumer is too slow - fix the consumer, don't increase the buffer indefinitely
- Large buffers increase memory usage and latency (data sits in the buffer longer)
- Exception: When you genuinely need to absorb bursts and have memory to spare

**The Golden Rule**: Start with the smallest buffer that achieves your throughput goals. If you find senders blocking frequently and you can handle more memory, increase the buffer. But first investigate whether you can make the receiver faster.

### Buffering and Memory

Buffered channels allocate memory for the buffer when created. The memory is proportional to capacity × element size.

```go
ch := make(chan [1024]byte, 1000)  // 1MB buffer (1024 bytes × 1000 capacity)
```

This allocates approximately 1MB immediately. If your elements are large (structs, arrays), buffering can consume significant memory. Consider sending pointers instead:

```go
ch := make(chan *[1024]byte, 1000)  // Only 8KB buffer on 64-bit (8 bytes × 1000)
```

Now the buffer holds only pointers (8 bytes each on 64-bit systems), dramatically reducing memory.

**Trade-off**: Sending pointers means the data lives on the heap (not transferred), so sender and receiver share the underlying data. This can be efficient but requires care to avoid concurrent access bugs.

### Buffering and Performance

Buffered channels can significantly improve performance by reducing synchronization overhead.

**Scenario**: Producer creating items quickly, consumer processing them slightly slower on average but catching up in bursts.

Without buffering:

- Producer creates item (fast)
- Producer sends on unbuffered channel (blocks until consumer ready)
- Consumer receives (wakes up)
- Consumer processes (slow)
- Producer continues (creates next item, blocks again)

Every item transfer requires a synchronization. The producer is constantly blocked.

With buffering (say, capacity 10):

- Producer creates 10 items quickly, sending them all to buffer (no blocking)
- Producer continues creating (while consumer works on first item)
- Consumer processes items from buffer (no blocking until buffer empty)

The producer is only blocked when the buffer fills, which might be never if the consumer keeps up on average. This eliminates most synchronization overhead.

**Measurement**: In benchmarks, buffered channels can be 2-10x faster than unbuffered for high-throughput scenarios. But for low-throughput scenarios (sending occasionally), the difference is negligible.

### Common Pitfall: Using Buffering to Hide Deadlocks

A dangerous mistake is using large buffers to avoid apparent deadlocks:

```go
// Bad: Hiding a design problem
ch := make(chan int, 1000000)  // Huge buffer

for i := 0; i < 1000000; i++ {
    ch <- i  // Never blocks (buffer so large)
}

// Forgot to receive! But no panic because buffer so large.
```

This seems to work, but it's a ticking time bomb. If you ever send more than 1,000,000 values, it deadlocks. If you run it again with 2,000,000 values, it breaks. The large buffer just masked the fact that you forgot to receive.

**Correct Approach**: Use appropriate buffer size for your use case, and ensure every send has a corresponding receive (or the channel is closed to signal completion).

---

## Channel Synchronization {#channel-synchronization}

Channels aren't just for passing data - they're powerful synchronization primitives. Many concurrent programming problems can be elegantly solved using channels for coordination.

### Synchronization Pattern 1: Wait for Completion

The most basic synchronization: one goroutine waits for another to complete some work.

**The Problem**: You spawn a goroutine to do work. Your main goroutine needs to wait until that work is done before proceeding.

**Without Channels (Naive Approach)**:

```go
done := false

go func() {
    // Do work
    time.Sleep(1 * time.Second)
    done = true  // Race condition!
}()

for !done {  // Busy-wait, wastes CPU
    time.Sleep(10 * time.Millisecond)
}
```

This has multiple problems:

- Race condition: reading/writing `done` without synchronization
- Busy-wait: constantly checking the flag wastes CPU
- Polling delay: main goroutine doesn't know immediately when work completes

**With Channels (Correct Approach)**:

```go
done := make(chan bool)

go func() {
    // Do work
    time.Sleep(1 * time.Second)
    done <- true  // Signal completion
}()

<-done  // Wait for signal
```

This is clean, safe, and efficient:

- No race condition: channel handles synchronization
- No busy-wait: main goroutine blocks (consuming no CPU) until signaled
- Immediate notification: main goroutine wakes up the instant work completes

**What Happens Internally**:

```
Timeline:
T=0ms:    Main goroutine creates channel
T=0ms:    Main goroutine spawns worker goroutine
T=0ms:    Main goroutine executes <-done
          [Main goroutine BLOCKS on receive]
          [Main goroutine is PARKED - not running]
          
T=0-1000ms: Worker goroutine does work
          [Main goroutine still parked]
          
T=1000ms: Worker goroutine executes done <- true
          [Send completes]
          [Main goroutine is UNPARKED - made runnable]
          
T=1000ms: Main goroutine resumes
          [Receive completes, got true]
          Main goroutine continues
```

The main goroutine spends 1000ms blocked, consuming zero CPU. The moment the worker signals, the main goroutine immediately resumes. Perfect synchronization.

**Optimization**: For pure synchronization (where you don't care about the value sent), use an empty struct:

```go
done := make(chan struct{})  // struct{} is zero-size

go func() {
    // Do work
    done <- struct{}{}  // Send zero-size value
}()

<-done  // Wait for signal (ignore value)
```

Why `struct{}`? It has zero size, so sending it allocates no memory. It's purely a signal. This is idiomatic Go for "I don't care about the value, just the signal."

### Synchronization Pattern 2: Wait for Multiple Goroutines

A common scenario: spawn N goroutines to do work in parallel, then wait for all to complete.

**Using N Channels (Naive)**:

```go
done1 := make(chan bool)
done2 := make(chan bool)
done3 := make(chan bool)

go worker(done1)
go worker(done2)
go worker(done3)

<-done1
<-done2
<-done3
```

This works but doesn't scale. With 100 goroutines, you need 100 channels and 100 receive operations. Tedious.

**Using One Channel with Counter**:

```go
done := make(chan bool, 3)  // Buffer for 3 signals

go worker(done)
go worker(done)
go worker(done)

for i := 0; i < 3; i++ {
    <-done  // Receive 3 times
}
```

Better, but you must track the count manually. Easy to make mistakes.

**Using sync.WaitGroup (Correct for This Case)**:

```go
var wg sync.WaitGroup

for i := 0; i < 3; i++ {
    wg.Add(1)
    go func() {
        defer wg.Done()
        // Do work
    }()
}

wg.Wait()  // Wait for all goroutines
```

For this specific pattern (wait for N goroutines), `sync.WaitGroup` is the right tool. But channels are more flexible for other synchronization patterns.

**When Channels are Better**: When you need to collect results from goroutines:

```go
results := make(chan Result, 10)

for i := 0; i < 10; i++ {
    go func(id int) {
        result := doWork(id)
        results <- result
    }(i)
}

for i := 0; i < 10; i++ {
    result := <-results
    process(result)
}
```

Here, channels serve dual purpose: synchronization (waiting for completion) and data transfer (getting results).

### Synchronization Pattern 3: Barrier (All Goroutines Wait for Each Other)

Sometimes you need all goroutines to reach a certain point before any proceed. This is called a barrier.

**Example Scenario**: Multi-phase computation where all goroutines must complete phase 1 before any start phase 2.

**Implementation with Channels**:

```go
func worker(id int, phase1Done chan struct{}, phase2Start chan struct{}) {
    // Phase 1
    fmt.Printf("Worker %d: Phase 1\n", id)
    time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
    
    phase1Done <- struct{}{}  // Signal phase 1 complete
    
    <-phase2Start  // Wait for signal to start phase 2
    
    // Phase 2
    fmt.Printf("Worker %d: Phase 2\n", id)
}

func main() {
    numWorkers := 5
    phase1Done := make(chan struct{}, numWorkers)
    phase2Start := make(chan struct{})
    
    // Start workers
    for i := 0; i < numWorkers; i++ {
        go worker(i, phase1Done, phase2Start)
    }
    
    // Wait for all workers to complete phase 1
    for i := 0; i < numWorkers; i++ {
        <-phase1Done
    }
    
    fmt.Println("All workers completed phase 1")
    
    // Signal all workers to start phase 2
    close(phase2Start)  // Closing signals all waiters!
    
    time.Sleep(1 * time.Second)  // Wait for phase 2 to complete
}
```

**Key Insight**: Closing a channel unblocks all goroutines waiting to receive from it. They all receive the zero value simultaneously. This is a broadcast primitive - one action (closing) signals multiple goroutines.

### Synchronization Pattern 4: Mutex Alternative

Channels can sometimes replace mutexes for protecting shared state. Instead of multiple goroutines accessing shared data with locks, a single goroutine owns the data and other goroutines send requests to it via channels.

**Example**: A counter that multiple goroutines need to increment.

**With Mutex**:

```go
var (
    counter int
    mu      sync.Mutex
)

func increment() {
    mu.Lock()
    counter++
    mu.Unlock()
}

// Many goroutines call increment()
```

**With Channels**:

```go
type request struct {
    op    string  // "increment" or "get"
    reply chan int
}

func counterManager(requests chan request) {
    counter := 0
    for req := range requests {
        switch req.op {
        case "increment":
            counter++
        case "get":
            req.reply <- counter
        }
    }
}

func main() {
    requests := make(chan request)
    go counterManager(requests)
    
    // To increment:
    requests <- request{op: "increment"}
    
    // To get value:
    reply := make(chan int)
    requests <- request{op: "get", reply: reply}
    value := <-reply
}
```

**Comparison**:

**Mutex Approach**:

- Pros: Simpler for straightforward cases, less overhead for high contention
- Cons: Easy to introduce deadlocks or forget to unlock, hard to do complex state transitions

**Channel Approach**:

- Pros: All state access is serialized naturally (only one goroutine touches data), complex operations are easier (multiple state changes are atomic), no deadlock from forgetting to unlock
- Cons: More goroutines, slightly higher overhead for simple operations

**When to Use Each**:

- Use mutex for simple, short critical sections (incrementing a counter, updating a flag)
- Use channels for complex state machines, when you need to coordinate multiple operations, or when the serialization model makes the code clearer

### Synchronization Pattern 5: Semaphore (Limiting Concurrency)

A semaphore limits how many goroutines can access a resource concurrently. This is useful for rate limiting, connection pooling, etc.

**Implementation with Buffered Channel**:

```go
// Semaphore with capacity 3 (at most 3 goroutines active)
sem := make(chan struct{}, 3)

for i := 0; i < 10; i++ {
    sem <- struct{}{}  // Acquire (blocks if 3 already acquired)
    
    go func(id int) {
        defer func() { <-sem }()  // Release
        
        // Critical section: only 3 goroutines here at once
        performExpensiveOperation(id)
    }(i)
}

// Wait for all to complete
for i := 0; i < 3; i++ {
    sem <- struct{}{}
}
```

**How It Works**: The buffered channel has capacity 3. Each `sem <- struct{}{}` consumes one slot. When all 3 slots are full, the next send blocks. This naturally limits concurrency to 3. When a goroutine completes and receives from `sem`, it frees a slot, allowing another goroutine to start.

This is a elegant way to limit concurrency without complex bookkeeping. The channel handles all the coordination.

## Channel Directions {#channel-directions}

Channel directions are a type system feature that makes your code safer and more self-documenting. They specify whether a channel can be used for sending, receiving, or both.

### The Three Channel Types

**Bidirectional Channel** (normal channel):

```go
ch := make(chan int)  // Can send and receive
```

This is what you get when you create a channel normally. You can both send (`ch <- value`) and receive (`<-ch`) on it.

**Send-Only Channel**:

```go
var sendCh chan<- int  // Can only send
```

The arrow points into the channel. This channel can only be used for sending. Attempting to receive from it is a compile-time error.

**Receive-Only Channel**:

```go
var recvCh <-chan int  // Can only receive
```

The arrow points out of the channel. This channel can only be used for receiving. Attempting to send on it is a compile-time error.

### Why Channel Directions Matter

At first, restricting what you can do with a channel seems limiting. Why would you want a channel you can only send on? The answer lies in **safety through type system** and **self-documenting code**.

**Problem Without Directions**:

Imagine you have a function that's supposed to only receive from a channel:

```go
func consumer(ch chan int) {
    for value := range ch {
        process(value)
    }
}
```

Nothing stops you from accidentally writing:

```go
func consumer(ch chan int) {
    ch <- 42  // Oops! Consumer shouldn't send
    for value := range ch {
        process(value)
    }
}
```

This is a logic error that might cause deadlocks or unexpected behavior. But the compiler can't help you - `ch` is bidirectional, so sending is allowed.

**Solution With Directions**:

```go
func consumer(ch <-chan int) {  // Receive-only
    // ch <- 42  // Compile error: cannot send on receive-only channel
    for value := range ch {
        process(value)
    }
}
```

Now if you accidentally try to send, the compiler immediately catches the error. You can't ship a bug that sends when it should only receive.

### Self-Documenting Code

Channel directions make function signatures self-documenting. Look at this function:

```go
func pipeline(in <-chan int, out chan<- int) {
    for value := range in {
        result := process(value)
        out <- result
    }
}
```

Without even reading the function body, you immediately know:

- This function receives from `in`
- This function sends to `out`
- This function is a middle stage in a pipeline

Compare to:

```go
func pipeline(in chan int, out chan int) {
    // What does this function do? Have to read the code to find out
}
```

With bidirectional channels, you can't tell what the function does without reading the implementation.

### Ownership and Responsibilities

Channel directions also document ownership and responsibility:

**Producer** (sends, should close):

```go
func producer(out chan<- int) {
    for i := 0; i < 10; i++ {
        out <- i
    }
    close(out)  // Producer closes when done
}
```

The send-only type documents that this function is a producer. By convention, producers close channels when done.

**Consumer** (receives, should not close):

```go
func consumer(in <-chan int) {
    for value := range in {
        process(value)
    }
    // Consumer never closes
}
```

The receive-only type documents that this function is a consumer. Consumers never close channels - they don't own them.

**Middle Stage** (receives from one, sends to another):

```go
func transformer(in <-chan int, out chan<- int) {
    defer close(out)  // Middle stage closes output
    
    for value := range in {
        result := transform(value)
        out <- result
    }
}
```

This function receives from `in` and sends to `out`, documenting its role as a pipeline stage.

### Conversion Rules

Here's an important rule: you can implicitly convert from bidirectional to directional, but not the reverse:

```go
ch := make(chan int)  // Bidirectional

var sendCh chan<- int = ch  // OK: bidirectional → send-only
var recvCh <-chan int = ch  // OK: bidirectional → receive-only

// But you CANNOT go back:
// var biCh chan int = sendCh  // Compile error!
```

This makes sense from a security perspective: you can give away capabilities (restrict what can be done), but you can't gain capabilities (that would bypass the restrictions).

**Function Parameters**:

When you pass a bidirectional channel to a function expecting a directional channel, the conversion happens automatically:

```go
func sender(ch chan<- int) {
    ch <- 42
}

func main() {
    ch := make(chan int)  // Bidirectional
    sender(ch)  // Automatically converts to chan<- int
}
```

This is why you can create a normal channel and pass it to functions with directional parameters - the restriction is applied at the function boundary.

### Complete Pipeline Example

Let's build a complete data processing pipeline using channel directions:

```go
// Generator: produces values
func generate(out chan<- int) {
    for i := 1; i <= 10; i++ {
        out <- i
    }
    close(out)  // Generator closes when done
}

// Squarer: receives values, squares them
func square(in <-chan int, out chan<- int) {
    defer close(out)  // Close output when done
    
    for value := range in {
        out <- value * value
    }
}

// Filter: only passes even values
func filterEven(in <-chan int, out chan<- int) {
    defer close(out)
    
    for value := range in {
        if value%2 == 0 {
            out <- value
        }
    }
}

// Printer: consumes and prints values
func printer(in <-chan int) {
    for value := range in {
        fmt.Println(value)
    }
}

func main() {
    // Create channels (bidirectional)
    ch1 := make(chan int)
    ch2 := make(chan int)
    ch3 := make(chan int)
    
    // Wire up pipeline
    go generate(ch1)      // ch1 becomes chan<- int
    go square(ch1, ch2)   // ch1 becomes <-chan int, ch2 becomes chan<- int
    go filterEven(ch2, ch3)  // ch2 becomes <-chan int, ch3 becomes chan<- int
    printer(ch3)          // ch3 becomes <-chan int
}
```

**What This Achieves**:

**Type Safety**: Each function can only use its channels in the intended way. `printer` can't accidentally send. `generate` can't accidentally receive.

**Clear Responsibilities**: Just by looking at function signatures, you know who produces, who consumes, who transforms.

**Closing Safety**: Each stage closes its output channel when done. Consumers never close. This prevents the "close on closed channel" panic.

**Self-Documenting**: The pipeline flow is obvious: generate → square → filterEven → printer.

---

## Select Statement {#select}

The `select` statement is Go's multiplexer for channel operations. It lets you wait on multiple channel operations simultaneously, proceeding with whichever one becomes ready first.

### The Core Concept

Imagine you're a receptionist at a busy office. You have three phones on your desk: one for sales calls, one for support calls, one for internal calls. You can't predict which phone will ring next. You need to answer whichever phone rings first.

Without `select`, you'd have to check each phone in sequence:

1. Is phone 1 ringing? No, check next
2. Is phone 2 ringing? No, check next
3. Is phone 3 ringing? No, go back to phone 1
4. Repeat forever

This is wasteful - you're constantly checking even when no phones are ringing.

With `select`, you wait for any phone to ring. When one rings, you answer it immediately. This is exactly what Go's `select` does with channels.

### Basic Select Syntax

```go
select {
case msg1 := <-ch1:
    fmt.Println("Received from ch1:", msg1)
case msg2 := <-ch2:
    fmt.Println("Received from ch2:", msg2)
case ch3 <- value:
    fmt.Println("Sent on ch3")
}
```

The `select` statement evaluates all the channel operations simultaneously:

- If multiple cases are ready, one is chosen randomly
- If one case is ready, that case executes
- If no cases are ready, the select blocks until one becomes ready

### How Select Works Internally

When a `select` executes, here's what the runtime does:

**Step 1: Evaluate All Cases**

The runtime checks each channel operation:

- For receives: is data available in the channel?
- For sends: is there space in the buffer (or a waiting receiver)?

**Step 2: Choose a Ready Case**

**If multiple cases are ready**: The runtime randomly selects one. This randomness prevents starvation - no channel is always preferred over others.

**If one case is ready**: That case executes immediately.

**If no cases are ready**: The goroutine is added to the wait queue of all channels in the select. The goroutine blocks until any channel operation can proceed. When a channel becomes ready, the goroutine is woken up, and that case executes.

**Step 3: Execute the Selected Case**

The selected channel operation completes, and the code in that case block runs. After the case completes, the select statement finishes - it doesn't loop.

### Select vs. If-Else Chain

Newcomers sometimes think `select` is like a switch or if-else chain over channels. It's not. A key difference:

**If-Else Chain (Sequential Checking)**:

```go
if msg1, ok := <-ch1; ok {
    process(msg1)
} else if msg2, ok := <-ch2; ok {
    process(msg2)
} else if msg3, ok := <-ch3; ok {
    process(msg3)
}
```

This checks `ch1` first. If `ch1` has data, it processes it. Only if `ch1` has no data does it check `ch2`. This creates priority: `ch1` is always checked first.

**Select (Simultaneous Checking)**:

```go
select {
case msg1 := <-ch1:
    process(msg1)
case msg2 := <-ch2:
    process(msg2)
case msg3 := <-ch3:
    process(msg3)
}
```

This checks all channels simultaneously. If multiple have data, one is chosen randomly. No channel has priority. This is fair.

### The Default Case: Non-Blocking Select

A `select` with a `default` case never blocks:

```go
select {
case msg := <-ch:
    fmt.Println("Received:", msg)
default:
    fmt.Println("No data available")
}
```

If `ch` has data, the receive case executes. If `ch` has no data, the default case executes immediately. The select doesn't wait.

**Use Cases**:

**Non-Blocking Receive**: Try to receive, but don't wait if nothing's available.

```go
select {
case msg := <-ch:
    process(msg)
default:
    // No message, do something else
}
```

**Non-Blocking Send**: Try to send, but don't wait if channel is full.

```go
select {
case ch <- value:
    // Sent successfully
default:
    // Channel full, couldn't send
}
```

**Polling Loop**: Check channel periodically while doing other work.

```go
for {
    select {
    case msg := <-ch:
        process(msg)
    default:
        // No message, do other work
        doOtherWork()
    }
}
```

### Real-World Example: Server with Multiple Input Sources

Imagine a server that needs to handle requests from multiple sources: HTTP, gRPC, and WebSocket. Each source has its own channel. The server should process requests from whichever source has them.

```go
func server(httpRequests <-chan Request, grpcRequests <-chan Request, 
            wsRequests <-chan Request, quit <-chan struct{}) {
    for {
        select {
        case req := <-httpRequests:
            handleHTTPRequest(req)
        case req := <-grpcRequests:
            handleGRPCRequest(req)
        case req := <-wsRequests:
            handleWSRequest(req)
        case <-quit:
            fmt.Println("Server shutting down")
            return
        }
    }
}
```

**What This Achieves**:

**Fairness**: All request types are treated equally. If HTTP and gRPC both have pending requests, one is randomly chosen. Neither is starved.

**Efficiency**: The server blocks once (in the select) waiting for any request. It doesn't waste CPU polling each channel.

**Graceful Shutdown**: The `quit` channel allows clean termination. When closed, the server exits the loop and shuts down gracefully.

### Select in a Loop: Event Loop Pattern

The most common use of `select` is in a loop, creating an event loop:

```go
for {
    select {
    case event := <-eventCh:
        handleEvent(event)
    case err := <-errorCh:
        handleError(err)
    case <-stopCh:
        return
    }
}
```

This goroutine continuously waits for events, errors, or a stop signal. It's always responsive to whichever arrives first. This is the foundation of many concurrent architectures in Go.

---

## Timeouts {#timeouts}

Timeouts are essential in real-world systems. You can't wait forever for a network response, database query, or external service. `select` combined with `time.After` provides elegant timeout handling.

### The Timeout Pattern

The basic timeout pattern looks like this:

```go
select {
case result := <-ch:
    // Got result in time
    return result
case <-time.After(5 * time.Second):
    // Timeout: took too long
    return errors.New("operation timed out")
}
```

**How It Works**:

`time.After(duration)` returns a channel that receives the current time after the specified duration. So this select says: "Wait for a result from `ch`, but if it takes longer than 5 seconds, give up and return a timeout error."

**Timeline**:

```
T=0s:     Start select
          - Case 1: Waiting for ch
          - Case 2: Waiting for time.After (timer set for 5s)
          
T=0-5s:   Both cases are waiting
          [Goroutine blocked in select]
          
Scenario A: Result arrives at T=2s
          - ch receives a value at T=2s
          - Case 1 becomes ready
          - Select chooses case 1
          - Return result
          - Timer (from time.After) is abandoned
          
Scenario B: No result by T=5s
          - Timer expires at T=5s
          - time.After channel receives time.Time
          - Case 2 becomes ready
          - Select chooses case 2
          - Return timeout error
```

### Why Timeouts Matter

Without timeouts, operations can hang indefinitely:

**Network Requests**: A remote server might be down or overloaded. Without a timeout, your request waits forever, and your goroutine is stuck.

**Database Queries**: A database might be slow or deadlocked. Without a timeout, your application hangs.

**Channel Operations**: If you receive from a channel but nothing sends (due to a bug), you block forever. A timeout detects this.

Timeouts provide two benefits:

**Resource Recovery**: If an operation takes too long, the timeout unblocks the goroutine, which can then free resources (close connections, release locks, etc.).

**User Experience**: Instead of an application that appears frozen, users get an error message ("operation timed out, please try again"), which is much better.

### Common Timeout Patterns

**Pattern 1: Operation with Timeout**

Execute an operation, but give up if it takes too long:

```go
func fetchWithTimeout(url string, timeout time.Duration) ([]byte, error) {
    ch := make(chan []byte, 1)
    
    go func() {
        data, _ := http.Get(url)  // Assume this returns []byte
        ch <- data
    }()
    
    select {
    case data := <-ch:
        return data, nil
    case <-time.After(timeout):
        return nil, errors.New("fetch timeout")
    }
}
```

If the HTTP request completes within `timeout`, you get the data. If it takes too long, you get an error.

**Note**: This doesn't cancel the HTTP request. The goroutine and request continue running even after the timeout. To truly cancel, you need context (covered later).

**Pattern 2: Multiple Operations with Total Timeout**

You have several operations to perform, and the total time mustn't exceed a limit:

```go
func processWithTimeout(items []Item, timeout time.Duration) error {
    deadline := time.After(timeout)
    
    for _, item := range items {
        select {
        case <-deadline:
            return errors.New("processing timeout")
        default:
            // Process item
            if err := process(item); err != nil {
                return err
            }
        }
    }
    
    return nil
}
```

The `deadline` channel fires after `timeout` duration. Each iteration checks if the deadline has passed. If so, stop processing and return an error.

**Pattern 3: Retry with Timeout**

Retry an operation multiple times, but give up after a total timeout:

```go
func retryWithTimeout(fn func() error, timeout time.Duration) error {
    deadline := time.After(timeout)
    ticker := time.NewTicker(1 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-deadline:
            return errors.New("retry timeout")
        case <-ticker.C:
            if err := fn(); err == nil {
                return nil  // Success!
            }
            // Error, will retry on next tick
        }
    }
}
```

This tries to execute `fn()` every second. If `fn()` succeeds, return immediately. If the total time exceeds `timeout`, give up and return an error.

### Timeout Gotcha: Memory Leak

There's a subtle issue with `time.After`:

```go
for {
    select {
    case msg := <-ch:
        process(msg)
    case <-time.After(1 * time.Second):
        // Timeout handling
    }
}
```

**Problem**: Every iteration creates a new timer via `time.After`. Even if `msg` arrives immediately and the timer isn't used, the timer's resources aren't freed until it expires (1 second). If this loop runs frequently, you create many timers, consuming memory.

**Solution**: Use `time.NewTimer` and reset it:

```go
timer := time.NewTimer(1 * time.Second)
defer timer.Stop()

for {
    timer.Reset(1 * time.Second)
    
    select {
    case msg := <-ch:
        if !timer.Stop() {
            <-timer.C  // Drain timer
        }
        process(msg)
    case <-timer.C:
        // Timeout handling
    }
}
```

Now you reuse the same timer, avoiding the memory leak. The timer must be stopped and drained correctly to avoid subtle bugs.

**When to Use Which**:

- **time.After**: One-off timeouts, or infrequent operations. Simple and readable.
- **time.NewTimer**: Loops, or when you need to cancel/reset the timer. More control, slightly more complex.

## Non-Blocking Channel Operations {#non-blocking}

Non-blocking channel operations let you attempt a send or receive without waiting. If the operation can't complete immediately, you do something else instead of blocking.

### The Non-Blocking Pattern

The pattern uses `select` with a `default` case:

```go
select {
case ch <- value:
    // Sent successfully
default:
    // Channel full or no receiver, couldn't send
}
```

If the send can complete immediately (buffer has space or receiver waiting), the first case executes. If the send would block, the default case executes instead.

Similarly for receive:

```go
select {
case msg := <-ch:
    // Received message
default:
    // Channel empty, nothing to receive
}
```

### Why Non-Blocking Operations Matter

In many scenarios, you want to try a channel operation but can't afford to wait:

**Logging**: You want to log a message to a channel-based logger. If the logger's channel is full (it's overwhelmed with messages), you don't want to block your main work. Better to drop the log message and continue.

**Optional Operations**: You're checking for updates. If an update is available (in a channel), great - process it. If not, no problem - continue with other work.

**Work Stealing**: You're a worker checking if other workers have spare work (in a channel). If work is available, steal it. If not, continue with your current work.

### Real-World Example: Non-Blocking Logger

Imagine a high-performance server that logs to a channel. If logging blocks, the server's request handling would slow down. Non-blocking logging ensures logs never slow down the server:

```go
type Logger struct {
    messages chan string
}

func NewLogger(bufferSize int) *Logger {
    l := &Logger{
        messages: make(chan string, bufferSize),
    }
    go l.worker()
    return l
}

func (l *Logger) worker() {
    for msg := range l.messages {
        // Write to file, send to network, etc.
        time.Sleep(10 * time.Millisecond)  // Simulate slow I/O
    }
}

func (l *Logger) Log(msg string) {
    select {
    case l.messages <- msg:
        // Logged successfully
    default:
        // Logger is overwhelmed, drop this message
        // (In production, you might increment a "dropped logs" counter)
    }
}
```

**How This Works**:

Normal operation: `Log()` sends messages to the channel (non-blocking as long as buffer has space). The worker goroutine processes messages at its own pace.

Overwhelmed: If the server generates logs faster than the worker can process, the buffer fills. New `Log()` calls hit the default case and return immediately, dropping messages instead of blocking.

**Trade-off**: Some log messages are dropped under extreme load. But the alternative (blocking) would slow down the entire server, which is worse. You choose to drop logs to maintain server performance.

### Non-Blocking Receive Pattern: Check Without Waiting

You want to check if data is available, but continue immediately if not:

```go
func checkForUpdates(updateCh <-chan Update) *Update {
    select {
    case update := <-updateCh:
        return &update  // Got an update
    default:
        return nil  // No update available
    }
}

// Usage
for {
    update := checkForUpdates(updateCh)
    if update != nil {
        applyUpdate(update)
    }
    
    // Do other work
    doMainWork()
    
    time.Sleep(100 * time.Millisecond)
}
```

The main loop does work, periodically checking for updates. If an update is available, it's processed. If not, the loop continues without blocking.

### Combining Non-Blocking with Timeouts

You can create a pattern that tries non-blocking first, then waits with timeout:

```go
// Try non-blocking first
select {
case result := <-ch:
    return result
default:
    // Didn't get result immediately
}

// Now wait with timeout
select {
case result := <-ch:
    return result
case <-time.After(5 * time.Second):
    return errors.New("timeout")
}
```

This pattern says: "Try to get result immediately. If not available, wait up to 5 seconds. If still not available, timeout."

### Gotcha: Busy-Waiting

A common mistake is using non-blocking operations in a tight loop:

```go
// Bad: Busy-waiting, wastes CPU
for {
    select {
    case msg := <-ch:
        process(msg)
    default:
        // Nothing to do, loop immediately
    }
}
```

This loop runs as fast as possible, constantly checking the channel. It wastes CPU doing nothing useful. The goroutine's CPU usage is 100% even when there's no work.

**Fix**: Add a sleep or use blocking receive:

```go
// Better: Sleep between checks
for {
    select {
    case msg := <-ch:
        process(msg)
    default:
        time.Sleep(10 * time.Millisecond)
    }
}

// Best: Just block (no default case)
for msg := range ch {
    process(msg)
}
```

The blocking version is most efficient - the goroutine consumes zero CPU when waiting.

---

## Closing Channels {#closing-channels}

Closing channels is a powerful signaling mechanism in Go. It's not just about cleanup - it's a way to broadcast to all receivers that no more data is coming.

### What Closing Does

When you close a channel:

```go
close(ch)
```

**Effects**:

1. **No More Sends**: Any attempt to send on a closed channel panics immediately. This is a fatal error.
    
2. **Pending Sends Complete**: If there are goroutines blocked trying to send on the channel, they panic when the channel is closed.
    
3. **Receives Continue**: You can still receive from a closed channel. If the channel has buffered data, receives get that data. Once the buffer is empty, receives return the zero value immediately (and `ok=false` if using two-value receive).
    
4. **Broadcast**: All goroutines waiting to receive from the channel are unblocked simultaneously. They all get the zero value.
    

### The Two-Value Receive: Detecting Closure

To distinguish "received zero value" from "channel is closed," use the two-value receive:

```go
value, ok := <-ch

if ok {
    // Received actual value
    process(value)
} else {
    // Channel is closed and empty
    return
}
```

`ok` is `true` if you received a real value, `false` if the channel is closed and empty.

### Why Close Channels?

Closing serves two main purposes:

**Purpose 1: Signal "No More Data"**

The sender closes the channel to tell receivers "I'm done sending." Receivers know to stop waiting:

```go
// Sender
func producer(ch chan<- int) {
    for i := 0; i < 10; i++ {
        ch <- i
    }
    close(ch)  // "No more data coming"
}

// Receiver
func consumer(ch <-chan int) {
    for {
        value, ok := <-ch
        if !ok {
            // Channel closed, no more data
            return
        }
        process(value)
    }
}
```

Without closing, the receiver doesn't know when to stop. It would wait forever for the 11th value that never comes.

**Purpose 2: Broadcast Signal**

Closing a channel unblocks all goroutines waiting on it. This is useful for signaling multiple goroutines:

```go
done := make(chan struct{})

// Start 5 workers
for i := 0; i < 5; i++ {
    go func(id int) {
        <-done  // Wait for signal
        fmt.Printf("Worker %d starting\n", id)
        // Do work
    }(i)
}

// Signal all workers to start
close(done)  // All 5 goroutines unblock simultaneously
```

Closing `done` broadcasts to all waiting goroutines. This is more efficient than sending 5 times (and you don't need to know how many goroutines are waiting).

### Who Should Close?

**Golden Rule**: Only the sender should close a channel. Never the receiver.

**Why?** If a receiver closes and there are other senders, those senders will panic when they try to send. If multiple receivers try to close, one will succeed and others will panic (closing a closed channel panics).

**Typical Ownership**:

1. **Single Producer**: The producer creates the channel and closes it when done.

```go
ch := make(chan int)
go producer(ch)  // Producer closes
consumer(ch)     // Consumer never closes
```

2. **Multiple Producers**: If multiple goroutines send on the same channel, closing is tricky. Options:
    
    **A. Coordinator**: A coordinating goroutine waits for all producers to finish (sync.WaitGroup), then closes:
    
    ```go
    ch := make(chan int)
    var wg sync.WaitGroup
    
    for i := 0; i < 5; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            // Produce data
        }()
    }
    
    go func() {
        wg.Wait()
        close(ch)  // Close after all producers done
    }()
    ```
    
    **B. Don't Close**: If the channel's lifetime matches the program's lifetime, you don't need to close. The GC will clean it up.
    

### Closing is Optional

You don't need to close every channel. Closing is primarily for signaling. If you don't need to signal, don't close.

**When to Close**:

- When receivers need to know "no more data"
- When you want to broadcast a signal to multiple goroutines
- When you're using `range` over the channel (range needs the channel to close to terminate)

**When Not to Close**:

- Channel's lifetime matches program's lifetime
- Only one receiver, and it knows when to stop by other means
- Multiple senders (closing is complex)

Channels are garbage collected like any other value. You don't need to close for memory cleanup.

### Panic Scenarios

Three operations cause panics:

1. **Send on Closed Channel**:

```go
ch := make(chan int)
close(ch)
ch <- 42  // PANIC: send on closed channel
```

2. **Close Closed Channel**:

```go
ch := make(chan int)
close(ch)
close(ch)  // PANIC: close of closed channel
```

3. **Close Nil Channel**:

```go
var ch chan int  // nil
close(ch)  // PANIC: close of nil channel
```

These panics are by design - they catch programming errors. If you're panicking on channel operations, your ownership and lifecycle management needs review.

---

## Range over Channels {#range-channels}

The `range` keyword provides elegant iteration over channels, automatically handling closure and zero values.

### Basic Range Syntax

```go
for value := range ch {
    process(value)
}
```

This is equivalent to:

```go
for {
    value, ok := <-ch
    if !ok {
        break
    }
    process(value)
}
```

But much cleaner. `range` receives from the channel repeatedly until the channel is closed and empty, then the loop exits.

### How Range Works

**While Channel Open**: `range` receives values and executes the loop body for each value.

**When Channel Closes**: After the channel is closed and all buffered values are received, `range` exits the loop. No panic, no explicit check needed.

**If Channel Never Closes**: `range` blocks forever waiting for the next value. This is usually a bug (deadlock or goroutine leak).

### Why Range is Idiomatic

Range makes consumer code clean and obvious:

**Without Range**:

```go
func consumer(ch <-chan int) {
    for {
        value, ok := <-ch
        if !ok {
            return
        }
        process(value)
    }
}
```

You need to remember to use two-value receive and check `ok`. Easy to forget.

**With Range**:

```go
func consumer(ch <-chan int) {
    for value := range ch {
        process(value)
    }
}
```

Cleaner, less error-prone. The intent is clear: process all values until the channel closes.

### Complete Producer-Consumer Example

```go
// Producer: generates values and closes
func producer(ch chan<- int) {
    for i := 1; i <= 10; i++ {
        ch <- i
    }
    close(ch)  // Signal "no more data"
}

// Consumer: processes all values until channel closes
func consumer(ch <-chan int) {
    for value := range ch {
        fmt.Println("Processing:", value)
        time.Sleep(100 * time.Millisecond)
    }
    fmt.Println("Consumer done (channel closed)")
}

func main() {
    ch := make(chan int, 5)  // Buffered for efficiency
    
    go producer(ch)
    consumer(ch)  // Blocks until channel closes
    
    fmt.Println("Main done")
}
```

**Timeline**:

```
T=0ms:    Main starts
T=0ms:    Producer goroutine starts
T=0ms:    Producer sends 1, 2, 3, 4, 5 (fills buffer)
T=0ms:    Producer blocks (buffer full)
T=0ms:    Consumer starts ranging, receives 1
T=100ms:  Consumer processes 1, receives 2
T=200ms:  Consumer processes 2, receives 3 (producer unblocks, sends 6)
T=300ms:  Consumer processes 3, receives 4 (producer sends 7)
...
T=900ms:  Consumer processes 9, receives 10
T=1000ms: Consumer processes 10, tries to receive again
          Producer has closed channel, range loop exits
T=1000ms: Consumer prints "done", returns
T=1000ms: Main continues
```

The beauty: consumer doesn't need to know how many values to expect. It just processes everything until the channel closes.

### Range with Buffered Channels

Range works with both buffered and unbuffered channels. With buffered channels, the producer can send multiple values before the consumer starts:

```go
ch := make(chan int, 100)

// Producer fills buffer
for i := 0; i < 100; i++ {
    ch <- i
}
close(ch)

// Consumer processes all 100 values
for value := range ch {
    process(value)
}
```

The consumer processes all buffered values, then the loop exits (because the channel is closed and empty).

### Multiple Goroutines Ranging

You can have multiple goroutines ranging over the same channel. They share the values:

```go
ch := make(chan int, 10)

// Start 3 consumers
for i := 0; i < 3; i++ {
    go func(id int) {
        for value := range ch {
            fmt.Printf("Consumer %d got %d\n", id, value)
        }
    }(i)
}

// Producer sends 10 values
for i := 0; i < 10; i++ {
    ch <- i
}
close(ch)

time.Sleep(1 * time.Second)  // Wait for consumers
```

Each value is received by exactly one consumer. They compete for values. This is a simple way to distribute work among multiple workers.

---

## Advanced Patterns and Real-World Examples {#advanced-patterns}

Let's combine everything we've learned into sophisticated, production-ready patterns.

### Pattern 1: Pipeline with Cancellation

A data processing pipeline that can be canceled at any stage:

```go
func pipeline(ctx context.Context, data []int) <-chan int {
    out := make(chan int)
    
    go func() {
        defer close(out)
        
        for _, value := range data {
            select {
            case out <- value * value:  // Send squared value
            case <-ctx.Done():
                return  // Cancel
            }
        }
    }()
    
    return out
}

func main() {
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    data := make([]int, 1000)
    for i := range data {
        data[i] = i
    }
    
    results := pipeline(ctx, data)
    
    for value := range results {
        fmt.Println(value)
        // If this takes too long, context times out
        // and pipeline stops
    }
}
```

**Key Features**:

- Pipeline respects context cancellation
- Closes output channel when done
- Receiver uses range for clean consumption

### Pattern 2: Request-Response with Timeout

A request-response pattern where each request has its own reply channel:

```go
type Request struct {
    ID    int
    Data  string
    Reply chan<- Response
}

type Response struct {
    ID     int
    Result string
}

func server(requests <-chan Request) {
    for req := range requests {
        go func(r Request) {
            // Process request
            time.Sleep(100 * time.Millisecond)
            
            // Send response
            r.Reply <- Response{
                ID:     r.ID,
                Result: "Processed " + r.Data,
            }
        }(req)
    }
}

func client(requests chan<- Request) {
    for i := 0; i < 5; i++ {
        reply := make(chan Response, 1)
        
        // Send request
        requests <- Request{
            ID:    i,
            Data:  fmt.Sprintf("request-%d", i),
            Reply: reply,
        }
        
        // Wait for response with timeout
        select {
        case resp := <-reply:
            fmt.Printf("Got response: %v\n", resp)
        case <-time.After(500 * time.Millisecond):
            fmt.Printf("Request %d timed out\n", i)
        }
    }
}

func main() {
    requests := make(chan Request, 10)
    
    go server(requests)
    
    client(requests)
}
```

**Key Features**:

- Each request has its own reply channel
- Client waits with timeout
- Server processes requests concurrently

### Pattern 3: Fan-Out, Fan-In with Done Channel

Distribute work to multiple workers, collect all results:

```go
func fanOut(in <-chan int, numWorkers int, done <-chan struct{}) []<-chan int {
    outputs := make([]<-chan int, numWorkers)
    
    for i := 0; i < numWorkers; i++ {
        out := make(chan int)
        outputs[i] = out
        
        go func(id int, out chan<- int) {
            defer close(out)
            
            for value := range in {
                select {
                case out <- value * value:
                case <-done:
                    return
                }
            }
        }(i, out)
    }
    
    return outputs
}

func fanIn(inputs []<-chan int, done <-chan struct{}) <-chan int {
    out := make(chan int)
    var wg sync.WaitGroup
    
    for _, input := range inputs {
        wg.Add(1)
        go func(ch <-chan int) {
            defer wg.Done()
            
            for value := range ch {
                select {
                case out <- value:
                case <-done:
                    return
                }
            }
        }(input)
    }
    
    go func() {
        wg.Wait()
        close(out)
    }()
    
    return out
}

func main() {
    // Create input channel
    input := make(chan int)
    done := make(chan struct{})
    
    // Generate input
    go func() {
        defer close(input)
        for i := 0; i < 100; i++ {
            input <- i
        }
    }()
    
    // Fan out to 5 workers
    workers := fanOut(input, 5, done)
    
    // Fan in results
    results := fanIn(workers, done)
    
    // Consume results
    for value := range results {
        fmt.Println(value)
    }
}
```

**Key Features**:

- Work distributed across multiple workers
- Results merged back into single channel
- Respects done signal for clean shutdown

### Pattern 4: Rate Limiter

Limit operations to N per second using channels:

```go
func rateLimiter(requestsPerSecond int) <-chan time.Time {
    limiter := make(chan time.Time, requestsPerSecond)
    
    go func() {
        ticker := time.NewTicker(time.Second / time.Duration(requestsPerSecond))
        defer ticker.Stop()
        
        for t := range ticker.C {
            select {
            case limiter <- t:
            default:
                // Buffer full, skip this tick
            }
        }
    }()
    
    return limiter
}

func main() {
    limiter := rateLimiter(10)  // 10 requests per second
    
    for i := 0; i < 50; i++ {
        <-limiter  // Block until rate limit allows
        
        go func(id int) {
            fmt.Printf("Request %d at %v\n", id, time.Now())
            // Make request
        }(i)
    }
}
```

**How It Works**: The limiter channel receives a token every 100ms (for 10/second). Each request must consume a token. This naturally limits the rate to 10/second.

---

## Summary: Channels as Coordination Primitives

Channels in Go are much more than data pipes. They're sophisticated coordination mechanisms:

**Buffering** provides elasticity, decoupling senders and receivers in time while providing automatic backpressure.

**Synchronization** uses channels for coordination, not just data transfer - waiting for completion, barriers, semaphores.

**Directions** make code safer through the type system and more self-documenting.

**Select** enables multiplexing multiple channels, responding to whichever is ready first.

**Timeouts** prevent operations from hanging indefinitely, essential for production systems.

**Non-Blocking** operations allow checking channels without waiting, useful for logging, polling, and optional operations.

**Closing** broadcasts signals to multiple goroutines and indicates "no more data" to receivers.

**Range** provides clean iteration over channel values until closure.

Together, these features enable elegant concurrent programs that are safer and more maintainable than traditional lock-based approaches. Master these patterns, and you'll write concurrent code that's both efficient and understandable.