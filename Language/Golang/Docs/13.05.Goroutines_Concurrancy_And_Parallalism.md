# The Complete Guide to Concurrency and Parallelism in Go with Goroutines

## Table of Contents

1. [Introduction - The Fundamental Difference](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#introduction)
2. [What is Concurrency?](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#concurrency)
3. [What is Parallelism?](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#parallelism)
4. [Concurrency vs Parallelism](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#concurrency-vs-parallelism)
5. [How Go Achieves Concurrency](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#go-concurrency)
6. [How Go Achieves Parallelism](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#go-parallelism)
7. [GOMAXPROCS - Controlling Parallelism](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#gomaxprocs)
8. [Concurrent Patterns Without Parallelism](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#concurrent-without-parallel)
9. [Parallel Patterns With Concurrency](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#parallel-with-concurrent)
10. [Practical Examples](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#practical-examples)
11. [Performance Analysis](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#performance-analysis)
12. [Best Practices](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#best-practices)

---

## Introduction - The Fundamental Difference {#introduction}

One of the most misunderstood concepts in programming is the difference between concurrency and parallelism. Many developers use these terms interchangeably, but they represent fundamentally different concepts. Understanding this difference is crucial for writing efficient Go programs.

### The Famous Rob Pike Quote

Rob Pike, one of Go's creators, famously said:

> "Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once."

Let's understand what this means through a real-world analogy.

### The Coffee Shop Analogy

**Concurrency (One Barista, Multiple Tasks):**

Imagine a coffee shop with one barista (one CPU core) handling multiple orders:

```
Timeline (One Barista):

Order 1: [Take order] → [Start espresso machine] → wait...
         While espresso brewing:
Order 2:    [Take order] → [Start grinding beans] → wait...
            While grinding:
Order 3:       [Take order] → [Steam milk] → wait...
               While milk steaming:
Order 1:          [Espresso ready, pour] → [Serve]
Order 2:       [Beans ready, brew] → wait...
               While brewing:
Order 3:    [Milk ready, add to coffee] → [Serve]
Order 2: [Coffee ready] → [Serve]

Result: One barista handled 3 orders CONCURRENTLY
        by switching between tasks during waiting periods.
        Only ONE task executing at any moment.
```

**Parallelism (Multiple Baristas, Multiple Tasks):**

Now imagine the same shop with three baristas (three CPU cores):

```
Timeline (Three Baristas):

Barista 1: Order 1 [Take] → [Espresso] → [Pour] → [Serve]
Barista 2: Order 2 [Take] → [Grind] → [Brew] → [Serve]
Barista 3: Order 3 [Take] → [Steam] → [Mix] → [Serve]

Result: Three baristas worked PARALLEL,
        executing THREE tasks simultaneously.
```

### Visual Representation

```
CONCURRENCY (Structure - How you organize code):
┌─────────────────────────────────────────────┐
│ Time →                                      │
│ ═══════════════════════════════════════════ │
│                                             │
│ Task A: ▓▓░░░░░░▓▓░░░░░░▓▓                  │
│ Task B: ░░▓▓░░░░░░▓▓░░░░░░▓▓                │
│ Task C: ░░░░▓▓░░░░░░▓▓░░░░░░▓▓              │
│                                             │
│ One CPU switching between tasks             │
│ ▓ = Executing   ░ = Waiting/Blocked         │
└─────────────────────────────────────────────┘

PARALLELISM (Execution - How code actually runs):
┌─────────────────────────────────────────────┐
│ Time →                                      │
│ ═══════════════════════════════════════════ │
│                                             │
│ CPU 1: ▓▓▓▓▓▓▓▓▓▓▓▓ Task A                  │
│ CPU 2: ▓▓▓▓▓▓▓▓▓▓▓▓ Task B                  │
│ CPU 3: ▓▓▓▓▓▓▓▓▓▓▓▓ Task C                  │
│                                             │
│ Multiple CPUs running simultaneously        │
│ ▓ = Executing (all at the same time!)       │
└─────────────────────────────────────────────┘
```

### Key Insight

**Concurrency** is about the **structure** of your program - how you compose tasks that can make progress independently.

**Parallelism** is about the **execution** - whether tasks actually run at the same physical moment in time.

**You can have:**

- Concurrency WITHOUT parallelism (1 CPU, many goroutines)
- Parallelism WITH concurrency (multiple CPUs, many goroutines)
- You CANNOT have parallelism without concurrency (you need concurrent structure first)

---

## What is Concurrency? {#concurrency}

Concurrency is the composition of independently executing processes (in Go's case, goroutines). It's about structure and design, not execution.

### Concurrency is About Structure

When you write concurrent code, you're expressing that multiple computations can make progress independently, even if they're not running simultaneously.

```go
// Concurrent structure
func main() {
    go task1()  // Can make progress independently
    go task2()  // Can make progress independently
    go task3()  // Can make progress independently
    
    // These tasks are CONCURRENT by design
    // Whether they run in PARALLEL depends on CPUs
}
```

### Why Concurrency Matters

Even on a single CPU, concurrency improves program efficiency by:

1. **Better Resource Utilization**: While one task waits (I/O, network), another task runs
2. **Responsiveness**: UI remains responsive while background work happens
3. **Modularity**: Independent tasks are easier to reason about
4. **Scalability**: Code naturally scales when more CPUs become available

### Concurrency Models

Different languages handle concurrency differently:

```
Thread-Based (Java, C++):
┌─────────────────────────────────────────────┐
│ Programmer creates OS threads               │
│ OS scheduler manages execution              │
│ Heavy: 1-8MB per thread                     │
│ Context switch: 1-2 microseconds            │
│ Limit: ~10,000 threads                      │
└─────────────────────────────────────────────┘

Async/Await (JavaScript, Python):
┌─────────────────────────────────────────────┐
│ Event loop in single thread                 │
│ Callbacks/promises for async operations     │
│ Light: No thread overhead                   │
│ Limitation: CPU-bound code blocks everything│
│ Requires async/await everywhere             │
└─────────────────────────────────────────────┘

Goroutines (Go):
┌─────────────────────────────────────────────┐
│ M:N threading (M goroutines on N OS threads)│
│ Go runtime scheduler                        │
│ Very light: 2KB per goroutine               │
│ Context switch: 0.2 microseconds            │
│ Limit: Millions of goroutines               │
│ Transparent: No async/await needed          │
└─────────────────────────────────────────────┘
```

### Concurrency in Action: I/O Bound Task

Let's see how concurrency helps with I/O-bound tasks:

```go
package main

import (
    "fmt"
    "time"
)

// Simulate downloading a file (I/O-bound, mostly waiting)
func downloadFile(filename string) {
    fmt.Printf("[%v] Starting download: %s\n", time.Now().Format("15:04:05"), filename)
    
    // Simulate network delay (90% of time is waiting)
    time.Sleep(1 * time.Second)
    
    fmt.Printf("[%v] Finished download: %s\n", time.Now().Format("15:04:05"), filename)
}

// Sequential (No concurrency)
func sequentialDownload() {
    start := time.Now()
    
    downloadFile("file1.txt")
    downloadFile("file2.txt")
    downloadFile("file3.txt")
    
    fmt.Printf("Sequential took: %v\n", time.Since(start))
}

// Concurrent (Even on 1 CPU!)
func concurrentDownload() {
    start := time.Now()
    
    done := make(chan bool, 3)
    
    go func() {
        downloadFile("file1.txt")
        done <- true
    }()
    
    go func() {
        downloadFile("file2.txt")
        done <- true
    }()
    
    go func() {
        downloadFile("file3.txt")
        done <- true
    }()
    
    // Wait for all
    for i := 0; i < 3; i++ {
        <-done
    }
    
    fmt.Printf("Concurrent took: %v\n", time.Since(start))
}

func main() {
    fmt.Println("=== Sequential Downloads ===")
    sequentialDownload()
    // Output: ~3 seconds (1s + 1s + 1s)
    
    time.Sleep(100 * time.Millisecond)
    
    fmt.Println("\n=== Concurrent Downloads ===")
    concurrentDownload()
    // Output: ~1 second (all waiting happens "at once")
}
```

**Timeline Visualization:**

```
Sequential Execution (No Concurrency):
┌─────────────────────────────────────────────┐
│ Time:  0s      1s      2s      3s           │
│ CPU:   [file1] [file2] [file3]              │
│        ↓       ↓       ↓                    │
│        waiting waiting waiting              │
│                                             │
│ Total: 3 seconds                            │
└─────────────────────────────────────────────┘

Concurrent Execution (Even on 1 CPU):
┌─────────────────────────────────────────────┐
│ Time:  0s      1s                           │
│ CPU:   [file1] ↓                            │
│        [file2] ↓  All downloading           │
│        [file3] ↓  simultaneously            │
│                                             │
│ Total: 1 second                             │
└─────────────────────────────────────────────┘

Note: All three are waiting for I/O simultaneously.
      The CPU can handle all three because most time
      is spent waiting, not computing.
```

### Concurrency Patterns in Go

Go provides several primitives for concurrent programming:

**1. Goroutines** - Lightweight threads of execution

```go
go func() {
    // This runs concurrently
}()
```

**2. Channels** - Communication between goroutines

```go
ch := make(chan int)
go func() {
    ch <- 42  // Send
}()
value := <-ch  // Receive
```

**3. Select** - Multiplexing on multiple channels

```go
select {
case msg := <-ch1:
    // Handle ch1
case msg := <-ch2:
    // Handle ch2
case <-time.After(1 * time.Second):
    // Timeout
}
```

**4. Sync Primitives** - Mutexes, WaitGroups, etc.

```go
var mu sync.Mutex
var wg sync.WaitGroup
```

---

## What is Parallelism? {#parallelism}

Parallelism is the simultaneous execution of multiple computations. It requires multiple physical CPU cores and is about execution, not structure.

### Parallelism Requires Hardware

Parallelism can only occur when you have multiple CPU cores:

```
Single CPU (No Parallelism Possible):
┌─────────────────────────────────────────────┐
│ CPU Core 1                                  │
│ ┌─────────────────────────────────────────┐ │
│ │ Executes one instruction at a time      │ │
│ │ Task A → Task B → Task A → Task C       │ │
│ └─────────────────────────────────────────┘ │
│                                             │
│ Can do concurrency (switching tasks)        │
│ CANNOT do parallelism (only one core)       │
└─────────────────────────────────────────────┘

Multiple CPUs (Parallelism Possible):
┌────────────────────────────────────────────┐
│ CPU Core 1          CPU Core 2             │
│ ┌───────────────┐  ┌───────────────┐       │
│ │ Task A running│  │ Task B running│       │
│ │ ▓▓▓▓▓▓▓▓▓▓▓▓  │  │ ▓▓▓▓▓▓▓▓▓▓▓▓  │       │
│ └───────────────┘  └───────────────┘       │
│                                            │
│ CPU Core 3          CPU Core 4             │
│ ┌───────────────┐  ┌───────────────┐       │
│ │ Task C running│  │ Task D running│       │
│ │ ▓▓▓▓▓▓▓▓▓▓▓▓  │  │ ▓▓▓▓▓▓▓▓▓▓▓▓  │       │
│ └───────────────┘  └───────────────┘       │
│                                            │
│ All four tasks execute SIMULTANEOUSLY      │
│ This is TRUE PARALLELISM                   │
└────────────────────────────────────────────┘
```

### When Parallelism Helps

Parallelism improves performance for **CPU-bound** tasks - operations that spend most time computing rather than waiting.

**CPU-Bound Examples:**

- Mathematical calculations
- Image/video processing
- Data compression/encryption
- Machine learning inference
- Sorting large datasets

**I/O-Bound Examples (parallelism helps less):**

- Network requests
- File reading/writing
- Database queries
- User input

### Parallelism in Action: CPU-Bound Task

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

// CPU-intensive task: calculate prime numbers
func isPrime(n int) bool {
    if n < 2 {
        return false
    }
    for i := 2; i*i <= n; i++ {
        if n%i == 0 {
            return false
        }
    }
    return true
}

func countPrimes(start, end int) int {
    count := 0
    for i := start; i <= end; i++ {
        if isPrime(i) {
            count++
        }
    }
    return count
}

// Sequential (No parallelism)
func sequentialCount() {
    start := time.Now()
    
    total := countPrimes(1, 1000000)
    
    fmt.Printf("Sequential found %d primes in %v\n", total, time.Since(start))
}

// Parallel (Uses multiple CPUs)
func parallelCount() {
    start := time.Now()
    
    numCPUs := runtime.NumCPU()
    chunkSize := 1000000 / numCPUs
    
    results := make(chan int, numCPUs)
    var wg sync.WaitGroup
    
    for i := 0; i < numCPUs; i++ {
        wg.Add(1)
        
        rangeStart := i*chunkSize + 1
        rangeEnd := (i + 1) * chunkSize
        if i == numCPUs-1 {
            rangeEnd = 1000000
        }
        
        go func(start, end int) {
            defer wg.Done()
            count := countPrimes(start, end)
            results <- count
        }(rangeStart, rangeEnd)
    }
    
    wg.Wait()
    close(results)
    
    total := 0
    for count := range results {
        total += count
    }
    
    fmt.Printf("Parallel (%d CPUs) found %d primes in %v\n", 
        numCPUs, total, time.Since(start))
}

func main() {
    fmt.Printf("Running on %d CPUs\n\n", runtime.NumCPU())
    
    sequentialCount()
    parallelCount()
}
```

**Expected Output (on 4-core CPU):**

```
Running on 4 CPUs

Sequential found 78498 primes in 2.5s
Parallel (4 CPUs) found 78498 primes in 0.7s

Speedup: 3.6x (near-linear scaling!)
```

**Execution Visualization:**

```
Sequential (No Parallelism):
┌─────────────────────────────────────────────┐
│ CPU Core 1: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓       │
│ CPU Core 2: ░░░░░░░░░░░░░░░░░░░░░░░░░ (idle)│
│ CPU Core 3: ░░░░░░░░░░░░░░░░░░░░░░░░░ (idle)│
│ CPU Core 4: ░░░░░░░░░░░░░░░░░░░░░░░░░ (idle)│
│                                             │
│ Time: 2.5 seconds                           │
│ CPU Utilization: 25%                        │
└─────────────────────────────────────────────┘

Parallel (True Parallelism):
┌─────────────────────────────────────────────┐
│ CPU Core 1: ▓▓▓▓▓▓ (1 to 250k)              │
│ CPU Core 2: ▓▓▓▓▓▓ (250k to 500k)           │
│ CPU Core 3: ▓▓▓▓▓▓ (500k to 750k)           │
│ CPU Core 4: ▓▓▓▓▓▓ (750k to 1M)             │
│                                             │
│ Time: 0.7 seconds                           │
│ CPU Utilization: 95%                        │
└─────────────────────────────────────────────┘

All cores working simultaneously!
```

---

## Concurrency vs Parallelism {#concurrency-vs-parallelism}

Let's crystallize the difference with clear examples and diagrams.

### The Fundamental Difference

```
┌─────────────────────────────────────────────┐
│            CONCURRENCY                      │
├─────────────────────────────────────────────┤
│ • About STRUCTURE (how code is organized)   │
│ • Multiple tasks CAN make progress          │
│ • Doesn't require multiple CPUs             │
│ • Deals with lots of things at once         │
│ • Composition of independent processes      │
│ • Design pattern                            │
└─────────────────────────────────────────────┘

┌─────────────────────────────────────────────┐
│            PARALLELISM                      │
├─────────────────────────────────────────────┤
│ • About EXECUTION (how code actually runs)  │
│ • Multiple tasks DO run simultaneously      │
│ • Requires multiple CPUs                    │
│ • Doing lots of things at once              │
│ • Simultaneous execution                    │
│ • Runtime property                          │
└─────────────────────────────────────────────┘
```

### Four Possible Scenarios

**Scenario 1: No Concurrency, No Parallelism**

```go
// Sequential code
func main() {
    task1()
    task2()
    task3()
}
```

```
Execution:
┌─────────────────────────────────────────────┐
│ CPU: [Task1] [Task2] [Task3]                │
│                                             │
│ One task at a time, in order               │
└─────────────────────────────────────────────┘
```

**Scenario 2: Concurrency WITHOUT Parallelism** (1 CPU, multiple goroutines)

```go
func main() {
    runtime.GOMAXPROCS(1)  // Force single CPU
    
    go task1()
    go task2()
    go task3()
}
```

```
Execution on 1 CPU:
┌─────────────────────────────────────────────┐
│ CPU: [T1][T2][T1][T3][T2][T1][T3][T2][T3]   │
│                                             │
│ Tasks interleaved (concurrent structure)    │
│ But only one executes at a time             │
│ (no parallel execution)                     │
└─────────────────────────────────────────────┘
```

**Scenario 3: Parallelism WITH Concurrency** (Multiple CPUs, multiple goroutines)

```go
func main() {
    runtime.GOMAXPROCS(4)  // Use 4 CPUs
    
    go task1()
    go task2()
    go task3()
    go task4()
}
```

```
Execution on 4 CPUs:
┌─────────────────────────────────────────────┐
│ CPU1: [Task1 running]                       │
│ CPU2: [Task2 running]   All running         │
│ CPU3: [Task3 running]   simultaneously!     │
│ CPU4: [Task4 running]                       │
│                                             │
│ Concurrent structure + Parallel execution   │
└─────────────────────────────────────────────┘
```

**Scenario 4: Parallelism WITHOUT Concurrency** (Not possible in practice!)

You cannot have parallelism without some form of concurrent structure. If your code has no mechanism for expressing independent tasks, there's nothing to parallelize.

### Decision Tree

```
Question: Should I use goroutines?
│
├─ Are tasks independent? ────────→ NO ─→ Sequential is fine
│   (Can run without coordinating)
│
└─ YES
    │
    ├─ Mostly waiting (I/O)? ─────→ YES ─→ Concurrency helps!
    │                                      (Even on 1 CPU)
    │
    └─ NO (CPU-intensive)
        │
        ├─ Have multiple CPUs? ───→ YES ─→ Parallelism helps!
        │                                  (Concurrent + Parallel)
        │
        └─ NO (1 CPU only) ───────────→ Limited benefit
                                        (Maybe for structure)
```

## How Go Achieves Concurrency {#go-concurrency}

Go's concurrency model is built on three pillars: goroutines, channels, and the scheduler. Let's understand each.

### Goroutines: Lightweight Concurrent Units

Goroutines are Go's answer to concurrent execution. They're not OS threads - they're much lighter.

```
OS Thread vs Goroutine:

OS Thread:
┌─────────────────────────────────────────────┐
│ Size: 1-8MB stack                           │
│ Creation time: ~20 microseconds             │
│ Context switch: ~1-2 microseconds           │
│ Managed by: OS kernel                       │
│ Limit: ~10,000 per system                   │
└─────────────────────────────────────────────┘

Goroutine:
┌─────────────────────────────────────────────┐
│ Size: 2KB stack (grows dynamically)         │
│ Creation time: ~0.2 microseconds            │
│ Context switch: ~0.2 microseconds           │
│ Managed by: Go runtime                      │
│ Limit: Millions per system                  │
└─────────────────────────────────────────────┘

Goroutines are 100x lighter!
```

### The Go Scheduler: M:N Threading

Go implements M:N threading - M goroutines are multiplexed onto N OS threads.

```
Go's M:N Scheduler:

User Space (Go Runtime):
┌─────────────────────────────────────────────┐
│  1,000,000 Goroutines (G)                   │
│  ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓                        │
│  Scheduled by Go runtime                    │
│  ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓                            │
│  8 Logical Processors (P)                   │
│  ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓                            │
└─────────────────────────────────────────────┘

Kernel Space (Operating System):
┌─────────────────────────────────────────────┐
│  8 OS Threads (M)                           │
│  ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓                            │
│  Scheduled by OS                            │
│  ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓                            │
└─────────────────────────────────────────────┘

Hardware:
┌─────────────────────────────────────────────┐
│  8 CPU Cores                                │
└─────────────────────────────────────────────┘

Key: 1M goroutines → 8 OS threads → 8 CPU cores
```

### How Goroutines Enable Concurrency

When goroutines block (waiting for I/O, channels, etc.), the Go scheduler automatically switches to another runnable goroutine:

```go
package main

import (
    "fmt"
    "time"
)

func worker(id int) {
    fmt.Printf("Worker %d starting\n", id)
    
    // Simulate I/O operation (blocks this goroutine)
    time.Sleep(100 * time.Millisecond)
    
    fmt.Printf("Worker %d done\n", id)
}

func main() {
    // Spawn 10 goroutines
    for i := 1; i <= 10; i++ {
        go worker(i)
    }
    
    // Wait for all to complete
    time.Sleep(200 * time.Millisecond)
}
```

**What happens internally:**

```
Timeline (Even on 1 CPU):

T=0ms:
  Spawn G1-G10 (all goroutines created)
  All are runnable

T=0ms:
  G1 runs → calls Sleep() → BLOCKS
  Scheduler switches to G2

T=0ms:
  G2 runs → calls Sleep() → BLOCKS
  Scheduler switches to G3

T=0ms-100ms:
  All 10 goroutines call Sleep() and block
  All are now in "waiting" state
  CPU is free (can do other work)

T=100ms:
  All Sleep() calls complete
  All goroutines become runnable
  Scheduler runs them one by one

Result: 10 concurrent operations completed
        in ~100ms on a single CPU
        (vs 1000ms sequential)
```

### Channels: Communication Between Goroutines

Channels are Go's way of letting goroutines communicate:

```go
package main

import "fmt"

func producer(ch chan int) {
    for i := 1; i <= 5; i++ {
        fmt.Printf("Producing: %d\n", i)
        ch <- i  // Send to channel
    }
    close(ch)
}

func consumer(ch chan int) {
    for value := range ch {
        fmt.Printf("Consuming: %d\n", value)
    }
}

func main() {
    ch := make(chan int)
    
    go producer(ch)  // Runs concurrently
    consumer(ch)     // Main goroutine consumes
}
```

**Execution flow:**

```
Concurrent Communication:

Producer Goroutine         Main Goroutine (Consumer)
│                          │
├─ Produce 1               │
├─ Send to channel ───────→├─ Receive 1
│                          ├─ Consume 1
├─ Produce 2               │
├─ Send to channel ───────→├─ Receive 2
│                          ├─ Consume 2
├─ Produce 3               │
├─ Send to channel ───────→├─ Receive 3
│                          ├─ Consume 3
...

Both goroutines make progress concurrently
Channel synchronizes their communication
```

---

## How Go Achieves Parallelism {#go-parallelism}

Parallelism in Go happens when multiple goroutines run on multiple CPU cores simultaneously.

### From Concurrency to Parallelism

The same concurrent code runs in parallel when multiple CPUs are available:

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
)

func cpuIntensiveWork(id int, wg *sync.WaitGroup) {
    defer wg.Done()
    
    // Pure CPU work (no waiting)
    sum := 0
    for i := 0; i < 100000000; i++ {
        sum += i
    }
    
    fmt.Printf("Worker %d completed\n", id)
}

func main() {
    numCPUs := runtime.NumCPU()
    fmt.Printf("Available CPUs: %d\n", numCPUs)
    
    var wg sync.WaitGroup
    
    // Spawn one goroutine per CPU
    for i := 0; i < numCPUs; i++ {
        wg.Add(1)
        go cpuIntensiveWork(i, &wg)
    }
    
    wg.Wait()
}
```

**What happens:**

```
With 4 CPUs (GOMAXPROCS=4):

CPU Core 1: [Goroutine 0 executing] ▓▓▓▓▓▓▓▓▓▓
CPU Core 2: [Goroutine 1 executing] ▓▓▓▓▓▓▓▓▓▓
CPU Core 3: [Goroutine 2 executing] ▓▓▓▓▓▓▓▓▓▓
CPU Core 4: [Goroutine 3 executing] ▓▓▓▓▓▓▓▓▓▓

All four goroutines execute in TRUE PARALLEL!
Each has its own CPU core.
```

### When Parallelism Kicks In

Parallelism automatically happens when:

1. **Multiple CPUs available**: System has multiple cores
2. **GOMAXPROCS ≥ 2**: Go runtime uses multiple OS threads
3. **Multiple runnable goroutines**: Enough work to distribute
4. **CPU-bound work**: Goroutines actually need CPU time

```
Decision Flow:

Is GOMAXPROCS > 1?
│
├─ NO ─→ All goroutines run on 1 OS thread
│        Concurrent but NOT parallel
│
└─ YES ─→ Are there multiple runnable goroutines?
          │
          ├─ NO ─→ Nothing to parallelize
          │
          └─ YES ─→ Is work CPU-bound?
                    │
                    ├─ NO (I/O-bound) ─→ Limited parallel benefit
                    │                     (mostly waiting anyway)
                    │
                    └─ YES (CPU-bound) ─→ TRUE PARALLELISM!
                                          Multiple goroutines run
                                          simultaneously on different CPUs
```

---

## GOMAXPROCS - Controlling Parallelism {#gomaxprocs}

`GOMAXPROCS` is the key to controlling how many OS threads Go uses for parallel execution.

### What is GOMAXPROCS?

```go
import "runtime"

// Get current setting
numProcs := runtime.GOMAXPROCS(0)

// Set to specific value
runtime.GOMAXPROCS(4)

// Set to number of CPUs (default since Go 1.5)
runtime.GOMAXPROCS(runtime.NumCPU())
```

**What it controls:**

```
GOMAXPROCS = Number of OS threads that can execute 
             user-level Go code simultaneously

┌─────────────────────────────────────────────┐
│ GOMAXPROCS=1                                │
│ ┌─────────────────────────────────────────┐ │
│ │ One OS thread executes all goroutines   │ │
│ │ Concurrent but NOT parallel             │ │
│ └─────────────────────────────────────────┘ │
└─────────────────────────────────────────────┘

┌─────────────────────────────────────────────┐
│ GOMAXPROCS=4                                │
│ ┌──────────┐ ┌──────────┐                   │
│ │ Thread 1 │ │ Thread 2 │                   │
│ └──────────┘ └──────────┘                   │
│ ┌──────────┐ ┌──────────┐                   │
│ │ Thread 3 │ │ Thread 4 │                   │
│ └──────────┘ └──────────┘                   │
│ Four OS threads can run goroutines in       │
│ parallel on 4 CPU cores                     │
└─────────────────────────────────────────────┘
```

### Impact of GOMAXPROCS

Let's see the dramatic impact:

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

func cpuWork() {
    // Pure CPU computation
    sum := 0
    for i := 0; i < 100000000; i++ {
        sum += i
    }
}

func benchmark(numWorkers, gomaxprocs int) time.Duration {
    runtime.GOMAXPROCS(gomaxprocs)
    
    start := time.Now()
    
    var wg sync.WaitGroup
    for i := 0; i < numWorkers; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            cpuWork()
        }()
    }
    
    wg.Wait()
    return time.Since(start)
}

func main() {
    numCPUs := runtime.NumCPU()
    fmt.Printf("System has %d CPUs\n\n", numCPUs)
    
    workers := numCPUs
    
    // Test with GOMAXPROCS=1
    time1 := benchmark(workers, 1)
    fmt.Printf("GOMAXPROCS=1:  %v\n", time1)
    
    // Test with GOMAXPROCS=NumCPU
    timeN := benchmark(workers, numCPUs)
    fmt.Printf("GOMAXPROCS=%d: %v\n", numCPUs, timeN)
    
    speedup := float64(time1) / float64(timeN)
    fmt.Printf("\nSpeedup: %.2fx\n", speedup)
}
```

**Expected output (on 8-core system):**

```
System has 8 CPUs

GOMAXPROCS=1:  16.2s
GOMAXPROCS=8:  2.1s

Speedup: 7.71x

Analysis:
- With GOMAXPROCS=1: All work serialized
- With GOMAXPROCS=8: Work distributed across 8 cores
- Near-linear speedup (7.71x on 8 cores)
```

### When to Adjust GOMAXPROCS

**Default (recommended):** Let Go use all CPUs

```go
// Since Go 1.5, this is automatic:
runtime.GOMAXPROCS(runtime.NumCPU())
```

**Limit CPU usage:**

```go
// Use only 50% of CPUs (e.g., on shared systems)
runtime.GOMAXPROCS(runtime.NumCPU() / 2)
```

**Force serial execution (for testing):**

```go
// Test concurrent code without parallelism
runtime.GOMAXPROCS(1)
```

**Never exceed NumCPU:**

```go
// This wastes resources (context switching overhead)
// DON'T DO THIS:
runtime.GOMAXPROCS(runtime.NumCPU() * 2)  // ❌ Bad
```

---

## Concurrent Patterns Without Parallelism {#concurrent-without-parallel}

These patterns benefit from concurrency even on a single CPU.

### Pattern 1: Concurrent I/O Operations

```go
package main

import (
    "fmt"
    "runtime"
    "time"
)

func fetchFromAPI(api string, ch chan string) {
    // Simulate API call (mostly waiting)
    time.Sleep(100 * time.Millisecond)
    ch <- fmt.Sprintf("Data from %s", api)
}

func main() {
    runtime.GOMAXPROCS(1)  // Force single CPU
    
    start := time.Now()
    
    apis := []string{"API-1", "API-2", "API-3", "API-4", "API-5"}
    ch := make(chan string, len(apis))
    
    // Launch all fetches concurrently
    for _, api := range apis {
        go fetchFromAPI(api, ch)
    }
    
    // Collect results
    for i := 0; i < len(apis); i++ {
        fmt.Println(<-ch)
    }
    
    elapsed := time.Since(start)
    fmt.Printf("\nCompleted in %v\n", elapsed)
    fmt.Println("(~100ms, not 500ms, even on 1 CPU!)")
}
```

**Why it works on 1 CPU:**

```
Single CPU Execution:

T=0ms:  Launch all 5 goroutines
        Each calls Sleep() and BLOCKS

T=0-100ms: CPU is idle (all goroutines waiting)

T=100ms: All Sleep() calls complete
         All goroutines become runnable
         Scheduler runs them quickly

Result: Total time ≈ 100ms
        (Sequential would be 500ms)

Key: Waiting doesn't use CPU!
     Concurrency allows overlapping waits.
```

### Pattern 2: Event-Driven Architecture

```go
package main

import (
    "fmt"
    "runtime"
    "time"
)

type Event struct {
    Type string
    Data interface{}
}

func eventProcessor(events chan Event) {
    for event := range events {
        fmt.Printf("Processing event: %s\n", event.Type)
        time.Sleep(10 * time.Millisecond)  // Simulate processing
    }
}

func main() {
    runtime.GOMAXPROCS(1)
    
    events := make(chan Event, 100)
    
    // Start multiple processors (concurrent, not parallel)
    for i := 0; i < 5; i++ {
        go eventProcessor(events)
    }
    
    // Generate events
    go func() {
        for i := 0; i < 20; i++ {
            events <- Event{
                Type: fmt.Sprintf("Event-%d", i),
                Data: i,
            }
        }
        close(events)
    }()
    
    time.Sleep(500 * time.Millisecond)
    fmt.Println("\nAll events processed concurrently on 1 CPU")
}
```

---

## Parallel Patterns With Concurrency {#parallel-with-concurrent}

These patterns fully utilize multiple CPUs for maximum performance.

### Pattern 1: Parallel Data Processing

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
)

func processChunk(data []int, start, end int, result chan int, wg *sync.WaitGroup) {
    defer wg.Done()
    
    sum := 0
    for i := start; i < end; i++ {
        sum += data[i] * data[i]  // CPU-intensive
    }
    
    result <- sum
}

func parallelSum(data []int) int {
    numCPUs := runtime.NumCPU()
    runtime.GOMAXPROCS(numCPUs)
    
    chunkSize := len(data) / numCPUs
    results := make(chan int, numCPUs)
    var wg sync.WaitGroup
    
    // Spawn one goroutine per CPU
    for i := 0; i < numCPUs; i++ {
        start := i * chunkSize
        end := start + chunkSize
        if i == numCPUs-1 {
            end = len(data)
        }
        
        wg.Add(1)
        go processChunk(data, start, end, results, &wg)
    }
    
    wg.Wait()
    close(results)
    
    // Combine results
    total := 0
    for sum := range results {
        total += sum
    }
    
    return total
}

func main() {
    data := make([]int, 10000000)
    for i := range data {
        data[i] = i
    }
    
    numCPUs := runtime.NumCPU()
    fmt.Printf("Processing on %d CPUs\n", numCPUs)
    
    result := parallelSum(data)
    fmt.Printf("Result: %d\n", result)
}
```

**Execution visualization:**

```
Parallel Execution on 4 CPUs:

Data: [10,000,000 integers]

CPU 1: Process [0 to 2.5M]     ▓▓▓▓▓▓▓▓▓▓
CPU 2: Process [2.5M to 5M]    ▓▓▓▓▓▓▓▓▓▓
CPU 3: Process [5M to 7.5M]    ▓▓▓▓▓▓▓▓▓▓
CPU 4: Process [7.5M to 10M]   ▓▓▓▓▓▓▓▓▓▓

All CPUs working simultaneously!
True parallel execution.
```

### Pattern 2: Map-Reduce

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
)

// Map: Apply function to each element in parallel
func parallelMap(data []int, fn func(int) int) []int {
    numCPUs := runtime.NumCPU()
    runtime.GOMAXPROCS(numCPUs)
    
    result := make([]int, len(data))
    chunkSize := len(data) / numCPUs
    
    var wg sync.WaitGroup
    
    for i := 0; i < numCPUs; i++ {
        start := i * chunkSize
        end := start + chunkSize
        if i == numCPUs-1 {
            end = len(data)
        }
        
        wg.Add(1)
        go func(start, end int) {
            defer wg.Done()
            
            for j := start; j < end; j++ {
                result[j] = fn(data[j])
            }
        }(start, end)
    }
    
    wg.Wait()
    return result
}

// Reduce: Combine all elements in parallel
func parallelReduce(data []int, fn func(int, int) int) int {
    numCPUs := runtime.NumCPU()
    runtime.GOMAXPROCS(numCPUs)
    
    // Phase 1: Parallel reduction per chunk
    chunkSize := len(data) / numCPUs
    partials := make([]int, numCPUs)
    
    var wg sync.WaitGroup
    
    for i := 0; i < numCPUs; i++ {
        start := i * chunkSize
        end := start + chunkSize
        if i == numCPUs-1 {
            end = len(data)
        }
        
        wg.Add(1)
        go func(cpu, start, end int) {
            defer wg.Done()
            
            acc := data[start]
            for j := start + 1; j < end; j++ {
                acc = fn(acc, data[j])
            }
            partials[cpu] = acc
        }(i, start, end)
    }
    
    wg.Wait()
    
    // Phase 2: Final reduction
    result := partials[0]
    for i := 1; i < len(partials); i++ {
        result = fn(result, partials[i])
    }
    
    return result
}

func main() {
    data := make([]int, 10000000)
    for i := range data {
        data[i] = i + 1
    }
    
    // Map: Square all numbers
    squared := parallelMap(data, func(x int) int {
        return x * x
    })
    
    fmt.Printf("Mapped %d elements\n", len(squared))
    
    // Reduce: Sum all numbers
    sum := parallelReduce(squared, func(a, b int) int {
        return a + b
    })
    
    fmt.Printf("Sum: %d\n", sum)
}
```

This demonstrates the full power of Go's concurrency and parallelism working together!

## Practical Examples {#practical-examples}

Let's build complete, real-world examples that demonstrate both concurrency and parallelism.

### Example 1: Web Scraper (Concurrency-Focused)

This example benefits primarily from concurrency since it's I/O-bound:

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

type Page struct {
    URL     string
    Content string
    Error   error
}

// Simulate fetching a web page (I/O-bound)
func fetchPage(url string) Page {
    // Simulate network latency
    time.Sleep(200 * time.Millisecond)
    
    return Page{
        URL:     url,
        Content: fmt.Sprintf("Content of %s", url),
        Error:   nil,
    }
}

// Sequential scraper
func scrapeSequential(urls []string) []Page {
    pages := make([]Page, 0, len(urls))
    
    for _, url := range urls {
        page := fetchPage(url)
        pages = append(pages, page)
    }
    
    return pages
}

// Concurrent scraper
func scrapeConcurrent(urls []string) []Page {
    pages := make([]Page, len(urls))
    var wg sync.WaitGroup
    
    for i, url := range urls {
        wg.Add(1)
        
        go func(index int, url string) {
            defer wg.Done()
            pages[index] = fetchPage(url)
        }(i, url)
    }
    
    wg.Wait()
    return pages
}

func main() {
    urls := []string{
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3",
        "https://example.com/page4",
        "https://example.com/page5",
        "https://example.com/page6",
        "https://example.com/page7",
        "https://example.com/page8",
        "https://example.com/page9",
        "https://example.com/page10",
    }
    
    fmt.Printf("System CPUs: %d\n", runtime.NumCPU())
    fmt.Printf("URLs to scrape: %d\n\n", len(urls))
    
    // Test with GOMAXPROCS=1 (concurrent, not parallel)
    runtime.GOMAXPROCS(1)
    fmt.Println("=== Concurrent on 1 CPU ===")
    
    start := time.Now()
    scrapeSequential(urls)
    seqTime := time.Since(start)
    fmt.Printf("Sequential: %v\n", seqTime)
    
    start = time.Now()
    scrapeConcurrent(urls)
    concTime := time.Since(start)
    fmt.Printf("Concurrent: %v\n", concTime)
    fmt.Printf("Speedup: %.2fx\n\n", float64(seqTime)/float64(concTime))
    
    // Test with GOMAXPROCS=NumCPU (parallel)
    runtime.GOMAXPROCS(runtime.NumCPU())
    fmt.Println("=== Concurrent on Multiple CPUs ===")
    
    start = time.Now()
    scrapeConcurrent(urls)
    parTime := time.Since(start)
    fmt.Printf("Concurrent: %v\n", parTime)
    
    fmt.Println("\nAnalysis:")
    fmt.Println("- I/O-bound task (mostly waiting)")
    fmt.Println("- Concurrency helps even on 1 CPU")
    fmt.Println("- Additional CPUs provide little extra benefit")
}
```

**Expected output:**

```
System CPUs: 8
URLs to scrape: 10

=== Concurrent on 1 CPU ===
Sequential: 2.0s
Concurrent: 0.2s
Speedup: 10.00x

=== Concurrent on Multiple CPUs ===
Concurrent: 0.2s

Analysis:
- I/O-bound task (mostly waiting)
- Concurrency helps even on 1 CPU
- Additional CPUs provide little extra benefit
```

### Example 2: Image Processing (Parallelism-Focused)

This example benefits from parallelism since it's CPU-bound:

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

type Image struct {
    Width  int
    Height int
    Data   []byte
}

// Simulate CPU-intensive image processing
func processImage(img *Image) *Image {
    // Simulate complex computation
    sum := 0
    for i := 0; i < 10000000; i++ {
        sum += i * i
    }
    
    return &Image{
        Width:  img.Width,
        Height: img.Height,
        Data:   img.Data,
    }
}

// Sequential processing
func processImagesSequential(images []*Image) []*Image {
    results := make([]*Image, len(images))
    
    for i, img := range images {
        results[i] = processImage(img)
    }
    
    return results
}

// Parallel processing
func processImagesParallel(images []*Image, numWorkers int) []*Image {
    results := make([]*Image, len(images))
    
    jobs := make(chan int, len(images))
    var wg sync.WaitGroup
    
    // Start workers
    for w := 0; w < numWorkers; w++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            
            for i := range jobs {
                results[i] = processImage(images[i])
            }
        }()
    }
    
    // Send jobs
    for i := range images {
        jobs <- i
    }
    close(jobs)
    
    wg.Wait()
    return results
}

func main() {
    numCPUs := runtime.NumCPU()
    
    // Create test images
    images := make([]*Image, 20)
    for i := range images {
        images[i] = &Image{
            Width:  1920,
            Height: 1080,
            Data:   make([]byte, 1920*1080*3),
        }
    }
    
    fmt.Printf("System CPUs: %d\n", numCPUs)
    fmt.Printf("Images to process: %d\n\n", len(images))
    
    // Test with GOMAXPROCS=1 (no parallelism)
    runtime.GOMAXPROCS(1)
    fmt.Println("=== Without Parallelism (GOMAXPROCS=1) ===")
    
    start := time.Now()
    processImagesSequential(images)
    seqTime := time.Since(start)
    fmt.Printf("Sequential: %v\n", seqTime)
    
    start = time.Now()
    processImagesParallel(images, numCPUs)
    noParTime := time.Since(start)
    fmt.Printf("Concurrent (1 CPU): %v\n", noParTime)
    fmt.Printf("Speedup: %.2fx\n\n", float64(seqTime)/float64(noParTime))
    
    // Test with GOMAXPROCS=NumCPU (full parallelism)
    runtime.GOMAXPROCS(numCPUs)
    fmt.Printf("=== With Parallelism (GOMAXPROCS=%d) ===\n", numCPUs)
    
    start = time.Now()
    processImagesParallel(images, numCPUs)
    parTime := time.Since(start)
    fmt.Printf("Parallel (%d CPUs): %v\n", numCPUs, parTime)
    fmt.Printf("Speedup vs Sequential: %.2fx\n", float64(seqTime)/float64(parTime))
    fmt.Printf("Speedup vs Concurrent (1 CPU): %.2fx\n\n", float64(noParTime)/float64(parTime))
    
    fmt.Println("Analysis:")
    fmt.Println("- CPU-bound task (pure computation)")
    fmt.Println("- Concurrency alone provides minimal benefit")
    fmt.Println("- Parallelism provides near-linear speedup")
}
```

**Expected output (8-core system):**

```
System CPUs: 8
Images to process: 20

=== Without Parallelism (GOMAXPROCS=1) ===
Sequential: 8.0s
Concurrent (1 CPU): 8.0s
Speedup: 1.00x

=== With Parallelism (GOMAXPROCS=8) ===
Parallel (8 CPUs): 1.1s
Speedup vs Sequential: 7.27x
Speedup vs Concurrent (1 CPU): 7.27x

Analysis:
- CPU-bound task (pure computation)
- Concurrency alone provides minimal benefit
- Parallelism provides near-linear speedup
```

### Example 3: Hybrid Workload (Both Concurrency and Parallelism)

Real-world applications often have both I/O and CPU-bound components:

```go
package main

import (
    "fmt"
    "runtime"
    "sync"
    "time"
)

type Task struct {
    ID     int
    IOTime time.Duration // Simulated I/O wait
    CPUOps int           // CPU-intensive operations
}

func executeTask(task Task) {
    // Phase 1: I/O operation (benefits from concurrency)
    time.Sleep(task.IOTime)
    
    // Phase 2: CPU computation (benefits from parallelism)
    sum := 0
    for i := 0; i < task.CPUOps; i++ {
        sum += i * i
    }
}

func runTasks(tasks []Task, numWorkers int) time.Duration {
    start := time.Now()
    
    jobs := make(chan Task, len(tasks))
    var wg sync.WaitGroup
    
    // Start worker pool
    for w := 0; w < numWorkers; w++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            
            for task := range jobs {
                executeTask(task)
            }
        }()
    }
    
    // Send all tasks
    for _, task := range tasks {
        jobs <- task
    }
    close(jobs)
    
    wg.Wait()
    return time.Since(start)
}

func main() {
    numCPUs := runtime.NumCPU()
    
    // Create hybrid tasks (both I/O and CPU)
    tasks := make([]Task, 20)
    for i := range tasks {
        tasks[i] = Task{
            ID:     i,
            IOTime: 100 * time.Millisecond, // I/O component
            CPUOps: 50000000,                // CPU component
        }
    }
    
    fmt.Printf("System CPUs: %d\n", numCPUs)
    fmt.Printf("Tasks: %d (each has I/O + CPU work)\n\n", len(tasks))
    
    // Test 1: GOMAXPROCS=1
    runtime.GOMAXPROCS(1)
    time1 := runTasks(tasks, 4)
    fmt.Printf("GOMAXPROCS=1, 4 workers: %v\n", time1)
    
    // Test 2: GOMAXPROCS=NumCPU, few workers
    runtime.GOMAXPROCS(numCPUs)
    time2 := runTasks(tasks, 4)
    fmt.Printf("GOMAXPROCS=%d, 4 workers: %v\n", numCPUs, time2)
    
    // Test 3: GOMAXPROCS=NumCPU, many workers
    time3 := runTasks(tasks, numCPUs*2)
    fmt.Printf("GOMAXPROCS=%d, %d workers: %v\n", numCPUs, numCPUs*2, time3)
    
    fmt.Println("\nAnalysis:")
    fmt.Println("- Concurrency helps with I/O phases")
    fmt.Println("- Parallelism helps with CPU phases")
    fmt.Println("- Best performance: Both concurrency AND parallelism")
}
```

---

## Performance Analysis {#performance-analysis}

Let's analyze how different workload types benefit from concurrency and parallelism.

### Workload Classification

```
┌─────────────────────────────────────────────┐
│         I/O-Bound Workload                  │
├─────────────────────────────────────────────┤
│ Characteristics:                            │
│ - Spends 90%+ time waiting                  │
│ - Network calls, disk I/O, database queries │
│ - CPU mostly idle                           │
│                                             │
│ Best Strategy:                              │
│ - High concurrency (1000s of goroutines)    │
│ - GOMAXPROCS=1 often sufficient             │
│ - More CPUs provide diminishing returns     │
│                                             │
│ Examples:                                   │
│ - Web scraping                              │
│ - API aggregation                           │
│ - File downloading                          │
└─────────────────────────────────────────────┘

┌─────────────────────────────────────────────┐
│        CPU-Bound Workload                   │
├─────────────────────────────────────────────┤
│ Characteristics:                            │
│ - Spends 90%+ time computing                │
│ - Math calculations, encryption, encoding   │
│ - CPU constantly busy                       │
│                                             │
│ Best Strategy:                              │
│ - Moderate concurrency (NumCPU goroutines)  │
│ - GOMAXPROCS=NumCPU critical                │
│ - Near-linear scaling with CPU count        │
│                                             │
│ Examples:                                   │
│ - Image processing                          │
│ - Video encoding                            │
│ - Scientific computing                      │
└─────────────────────────────────────────────┘

┌─────────────────────────────────────────────┐
│         Hybrid Workload                     │
├─────────────────────────────────────────────┤
│ Characteristics:                            │
│ - Mix of I/O and CPU work                   │
│ - Both waiting and computing                │
│ - Most real-world applications              │
│                                             │
│ Best Strategy:                              │
│ - High concurrency for I/O phases           │
│ - GOMAXPROCS=NumCPU for CPU phases          │
│ - Balanced worker pool sizing               │
│                                             │
│ Examples:                                   │
│ - Web servers (I/O + business logic)        │
│ - Data pipelines (fetch + process)          │
│ - ETL jobs (extract + transform + load)    │
└─────────────────────────────────────────────┘
```

### Scalability Curves

```
I/O-Bound Task:
Performance
    ↑
    │     ┌─────────── Plateaus quickly
    │    ╱
    │   ╱
    │  ╱
    │ ╱
    └─────────────────────────→ Number of CPUs
    1  2  4  8  16
    
    Concurrency helps greatly
    Parallelism helps little


CPU-Bound Task:
Performance
    ↑
    │                 ╱
    │               ╱  Linear scaling
    │             ╱
    │           ╱
    │         ╱
    │       ╱
    └─────────────────────────→ Number of CPUs
    1  2  4  8  16
    
    Concurrency alone doesn't help
    Parallelism provides linear speedup


Hybrid Task:
Performance
    ↑
    │           ┌───────────── Sub-linear but good
    │         ╱
    │       ╱
    │     ╱
    │   ╱
    │  ╱
    └─────────────────────────→ Number of CPUs
    1  2  4  8  16
    
    Both concurrency AND parallelism help
```

### Amdahl's Law

Even with perfect parallelism, sequential portions limit speedup:

```
Amdahl's Law:
Speedup = 1 / (S + P/N)

Where:
S = Portion of program that's sequential (0-1)
P = Portion that's parallelizable (0-1)
N = Number of processors

Example:
Program is 75% parallelizable (S=0.25, P=0.75)

With 2 CPUs: Speedup = 1 / (0.25 + 0.75/2)  = 1.6x
With 4 CPUs: Speedup = 1 / (0.25 + 0.75/4)  = 2.3x
With 8 CPUs: Speedup = 1 / (0.25 + 0.75/8)  = 2.9x
With ∞ CPUs: Speedup = 1 / 0.25             = 4.0x

Maximum possible speedup: 4x
(Limited by the 25% sequential portion)
```

---

## Best Practices {#best-practices}

Guidelines for effectively using concurrency and parallelism in Go.

### Practice 1: Match Goroutines to Workload

```go
// I/O-Bound: Many goroutines are fine
func handleIOBound() {
    for i := 0; i < 10000; i++ {  // 10,000 goroutines OK!
        go func(id int) {
            fetchFromNetwork(id)   // Mostly waiting
        }(i)
    }
}

// CPU-Bound: Limit to NumCPU
func handleCPUBound() {
    numWorkers := runtime.NumCPU()  // Match CPU count
    
    for i := 0; i < numWorkers; i++ {
        go func() {
            for job := range jobs {
                processCompute(job)  // CPU-intensive
            }
        }()
    }
}
```

### Practice 2: Use Worker Pools for CPU-Bound Tasks

```go
// Good: Fixed worker pool
func goodCPUBound(items []Item) {
    numWorkers := runtime.NumCPU()
    jobs := make(chan Item, len(items))
    
    var wg sync.WaitGroup
    for i := 0; i < numWorkers; i++ {
        wg.Add(1)
        go worker(jobs, &wg)
    }
    
    for _, item := range items {
        jobs <- item
    }
    close(jobs)
    
    wg.Wait()
}

// Bad: Unbounded goroutines
func badCPUBound(items []Item) {
    var wg sync.WaitGroup
    for _, item := range items {
        wg.Add(1)
        go func(item Item) {  // Creates N goroutines!
            defer wg.Done()
            processCPUIntensive(item)
        }(item)
    }
    wg.Wait()
}
```

### Practice 3: Understand When Parallelism Helps

```go
import "runtime"

func processData(data []byte) {
    // Detect workload type
    if isIOBound(data) {
        // I/O-bound: Concurrency is enough
        runtime.GOMAXPROCS(1)  // Optional: save resources
        processConcurrent(data)
    } else {
        // CPU-bound: Use all CPUs
        runtime.GOMAXPROCS(runtime.NumCPU())
        processParallel(data)
    }
}
```

### Practice 4: Measure, Don't Assume

```go
import "testing"

func BenchmarkSequential(b *testing.B) {
    for i := 0; i < b.N; i++ {
        processSequential(data)
    }
}

func BenchmarkConcurrent1CPU(b *testing.B) {
    runtime.GOMAXPROCS(1)
    for i := 0; i < b.N; i++ {
        processConcurrent(data)
    }
}

func BenchmarkParallelAllCPU(b *testing.B) {
    runtime.GOMAXPROCS(runtime.NumCPU())
    for i := 0; i < b.N; i++ {
        processParallel(data)
    }
}

// Run: go test -bench=. -benchmem
```

### Practice 5: Know Your Bottlenecks

```
Decision Tree:

Is your program slow?
│
├─ Profile it!
│  go tool pprof cpu.prof
│
└─ What's the bottleneck?
    │
    ├─ Waiting for I/O?
    │  └─ Add concurrency (goroutines + channels)
    │     GOMAXPROCS doesn't matter much
    │
    ├─ CPU at 100% on one core?
    │  └─ Add parallelism
    │     Increase GOMAXPROCS
    │     Use worker pools
    │
    ├─ Memory allocations?
    │  └─ Reduce goroutine count
    │     Reuse objects (sync.Pool)
    │
    └─ Lock contention?
       └─ Reduce shared state
          Use channels instead of mutexes
```

---

## Summary

### Key Takeaways

**Concurrency:**

- About structure, not execution
- Goroutines can make progress independently
- Works on any number of CPUs (even 1)
- Essential for I/O-bound tasks
- Go's strength: easy to express concurrency

**Parallelism:**

- About execution, requires multiple CPUs
- Multiple tasks run simultaneously
- GOMAXPROCS controls degree of parallelism
- Essential for CPU-bound tasks
- Go's strength: parallelism comes naturally from concurrent code

**The Go Way:**

- Write concurrent code using goroutines
- Parallelism happens automatically when beneficial
- GOMAXPROCS defaults to NumCPU (usually perfect)
- Focus on clear, concurrent structure
- Let the runtime handle optimization

### When to Use Each

```
Use Concurrency When:
✓ Tasks can make independent progress
✓ I/O operations (network, disk, database)
✓ Event-driven systems
✓ Server handling multiple requests
✓ Improving responsiveness

Use Parallelism When:
✓ CPU-intensive computations
✓ Data processing at scale
✓ Image/video processing
✓ Scientific computing
✓ Reducing total execution time
```

### The Power of Go

Go's genius is that you write concurrent code (with goroutines) and parallelism comes for free when you have multiple CPUs. You don't need to think about threads, thread pools, or CPU affinity - just write clear, concurrent code and let Go handle the rest!

```go
// This code automatically benefits from both:
func main() {
    runtime.GOMAXPROCS(runtime.NumCPU())  // Usually automatic
    
    // Concurrent structure
    for i := 0; i < 1000; i++ {
        go processTask(i)
    }
    
    // Go runtime provides:
    // - Concurrency: Goroutines structured to make independent progress
    // - Parallelism: Automatically distributed across all CPUs
}
```

This is why Go is so powerful for building scalable systems!