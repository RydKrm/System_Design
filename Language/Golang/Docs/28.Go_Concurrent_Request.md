# Go Server Under Load: Handling 10,000 Requests Per Second

## Table of Contents

1. [Introduction - High Concurrency Challenge](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#introduction)
2. [Understanding 10,000 RPS](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#understanding-rps)
3. [Go's Concurrency Model at Scale](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#concurrency-model)
4. [Request Flow Under Load](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#request-flow)
5. [Goroutine Scheduling at Scale](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#scheduling)
6. [Memory Management](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#memory-management)
7. [CPU Utilization Patterns](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#cpu-patterns)
8. [Network Stack Behavior](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#network-stack)
9. [Database Connection Pooling](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#database)
10. [Bottlenecks and Solutions](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#bottlenecks)
11. [Real-World Load Test](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#load-test)
12. [Optimization Strategies](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#optimization)

---

## Introduction - High Concurrency Challenge {#introduction}

Imagine a massive concert venue with 10,000 people trying to enter simultaneously. How do you handle it?

**Traditional Approach (Thread per Request)**:

- Hire 10,000 security guards (threads)
- Each guard handles one person
- Each guard needs a desk, chair, equipment (8MB stack)
- Total: 80GB just for the guards!
- Context switching between 10,000 guards = chaos

**Go's Approach (Goroutine per Request)**:

- Hire 8 super-efficient coordinators (OS threads = CPU cores)
- 10,000 lightweight assistants (goroutines, 2KB each)
- Coordinators rapidly switch between assistants
- Total: 20MB for assistants, minimal overhead
- Efficient scheduling = smooth operation

This guide explains exactly how Go serves 10,000 requests per second on modest hardware, with complete visualizations of every component.

---

## Understanding 10,000 RPS {#understanding-rps}

Let's break down what 10,000 requests per second means.

### Request Rate Analysis

```
┌───────────────────────────────────────────────────────────────────────────┐
│              10,000 REQUESTS PER SECOND - WHAT DOES IT MEAN?              │
└───────────────────────────────────────────────────────────────────────────┘

Time Scale Breakdown:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Per Second:    10,000 requests
Per 100ms:      1,000 requests
Per 10ms:         100 requests
Per 1ms:           10 requests
Per 100µs:          1 request

Request Arrival Pattern:
┌────────────────────────────────────────────────────────────────┐
│  1 second = 10,000 requests                                    │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ Time (ms): 0   100   200   300   400   500   600   700   │  │
│  │ Requests:  ↓↓↓ ↓↓↓  ↓↓↓  ↓↓↓  ↓↓↓  ↓↓↓  ↓↓↓  ↓↓↓         │  │
│  │            1K  1K   1K   1K   1K   1K   1K   1K          │  │
│  │                                                          │  │
│  │  Every 100µs: New request arrives                        │  │
│  │  Every 1ms:   10 requests arrive                         │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘

Concurrent Connections at Any Moment:
┌────────────────────────────────────────────────────────────────┐
│  If each request takes 50ms to process:                        │
│                                                                │
│  Concurrent = RPS × Average_Response_Time                      │
│  Concurrent = 10,000 × 0.05s = 500 concurrent requests         │
│                                                                │
│  If each request takes 100ms:                                  │
│  Concurrent = 10,000 × 0.1s = 1,000 concurrent requests        │
│                                                                │
│  If each request takes 200ms (with DB query):                  │
│  Concurrent = 10,000 × 0.2s = 2,000 concurrent requests        │
└────────────────────────────────────────────────────────────────┘

System Requirements:
┌────────────────────────────────────────────────────────────────┐
│  With 2,000 concurrent requests (200ms avg response):          │
│                                                                │
│  Goroutines:   2,000 active                                    │
│  Memory:       2,000 × 10KB = 20MB (very manageable)           │
│  CPU Cores:    8 cores (running 8 goroutines simultaneously)   │
│  Network:      10,000 × 1KB = 10MB/s upload + download         │
│  Database:     25-50 connections (pooled, reused)              │
└────────────────────────────────────────────────────────────────┘
```

### Request Distribution Over Time

```
┌───────────────────────────────────────────────────────────────────────────┐
│              REQUEST ARRIVAL AND PROCESSING (1 SECOND VIEW)               │
└───────────────────────────────────────────────────────────────────────────┘

Time axis (milliseconds):
0ms                                                                    1000ms
├──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┤

Request Arrivals (each dot = 100 requests):
│●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●│
└────────────────────────────────────────────────────────────────┘
  100 requests arrive every 10ms continuously

Active Requests Over Time (assuming 50ms avg processing):
┌───────────────────────────────────────────────────────────────┐
│                                                               │
│  600 ├─────────────────────────────────────────────────       │
│      │                                     ╱╲  ╱╲             │
│  500 ├───────────────────────────────────╱─-─╲╱─-─╲───────────│
│      │                                 ╱           ╲          │
│  400 ├───────────────────────────────╱               ╲───────-│
│      │                             ╱                   ╲      │
│  300 ├───────────────────────────╱                       ╲───-│
│      │                         ╱                           ╲  │
│  200 ├───────────────────────╱                                │
│      │                     ╱    Steady state: ~500 active     │
│  100 ├───────────────────╱                                    │
│      │                 ╱ Ramp up phase                        │
│    0 ├───────────────╱──────────────────────────────────────  │
│      0ms          50ms         500ms                  1000ms  │
└───────────────────────────────────────────────────────────────┘

Phases:
1. Ramp Up (0-50ms): First requests arrive, concurrent builds
2. Steady State (50-1000ms): ~500 concurrent requests at all times
3. Continuous: New requests arrive as old ones complete
```

---

## Go's Concurrency Model at Scale {#concurrency-model}

How does Go handle 2,000 concurrent requests with just 8 CPU cores?

### The GMP Model Under Load

```
┌───────────────────────────────────────────────────────────────────────────┐
│              GO SCHEDULER (GMP) HANDLING 2,000 GOROUTINES                 │
└───────────────────────────────────────────────────────────────────────────┘

Components:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

G = Goroutines (2,000 active)
M = Machine (OS threads, 8 threads matching 8 CPU cores)
P = Processor (Logical processors, 8 to match cores)

Visual Representation:
┌─────────────────────────────────────────────────────────────────┐
│                     2,000 Goroutines (G)                        │
│                                                                 │
│  ┌──┐ ┌──┐ ┌──┐ ┌──┐     ┌──┐ ┌──┐ ┌──┐                         │
│  │G1│ │G2│ │G3│ │G4│ ... │G │ │G │ │G │  ... 2,000 goroutines   │
│  └┬─┘ └┬─┘ └┬─┘ └┬─┘     └┬─┘ └┬─┘ └┬─┘                         │
│   │    │    │    │         │    │    │                          │
│   └────┴────┴────┴─────────┴────┴────┴───┐                      │
│                                           │                     │
│                    ↓ Scheduled onto       │                     │
│                                           │                     │
│              8 Processors (P)             │                     │
│   ┌─────────────────────────────────────┐ │                     │
│   │  P0    P1    P2    P3    P4    P5  │ │  P6    P7            │
│   │  ┌─┐   ┌─┐   ┌─┐   ┌─┐   ┌─┐   ┌─┐│ │  ┌─┐   ┌─┐            │
│   │  │G│   │G│   │G│   │G│   │G│   │G││ │  │G│   │G│            │
│   │  └─┘   └─┘   └─┘   └─┘   └─┘   └─┘│ │  └─┘   └─┘            │
│   │  │     │     │     │     │     │   │ │  │     │             │
│   │  │RunQ │RunQ │RunQ │RunQ │RunQ │RunQ  │RunQ │RunQ           │
│   │  │250G │250G │250G │250G │250G │250G  │250G │250G           │
│   │  └────┬└────┬└────┬└────┬└────┬└────┬─└────┬└────┬          │
│   │       │     │     │     │     │     │      │     │          │
│   └───────┼─────┼─────┼─────┼─────┼─────┼──────┼─────┼───-──────│
│           │     │     │     │     │     │      │     │          │
│           ↓     ↓     ↓     ↓     ↓     ↓      ↓     ↓          │
│                                                                 │
│              8 OS Threads (M) = 8 CPU Cores                     │
│   ┌──────────────────────────────────────────────────────────┐  │
│   │   M0    M1    M2    M3    M4    M5    M6    M7           │  │
│   │   ┌──┐  ┌──┐  ┌──┐  ┌──┐  ┌──┐  ┌──┐  ┌──┐  ┌──┐         │  │
│   │   │██│  │██│  │██│  │██│  │██│  │██│  │██│  │██│         │  │
│   │   └──┘  └──┘  └──┘  └──┘  └──┘  └──┘  └──┘  └──┘         │  │
│   └──────────────────────────────────────────────────────────┘  │
│       │      │      │      │      │      │      │      │        │
│       ↓      ↓      ↓      ↓      ↓      ↓      ↓      ↓        │
│                                                                 │
│                       8 CPU Cores                               │
│   ┌──────────────────────────────────────────────────────────┐  │
│   │  Core0  Core1  Core2  Core3  Core4  Core5  Core6  Core7  │  │
│   │  [███]  [███]  [███]  [███]  [███]  [███]  [███]  [███]  │  │
│   └──────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘

Key Points:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. 2,000 goroutines distributed across 8 run queues (~250 per P)
2. Only 8 goroutines run simultaneously (one per core)
3. Go scheduler rapidly switches goroutines (every ~10ms or on blocking)
4. Blocked goroutines (I/O, DB) don't use CPU - scheduler runs others
5. Fair scheduling - all goroutines get CPU time
```

### Goroutine State Distribution

```
┌───────────────────────────────────────────────────────────────────────────┐
│              2,000 GOROUTINES STATE DISTRIBUTION                          │
└───────────────────────────────────────────────────────────────────────────┘

At any given moment with 2,000 concurrent requests:

┌────────────────────────────────────────────────────────────────┐
│  State          Count    %      What They're Doing             │
├────────────────────────────────────────────────────────────────┤
│  Running          8     0.4%   Executing on CPU cores          │
│                                 (reading, parsing, CPU work)   │
├────────────────────────────────────────────────────────────────┤
│  Runnable       392    19.6%   Waiting for CPU                 │
│                                 (in P run queues)              │
├────────────────────────────────────────────────────────────────┤
│  Waiting:                                                      │
│  - Network     1200    60.0%   Blocked on socket read/write    │
│  - Database     350    17.5%   Blocked on DB query             │
│  - Channel       40     2.0%   Blocked on channel ops          │
│  - Mutex         10     0.5%   Blocked on mutex lock           │
├────────────────────────────────────────────────────────────────┤
│  Total         2000    100%                                    │
└────────────────────────────────────────────────────────────────┘

Visual State Distribution:
┌────────────────────────────────────────────────────────────────┐
│ Running (8):        [███]                                      │
│ Runnable (392):     [█████████████████████]                    │
│ Network Wait (1200):[███████████████████████████████████████]  |
│ Database Wait (350):[████████████████]                         │
│ Other Wait (50):    [██]                                       │
└────────────────────────────────────────────────────────────────┘

CPU Utilization:
┌────────────────────────────────────────────────────────────────┐
│  8 Running + 392 Runnable = 400 goroutines need CPU            │
│  400 goroutines / 8 cores = 50 goroutines per core             │
│  Each goroutine gets ~0.2ms of CPU every 10ms                  │
│  1,600 goroutines blocked (no CPU needed) = 80% waiting        │
│                                                                │
│  Result: CPUs stay busy but not overloaded                     │
└────────────────────────────────────────────────────────────────┘
```

---

## Request Flow Under Load {#request-flow}

Let's trace what happens when 10,000 RPS hits the server.

### Accept Loop Under Load

```
┌───────────────────────────────────────────────────────────────────────────┐
│              ACCEPT LOOP PROCESSING 10,000 RPS                            │
└───────────────────────────────────────────────────────────────────────────┘

Main Goroutine (Accept Loop):

for {
    conn, err := listener.Accept()  // How fast can this go?
    if err != nil {
        continue
    }
    go c.serve(ctx)  // Spawn goroutine
}

Accept() Performance:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Time to accept one connection:  ~10-20µs
Time to spawn goroutine:        ~0.2µs
Total per connection:           ~20µs

Capacity: 1 second / 20µs = 50,000 connections/second
Our load: 10,000 connections/second
Utilization: 20% of accept loop capacity

Timeline (100µs window):
┌───────────────────────────────────────────────────────────────┐
│ T=0µs:   Accept() returns, conn1                              │
│ T=2µs:   Spawn goroutine for conn1                            │
│ T=2µs:   Back to Accept()                                     │
│          ↓                                                    │
│ T=10µs:  New connection arrives in kernel queue               │
│ T=12µs:  Accept() returns, conn2                              │
│ T=14µs:  Spawn goroutine for conn2                            │
│ T=14µs:  Back to Accept()                                     │
│          ↓                                                    │
│ T=24µs:  Accept() returns, conn3                              │
│ T=26µs:  Spawn goroutine for conn3                            │
│          ...                                                  │
│                                                               │
│ In 100µs: ~5 connections accepted and goroutines spawned      │
│ In 1ms:   ~50 connections                                     │
│ In 10ms:  ~500 connections                                    │
│ In 1s:    ~50,000 connections (but we only need 10,000)       │
└───────────────────────────────────────────────────────────────┘

Kernel Connection Queue:
┌────────────────────────────────────────────────────────────────┐
│  SYN Queue (half-open connections):                            │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Max: 128 (backlog parameter)                            │  │
│  │  Current: 5-10 (connections completing handshake)        │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                │
│  Accept Queue (completed connections):                         │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Max: 128                                                │  │
│  │  Current: 2-5 (waiting for Accept() to pick them up)     │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                │
│  Accept() picks from this queue in microseconds                │
│  Queue rarely fills up at 10,000 RPS                           │
└────────────────────────────────────────────────────────────────┘
```

### Goroutine Creation Rate

```
┌───────────────────────────────────────────────────────────────────────────┐
│              GOROUTINE CREATION AT 10,000 RPS                             │
└───────────────────────────────────────────────────────────────────────────┘

Creation Rate:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

10,000 RPS = 10,000 goroutines created per second
            = 10 goroutines per millisecond
            = 1 goroutine every 100µs

Creation Cost:
- Allocate 2KB stack:      ~50ns (nanoseconds!)
- Initialize goroutine:    ~100ns
- Schedule on P:           ~50ns
Total:                     ~200ns = 0.2µs

Memory Allocation Rate:
10,000 goroutines/sec × 2KB = 20MB/sec initial stacks
Goroutine lifetime: ~200ms average
Peak goroutines: 10,000 × 0.2s = 2,000 concurrent

Steady State Memory:
2,000 goroutines × 10KB avg (after growth) = 20MB

Timeline (1 millisecond view):
┌────────────────────────────────────────────────────────────────┐
│ Time        Goroutines Created    Total Active                 │
├────────────────────────────────────────────────────────────────┤
│ T=0µs           1                 2,000 (steady state)         │
│ T=100µs         1                 2,001                        │
│ T=200µs         1                 2,002                        │
│ T=300µs         1                 2,003                        │
│ T=400µs         1                 2,004                        │
│ T=500µs         1                 2,005                        │
│ T=600µs         1                 2,006                        │
│ T=700µs         1                 2,007                        │
│ T=800µs         1                 2,008                        │
│ T=900µs         1                 2,009                        │
│ T=1000µs       10 created total   2,010                        │
│                                                                │
│ Simultaneously, old goroutines complete:                       │
│ ~10 goroutines complete per ms (matching creation rate)        │
│ Result: Steady state maintained at ~2,000                      │
└────────────────────────────────────────────────────────────────┘

Goroutine Pool Over Time:
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│ 2500├────────────────────────────────────────────────────────  │
│     │                                                          │
│ 2000├═══════════════════════════════════════════════════════   │
│     │═════════════ Steady State: 2,000 active ═════════════    │
│ 1500├────────────────────────────────────────────────────────  │
│     │                                                          │
│ 1000├────────╱                                                 │
│     │      ╱  Ramp up                                          │
│  500├────╱                                                     │
│     │  ╱                                                       │
│    0├─┴──────────────────────────────────────────────────────  │
│     0ms    100ms   200ms   500ms              5000ms           │
│                                                                │
│ After initial ramp-up, creation rate = completion rate         │
└────────────────────────────────────────────────────────────────┘
```

### Connection Goroutine Lifecycle

```
┌───────────────────────────────────────────────────────────────────────────┐
│              SINGLE GOROUTINE LIFECYCLE (200ms AVG)                       │
└───────────────────────────────────────────────────────────────────────────┘

Lifetime: 200ms for a request with database query

T=0ms    ┌─────────────────────────────────────┐
         │ Goroutine Created                   │
         │ - Stack: 2KB allocated              │
         │ - State: Runnable                   │
         └──────────────┬──────────────────────┘
                        │
T=1ms    ┌──────────────▼──────────────────────┐
         │ Scheduled on CPU (Running)          │
         │ - Reading HTTP request              │
         │ - CPU work: parsing                 │
         └──────────────┬──────────────────────┘
                        │
T=5ms    ┌──────────────▼──────────────────────┐
         │ Request Parsed                      │
         │ - Route to handler                  │
         │ - Still CPU work                    │
         └──────────────┬──────────────────────┘
                        │
T=10ms   ┌──────────────▼──────────────────────┐
         │ Handler Executing                   │
         │ - Database query starts             │
         │ - Goroutine BLOCKS                  │
         │ - State: Waiting                    │
         │ - CPU freed for other goroutines    │
         └──────────────┬──────────────────────┘
                        │
         │ [150ms waiting on database]         │
         │ [Goroutine uses ZERO CPU]           │
         │ [Scheduler runs other goroutines]   │
                        │
T=160ms  ┌──────────────▼──────────────────────┐
         │ Database Returns                    │
         │ - Goroutine wakes up                │
         │ - State: Runnable                   │
         │ - Wait in run queue for CPU         │
         └──────────────┬──────────────────────┘
                        │
T=161ms  ┌──────────────▼──────────────────────┐
         │ Scheduled on CPU (Running)          │
         │ - Process DB results                │
         │ - Generate JSON response            │
         │ - CPU work: encoding                │
         └──────────────┬──────────────────────┘
                        │
T=165ms  ┌──────────────▼──────────────────────┐
         │ Write Response                      │
         │ - Write to TCP buffer               │
         │ - May block briefly on socket write │
         └──────────────┬──────────────────────┘
                        │
T=170ms  ┌──────────────▼──────────────────────┐
         │ Response Sent                       │
         │ - Check keep-alive                  │
         │ - If HTTP/1.1, loop back to read    │
         └──────────────┬──────────────────────┘
                        │
T=170ms  ┌──────────────▼──────────────────────┐
         │ Read Next Request (Blocking)        │
         │ - State: Waiting (network I/O)      │
         │ - Uses no CPU                       │
         └──────────────┬──────────────────────┘
                        │
         │ [Wait for next request on same conn]│
         │ [Or timeout after 60s]              │

Total Lifetime: 200ms
CPU Time Used: ~10ms (5%)
Wait Time: ~190ms (95%)

Key Insight:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
One goroutine uses only 10ms of CPU over 200ms lifetime.
One CPU core can handle: 200ms / 10ms = 20 goroutines concurrently
8 cores can handle: 8 × 20 = 160 goroutines simultaneously

But we have 2,000 goroutines active!
This works because 90% are WAITING (on I/O, DB), not using CPU.
Only ~200 need CPU at any moment, which 8 cores can handle.
```

## Goroutine Scheduling at Scale {#scheduling}

How does the Go scheduler handle switching between 2,000 goroutines on 8 cores?

### Scheduler Algorithm

```
┌───────────────────────────────────────────────────────────────────────────┐
│              GO SCHEDULER WITH 2,000 GOROUTINES                           │
└───────────────────────────────────────────────────────────────────────────┘

Per-Processor (P) Run Queue:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

2,000 goroutines / 8 processors = 250 goroutines per P (average)

Each P has a local run queue (max 256 goroutines):
┌────────────────────────────────────────────────────────────────┐
│  P0 Run Queue (250 goroutines)                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  [G1][G2][G3][G4]...[G249][G250]                         │  │
│  │   ↑                                                      │  │
│  │   Currently running                                      │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                │
│  When G1 blocks (I/O, DB, channel):                            │
│  1. G1 moved to wait queue                                     │
│  2. G2 starts running immediately                              │
│  3. No context switch to kernel (userspace scheduling)         │
│                                                                │
│  When G1 wakes up:                                             │
│  1. G1 added back to run queue                                 │
│  2. Will run when it reaches front                             │
└────────────────────────────────────────────────────────────────┘

Scheduling Events:
┌────────────────────────────────────────────────────────────────┐
│  Event             Frequency       Action                      │
├────────────────────────────────────────────────────────────────┤
│  Goroutine blocks  ~50,000/sec    Move to wait, run next       │
│  Goroutine wakes   ~50,000/sec    Add to run queue             │
│  Time slice        800/sec        Preempt after 10ms           │
│  System call       varies         Hand off P to another M      │
└────────────────────────────────────────────────────────────────┘
```

### Scheduler Timeline (10ms Window)

```
┌───────────────────────────────────────────────────────────────────────────┐
│              SCHEDULER ACTIVITY (10ms WINDOW ON ONE CORE)                 │
└───────────────────────────────────────────────────────────────────────────┘

CPU Core 0, Processor P0:

Time    Running Goroutine    Event                    State Changes
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

0.0ms   G1                   Parsing request          [Running]
        │
0.5ms   G1                   Makes DB query           G1: Running → Waiting
        └─→ Blocked!         ↓                        G2: Runnable → Running
0.5ms   G2                   Scheduler switches       
        │                    (50µs context switch)    
0.5ms   G2                   Processing request       [Running]
        │
1.2ms   G2                   Socket write blocks      G2: Running → Waiting
        └─→ Blocked!         ↓                        G3: Runnable → Running
1.2ms   G3                   Scheduler switches       
        │
1.2ms   G3                   Encoding JSON            [Running]
        │
2.0ms   G3                   Completes                G3: Running → Dead
        │                    ↓                        G4: Runnable → Running
2.0ms   G4                   Scheduler switches       
        │
        ... Pattern continues ...
        
8.5ms   G1                   DB query returns!        G1: Waiting → Runnable
        │                    (in background)          (added to run queue)
        
9.0ms   G15                  Currently running        [Running]
        │
10.0ms  G15                  Time slice expired       G15: Running → Runnable
        └─→ Preempted!       (forced switch)          G16: Runnable → Running
10.0ms  G16                  Scheduler switches       
        │

Summary of 10ms:
- 15 goroutines ran
- 12 voluntary switches (blocked on I/O)
- 1 preemptive switch (time slice)
- Total switches: 13 in 10ms
- Switch overhead: 13 × 50µs = 650µs (6.5% overhead)
- Productive work: 9.35ms (93.5%)
```

### Work Stealing

```
┌───────────────────────────────────────────────────────────────────────────┐
│              WORK STEALING BETWEEN PROCESSORS                             │
└───────────────────────────────────────────────────────────────────────────┘

Scenario: P0 is busy, P5 becomes idle

Initial State:
┌────────────────────────────────────────────────────────────────┐
│  P0 (Core 0)        P1 (Core 1)      P5 (Core 5)               │
│  ┌──────────────┐   ┌─────────────┐  ┌─────────────┐           │
│  │250 goroutines│   │248 goroutines│ │245 goroutines│          │
│  │ [G][G][G]... │   │ [G][G][G]...│  │ [G][G][G]...│           │
│  │ Very busy    │   │ Busy        │  │ Busy        │           │
│  └──────────────┘   └─────────────┘  └─────────────┘           │
│                                                                │
│                     P7 (Core 7)                                │
│                     ┌─────────────┐                            │
│                     │ 0 goroutines│                            │
│                     │ [IDLE!]     │                            │
│                     └─────────────┘                            │
└────────────────────────────────────────────────────────────────┘

Work Stealing Process:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Step 1: P7 becomes idle (its goroutines finished)
        P7 looks for work

Step 2: P7 randomly selects P0 to steal from

Step 3: P7 steals half of P0's run queue
        ┌────────────────────────────────────┐
        │ P0: 250 goroutines                 │
        │ Steal: 125 goroutines              │
        │ ↓                                  │
        │ P0 keeps: 125 goroutines           │
        │ P7 gets: 125 goroutines            │
        └────────────────────────────────────┘

Step 4: Both P0 and P7 now have work
        ┌────────────────────────────────────┐
        │ P0: [G][G][G]... (125 goroutines)  │
        │ P7: [G][G][G]... (125 goroutines)  │
        └────────────────────────────────────┘

Result:
┌────────────────────────────────────────────────────────────────┐
│ Load balanced automatically                                    │
│ No manual configuration                                        │
│ Happens in microseconds                                        │
│ All cores stay busy                                            │
└────────────────────────────────────────────────────────────────┘

Work Stealing Statistics (1 second at 10,000 RPS):
┌────────────────────────────────────────────────────────────────┐
│ Total work steals: ~100/second                                 │
│ Time per steal: ~10µs                                          │
│ Total overhead: 1ms/second (0.1%)                              │
│                                                                │
│ Benefit: Perfect load balancing across all cores               │
└────────────────────────────────────────────────────────────────┘
```

---

## Memory Management {#memory-management}

How does Go manage memory for 2,000 concurrent goroutines?

### Memory Layout

```
┌───────────────────────────────────────────────────────────────────────────┐
│              MEMORY USAGE AT 10,000 RPS (2,000 CONCURRENT)                │
└───────────────────────────────────────────────────────────────────────────┘

Total Memory Breakdown:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

┌────────────────────────────────────────────────────────────────┐
│  Component              Size        Notes                      │
├────────────────────────────────────────────────────────────────┤
│  Goroutine Stacks      20 MB       2,000 × 10KB avg            │
│  Connection Buffers    16 MB       2,000 × 8KB (4KB×2)         │
│  Request/Response      8 MB        2,000 × 4KB avg             │
│  HTTP Parser State     4 MB        2,000 × 2KB                 │
│  Go Runtime            50 MB       GC, scheduler, etc          │
│  Application Heap      50 MB       Your data structures        │
│  Database Conn Pool    5 MB        25 connections              │
│  Other                 47 MB       Misc overhead               │
├────────────────────────────────────────────────────────────────┤
│  Total                 200 MB      Very efficient!             │
└────────────────────────────────────────────────────────────────┘

Compare to Thread-Per-Request Model:
┌────────────────────────────────────────────────────────────────┐
│  2,000 OS threads × 8MB stack = 16,000 MB = 16 GB              │
│  80x more memory than Go!                                      │
└────────────────────────────────────────────────────────────────┘

Visual Memory Map:
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│  Goroutine Stacks (20MB):  [████████]                          │
│  Connection Buffers (16MB):[██████]                            │
│  Requests/Responses (8MB): [███]                               │
│  Runtime + Heap (100MB):   [████████████████████████████████]  │
│  Other (56MB):             [██████████████]                    │
│                                                                │
│  Total: 200MB                                                  │
└────────────────────────────────────────────────────────────────┘
```

### Goroutine Stack Growth

```
┌───────────────────────────────────────────────────────────────────────────┐
│              GOROUTINE STACK GROWTH MECHANISM                             │
└───────────────────────────────────────────────────────────────────────────┘

Lifecycle of a Goroutine Stack:

Step 1: Initial Allocation (2KB)
┌────────────────────────────────────┐
│  Stack: 2,048 bytes                │
│  ┌──────────────────────────────┐  │
│  │ [Available: 2KB]             │  │
│  │                              │  │
│  │  Enough for: serve() frame   │  │
│  │             + readRequest()  │  │
│  │             + a few vars     │  │
│  └──────────────────────────────┘  │
└────────────────────────────────────┘

Step 2: Stack Usage Grows
┌────────────────────────────────────┐
│  Function calls nest:              │
│  serve() → readRequest()           │
│         → parseHeaders()           │
│         → handler()                │
│         → json.Unmarshal()         │
│                                    │
│  Stack usage: ~3KB needed          │
│  Available: 2KB                    │
│  → Stack overflow imminent!        │
└────────────────────────────────────┘

Step 3: Automatic Growth
┌────────────────────────────────────┐
│  Go runtime detects stack too small│
│  1. Allocate new 4KB stack         │
│  2. Copy existing data             │
│  3. Update pointers                │
│  4. Continue execution             │
│  Time: ~50µs                       │
└────────────────────────────────────┘

Step 4: Final Size (typical)
┌────────────────────────────────────┐
│  Stack: 8-16KB                     │
│  ┌──────────────────────────────┐  │
│  │ [Used: 6KB]                  │  │
│  │ [Free: 10KB]                 │  │
│  │                              │  │
│  │  Plenty of space for deep    │  │
│  │  call stacks                 │  │
│  └──────────────────────────────┘  │
└────────────────────────────────────┘

Stack Size Distribution (2,000 goroutines):
┌────────────────────────────────────────────────────────────────┐
│  Size     Count   Percentage   Total Memory                    │
├────────────────────────────────────────────────────────────────┤
│  2KB      400     20%          800 KB                          │
│  4KB      600     30%          2.4 MB                          │
│  8KB      700     35%          5.6 MB                          │
│  16KB     250     12.5%        4 MB                            │
│  32KB+    50      2.5%         2 MB                            │
├────────────────────────────────────────────────────────────────┤
│  Total    2,000   100%         ~15 MB                          │
└────────────────────────────────────────────────────────────────┘

Average: 7.5KB per goroutine
```

### Garbage Collection Impact

```
┌───────────────────────────────────────────────────────────────────────────┐
│              GARBAGE COLLECTION AT HIGH LOAD                              │
└───────────────────────────────────────────────────────────────────────────┘

GC Triggers:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Heap size: 100MB
Target: 200MB (before GC triggers)
Allocation rate: 50MB/sec (at 10,000 RPS)
GC frequency: Every ~2 seconds

GC Cycle Timeline:
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│  T=0s     Heap: 100MB                                          │
│           Allocations continue...                              │
│                                                                │
│  T=2s     Heap: 200MB (GC threshold reached)                   │
│           ↓                                                    │
│           GC STARTS                                            │
│           ├─ Mark phase: 10ms                                  │
│           │  (Find all reachable objects)                      │
│           │  Uses 25% of CPU                                   │
│           │                                                    │
│           ├─ Concurrent mark: 40ms                             │
│           │  (App continues running)                           │
│           │  Uses 10% of CPU                                   │
│           │                                                    │
│           └─ Sweep: 5ms                                        │
│              (Reclaim unreachable objects)                     │
│              Uses 25% of CPU                                   │
│           ↓                                                    │
│  T=2.055s GC COMPLETE                                          │
│           Heap: 110MB (90MB freed)                             │
│           Back to normal operation                             │
│                                                                │
│  T=4s     Next GC cycle...                                     │
│                                                                │
└────────────────────────────────────────────────────────────────┘

GC Pauses:
┌────────────────────────────────────────────────────────────────┐
│  Stop-the-world pauses: 2 per GC cycle                         │
│  ├─ Mark start: ~0.5ms                                         │
│  └─ Mark termination: ~0.5ms                                   │
│                                                                │
│  Total pause per GC: ~1ms                                      │
│  GC frequency: Every 2s                                        │
│  Pause frequency: 1ms every 2s = 0.05% time paused             │
│                                                                │
│  Impact on requests:                                           │
│  - Most requests: No impact (concurrent GC)                    │
│  - 1-2 requests per GC: +1ms latency (during pause)            │
│  - 99.95% of requests: No GC pause                             │
└────────────────────────────────────────────────────────────────┘

Request Latency During GC:
┌────────────────────────────────────────────────────────────────┐
│           ┌─────────────────────────────────────────┐          │
│  100ms ├──┤                                         │          │
│        │  │        Without GC pauses                │          │
│   50ms ├──┼─────────────────────────────────────────┤          │
│        │  │░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ │ Avg: 50ms│
│   10ms ├──┼─────────────────────────────────────────┤          │
│        │  │▓▓ With GC pause                         │ +1ms     │
│    0ms ├──┼─────────────────────────────────────────┤          │
│           └─────────────────────────────────────────┘          │
│                                                                │
│  GC pause affects <0.1% of requests                            │
└────────────────────────────────────────────────────────────────┘
```

---

## CPU Utilization Patterns {#cpu-patterns}

How are the CPU cores utilized with 10,000 RPS?

### CPU Usage Timeline

```
┌───────────────────────────────────────────────────────────────────────────┐
│              CPU UTILIZATION (8 CORES, 1 SECOND VIEW)                     │
└───────────────────────────────────────────────────────────────────────────┘

Per-Core Utilization:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Core 0:  [████████████████████████████████████████████████] 90%
Core 1:  [███████████████████████████████████████████████ ] 89%
Core 2:  [████████████████████████████████████████████████] 91%
Core 3:  [███████████████████████████████████████████████ ] 88%
Core 4:  [████████████████████████████████████████████████] 92%
Core 5:  [███████████████████████████████████████████████ ] 87%
Core 6:  [████████████████████████████████████████████████] 90%
Core 7:  [███████████████████████████████████████████████ ] 89%

Average: 89.5% (well balanced!)

What's Using the CPU:
┌────────────────────────────────────────────────────────────────┐
│  Activity                  CPU Time    Percentage              │
├────────────────────────────────────────────────────────────────┤
│  HTTP Parsing              1,200ms     15%                     │
│  JSON Encoding/Decoding    2,400ms     30%                     │
│  Handler Logic             1,600ms     20%                     │
│  Network I/O (send/recv)   1,200ms     15%                     │
│  Scheduler Overhead        800ms       10%                     │
│  GC (concurrent)           400ms       5%                      │
│  Other                     400ms       5%                      │
├────────────────────────────────────────────────────────────────┤
│  Total Productive Work     7,200ms     90%                     │
│  Total Overhead            1,200ms     10%                     │
│  Total Capacity            8,000ms     100% (8 cores × 1s)     │
│  Idle Time                 800ms       10%                     │
└────────────────────────────────────────────────────────────────┘

CPU Timeline (100ms window):
┌────────────────────────────────────────────────────────────────┐
│  Time  Core0  Core1  Core2  Core3  Core4  Core5  Core6  Core7  │
├────────────────────────────────────────────────────────────────┤
│  0ms   G1     G5     G9     G13    G17    G21    G25    G29    │
│  10ms  G2     G6     G10    G14    G18    G22    G26    G30    │
│  20ms  G3     G7     G11    G15    G19    G23    G27    G31    │
│  30ms  G4     G8     G12    G16    G20    G24    G28    G32    │
│  ...                                                           │
│  90ms  G10    G14    G18    G22    G26    G30    G34    G38    │
│                                                                │
│  In 100ms: ~40 different goroutines ran                        │
│  Each core switched ~5 times                                   │
└────────────────────────────────────────────────────────────────┘
```

### CPU vs I/O Ratio

```
┌───────────────────────────────────────────────────────────────────────────┐
│              CPU-BOUND VS I/O-BOUND GOROUTINES                            │
└───────────────────────────────────────────────────────────────────────────┘

Goroutine Types:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Type A: Simple Response (10% of requests)
┌────────────────────────────────────────────────────────────────┐
│  Total time: 10ms                                              │
│  ├─ CPU work: 8ms (parsing, encoding)                          │
│  └─ I/O wait: 2ms (network)                                    │
│  CPU ratio: 80%                                                │
└────────────────────────────────────────────────────────────────┘

Type B: Database Query (70% of requests)
┌────────────────────────────────────────────────────────────────┐
│  Total time: 200ms                                             │
│  ├─ CPU work: 15ms (parsing, encoding, handler)                │
│  └─ I/O wait: 185ms (database)                                 │
│  CPU ratio: 7.5%                                               │
└────────────────────────────────────────────────────────────────┘

Type C: Multiple API Calls (20% of requests)
┌────────────────────────────────────────────────────────────────┐
│  Total time: 500ms                                             │
│  ├─ CPU work: 25ms (parsing, encoding, handler)                │
│  └─ I/O wait: 475ms (external APIs)                            │
│  CPU ratio: 5%                                                 │
└────────────────────────────────────────────────────────────────┘

Weighted Average CPU Usage:
┌────────────────────────────────────────────────────────────────┐
│  Type A: 10% × 8ms   = 0.8ms  per request                      │
│  Type B: 70% × 15ms  = 10.5ms per request                      │
│  Type C: 20% × 25ms  = 5.0ms  per request                      │
│  ────────────────────────────────────────────────              │
│  Average: 16.3ms CPU time per request                          │
│                                                                │
│  At 10,000 RPS:                                                │
│  Total CPU needed: 10,000 × 16.3ms = 163,000ms = 163 seconds   │
│  Available CPU: 8 cores × 1 second = 8 seconds                 │
│  Utilization: 163 / 8 = ~20 cores worth of work                │
│                                                                │
│  Wait, that's too much! How does it work?                      │
│                                                                │
│  Answer: Only ~400 goroutines need CPU at once!                │
│  The other 1,600 are waiting on I/O (not using CPU)            │
└────────────────────────────────────────────────────────────────┘

Concurrent Active Goroutines:
┌────────────────────────────────────────────────────────────────┐
│                                                                │
│  2000 ├──────────────────────────────────────────────────────  │
│       │                                                        │
│       │  ▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒ Waiting on I/O (1600)           │
│  1000 ├──────────────────────────────────────────────────────  │
│       │                                                        │
│       │  ████████ Need CPU (400)                               │
│     0 ├──────────────────────────────────────────────────────  │
│                                                                │
│  Only 400 goroutines competing for 8 cores at any moment       │
│  = 50 goroutines per core                                      │
│  Each gets 0.2ms every 10ms (2% of time)                       │
└────────────────────────────────────────────────────────────────┘
```

---

## Network Stack Behavior {#network-stack}

How does the network handle 10,000 RPS?

### Network I/O Pattern

```
┌───────────────────────────────────────────────────────────────────────────┐
│              NETWORK I/O AT 10,000 RPS                                    │
└───────────────────────────────────────────────────────────────────────────┘

Bandwidth Requirements:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Request size:  1KB average (headers + body)
Response size: 2KB average (headers + JSON)
Total per request: 3KB

Per second:
Upload:   10,000 requests × 1KB = 10 MB/s
Download: 10,000 responses × 2KB = 20 MB/s
Total:    30 MB/s = 240 Mbps

Network card capacity: 1 Gbps
Utilization: 240/1000 = 24% (plenty of headroom)

Socket I/O Operations:
┌────────────────────────────────────────────────────────────────┐
│  Operation          Count/sec   Time each   Total time         │
├────────────────────────────────────────────────────────────────┤
│  Accept()           10,000      20µs        200ms              │
│  Read (request)     10,000      100µs       1,000ms = 1s       │
│  Write (response)   10,000      50µs        500ms              │
├────────────────────────────────────────────────────────────────┤
│  Total I/O time                             1.7 seconds        │
└────────────────────────────────────────────────────────────────┘

But we only have 1 second!
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

How it works: CONCURRENCY!
- 2,000 goroutines doing I/O simultaneously
- Each goroutine blocks on its own socket
- Kernel handles all sockets in parallel using epoll/kqueue
- No goroutine waits for another's I/O

Timeline (simplified):
┌────────────────────────────────────────────────────────────────┐
│  T=0ms:    2,000 goroutines all call read() simultaneously     │
│            All block in kernel                                 │
│            ↓                                                   │
│  T=0-100ms: Data arrives for different sockets at different    │
│             times. Kernel wakes up corresponding goroutines    │
│             ↓                                                  │
│  T=100ms:  All 2,000 reads complete (overlapped)               │
│            Effective throughput: 20x serial performance        │
└────────────────────────────────────────────────────────────────┘
```

### Kernel Network Poller

```
┌───────────────────────────────────────────────────────────────────────────┐
│              GO RUNTIME NETWORK POLLER (NETPOLL)                          │
└───────────────────────────────────────────────────────────────────────────┘

Go's network poller integrates with OS kernel:

Linux: epoll
macOS: kqueue  
Windows: IOCP

How It Works:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Step 1: Goroutine tries to read from socket
┌────────────────────────────────────────┐
│  n, err := conn.Read(buf)              │
│  ↓                                     │
│  Socket not ready (EWOULDBLOCK)        │
└────────────────────────────────────────┘

Step 2: Runtime parks the goroutine
┌────────────────────────────────────────┐
│  - Goroutine removed from run queue    │
│  - Added to netpoll wait list          │
│  - Goroutine state: Waiting            │
│  - Uses ZERO CPU                       │
└────────────────────────────────────────┘

Step 3: Runtime registers interest with kernel
┌────────────────────────────────────────┐
│  epoll_ctl(ADD, fd, EPOLLIN)           │
│  "Wake me when this socket is readable"│
└────────────────────────────────────────┘

Step 4: Data arrives on socket
┌────────────────────────────────────────┐
│  - Kernel marks socket as ready        │
│  - epoll_wait() returns                │
│  - Go runtime notified                 │
└────────────────────────────────────────┘

Step 5: Runtime wakes goroutine
┌────────────────────────────────────────┐
│  - Goroutine moved to run queue        │
│  - State: Runnable                     │
│  - Will be scheduled soon              │
└────────────────────────────────────────┘

Step 6: Goroutine runs again
┌────────────────────────────────────────┐
│  conn.Read(buf) completes              │
│  Data copied to buffer                 │
│  Goroutine continues                   │
└────────────────────────────────────────┘

With 2,000 Sockets:
┌────────────────────────────────────────────────────────────────┐
│  Kernel epoll instance tracks 2,000 file descriptors           │
│  Single epoll_wait() call monitors ALL sockets                 │
│  When any socket has data: O(1) lookup                         │
│  No polling, no busy waiting, no wasted CPU                    │
│                                                                │
│  Scalability: Can handle 100,000+ connections                  │
│  with minimal overhead                                         │
└────────────────────────────────────────────────────────────────┘
```

This shows how Go efficiently handles massive concurrency - the key is that most goroutines are WAITING (on I/O, database), not using CPU, so 8 cores can serve thousands of concurrent requests!

## Database Connection Pooling {#database}

Database connections are expensive. How do 2,000 goroutines share 25 connections?

### Connection Pool Architecture

```
┌───────────────────────────────────────────────────────────────────────────┐
│              DATABASE CONNECTION POOL (25 CONNECTIONS)                    │
└───────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│  2,000 Goroutines (Request Handlers)                                    │
│  ┌──┐ ┌──┐ ┌──┐ ┌──┐     ┌──┐ ┌──┐ ┌──┐                                 │
│  │G1│ │G2│ │G3│ │G4│ ... │G │ │G │ │G │  ... 2,000 goroutines           │
│  └┬─┘ └┬─┘ └┬─┘ └┬─┘     └┬─┘ └┬─┘ └┬─┘                                 │
│   │    │    │    │         │    │    │                                  │
│   │ All need database access                                            │
│   └────┴────┴────┴─────────┴────┴────┴──-─┐                             │
│                                           │                             │
│                   ↓ Request connection    │                             │
│                                           │                             │
│              Connection Pool Manager      │                             │
│   ┌───────────────────────────────────────┐                             │
│   │  Available: [━━━━━━━━━] 10 free       │                             │
│   │  In Use:    [████████████] 15 busy    │                             │
│   │  Total:     25 connections            │                             │
│   │                                       │                             │
│   │  Wait Queue: [G1][G2][G3]...[G200]    │  ← 200 goroutines waiting   │
│   └───────────────┬───────────────────────┘                             │
│                   │                                                     │
│                   ↓ Assigned connections                                │
│                                                                         │
│              25 Database Connections                                    │
│   ┌──────────────────────────────────────────────────────────────┐      │
│   │  [C1: G500]  [C2: G501]  [C3: G502]  ... [C25: G524]         │      │
│   │   Active      Active      Active            Active           │      │
│   │                                                              │      │
│   │  [C10: Free] [C11: Free] ... [C15: Free]                     │      │
│   │   Available for next request                                 │      │
│   └──────────────────────────────────────────────────────────────┘      │
│                   │                                                     │
│                   ↓                                                     │
│                                                                         │
│              Database Server (PostgreSQL/MySQL)                         │
│   ┌──────────────────────────────────────────────────────────────┐      │
│   │  Handling 25 concurrent queries                              │      │
│   └──────────────────────────────────────────────────────────────┘      │
└─────────────────────────────────────────────────────────────────────────┘

Pool Configuration:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

db.SetMaxOpenConns(25)      // Max connections to DB
db.SetMaxIdleConns(25)      // Keep all connections open
db.SetConnMaxLifetime(5 * time.Minute)  // Recycle connections

Why 25 connections?
- PostgreSQL default: max_connections = 100
- Reserve 75 for other services
- 25 is optimal for this workload (explained below)
```

### Connection Acquisition Flow

```
┌───────────────────────────────────────────────────────────────────────────┐
│              CONNECTION ACQUISITION TIMELINE                              │
└───────────────────────────────────────────────────────────────────────────┘

Scenario: Goroutine needs database query

T=0ms: Goroutine calls db.Query()
       ┌────────────────────────────────────┐
       │ rows, err := db.Query(sql, args)   │
       └──────────────┬─────────────────────┘
                      │
                      ↓
T=0ms: Pool manager checks availability
       ┌────────────────────────────────────┐
       │ Is connection available?           │
       │                                    │
       │ Case A: YES (10/25 free)           │
       │ ├─ Grab connection                 │
       │ ├─ Mark as in-use                  │
       │ └─ Proceed immediately             │
       │                                    │
       │ Case B: NO (0/25 free)             │
       │ ├─ Add to wait queue               │
       │ ├─ Goroutine BLOCKS                │
       │ └─ Wait for connection release     │
       └────────────────────────────────────┘
                      │
                      ↓ (Case A - connection available)
T=0ms: Execute query on connection
       ┌────────────────────────────────────┐
       │ Send SQL to database               │
       │ Goroutine blocks on network I/O    │
       │ (Does not hold CPU)                │
       └──────────────┬─────────────────────┘
                      │
                      ↓
T=100ms: Database returns results
       ┌────────────────────────────────────┐
       │ Read rows from connection          │
       │ Parse results                      │
       └──────────────┬─────────────────────┘
                      │
                      ↓
T=105ms: Return connection to pool
       ┌────────────────────────────────────┐
       │ rows.Close() or iteration done     │
       │ ├─ Mark connection as free         │
       │ └─ Wake waiting goroutine (if any) │
       └────────────────────────────────────┘
                      │
                      ↓ If goroutines waiting
T=105ms: Waiting goroutine woken
       ┌────────────────────────────────────┐
       │ Goroutine G500 was waiting         │
       │ ├─ Get the freed connection        │
       │ └─ Execute its query               │
       └────────────────────────────────────┘

Timeline with contention (Case B):
┌────────────────────────────────────────────────────────────────┐
│  G1: ├───────Query (100ms)───────┤ Done                        │
│  G2:     ├───Query (100ms)───┤ Done                            │
│  ...                                                           │
│  G25:        ├──Query (100ms)──┤ Done                          │
│  G26:          ▒▒Wait▒▒├─Query─┤  ← Had to wait                │
│  G27:            ▒▒Wait▒▒├─Query─┤                             │
│                                                                │
│  ▒ = Waiting for connection (blocked, no CPU)                  │
│  ─ = Executing query (blocked on I/O, no CPU)                  │
└────────────────────────────────────────────────────────────────┘
```

### Pool Sizing Analysis

```
┌───────────────────────────────────────────────────────────────────────────┐
│              WHY 25 CONNECTIONS IS OPTIMAL                                │
└───────────────────────────────────────────────────────────────────────────┘

Database Query Characteristics:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Average query time: 100ms
70% of requests need database (7,000 queries/sec)
Database can handle: 250 queries/sec per connection

Math:
- 7,000 queries/sec needed
- Each connection handles: 1000ms / 100ms = 10 queries/sec
- Connections needed: 7,000 / 10 = 700 connections (WRONG!)

Wait, that doesn't match! Why only 25?

The key: Concurrent queries, not serial!

Correct calculation:
- At any moment, queries in progress = RPS × query_time
- Concurrent queries = 7,000/sec × 0.1sec = 700 concurrent
- But these are from 2,000 concurrent requests
- Not all 2,000 do DB queries simultaneously
- Average concurrent DB queries: ~400

With 25 connections:
- 25 connections × 10 queries/sec = 250 queries/sec capacity
- But our queries are concurrent!
- 25 connections can handle 25 concurrent queries
- Average wait: (400 - 25) / 25 = 15 goroutines per connection
- Average wait time: 100ms / 15 = 6.7ms extra

Actual Performance:
┌────────────────────────────────────────────────────────────────┐
│  Connections    Avg Latency    Max Queue    Throughput         │
├────────────────────────────────────────────────────────────────┤
│  10             150ms          High         Bottleneck         │
│  25             106ms          Medium       Good               │
│  50             102ms          Low          Diminishing        │
│  100            101ms          None         Overkill           │
└────────────────────────────────────────────────────────────────┘

Why not 100 connections?
1. Each connection uses memory (~10MB): 100 × 10MB = 1GB
2. Database overhead increases with connection count
3. Diminishing returns after 25-50 connections
4. Network round-trip dominates, not connection count
```

### Connection Lifecycle

```
┌───────────────────────────────────────────────────────────────────────────┐
│              DATABASE CONNECTION LIFECYCLE                                │
└───────────────────────────────────────────────────────────────────────────┘

Connection States:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

State 1: IDLE in pool
┌────────────────────────────────────────┐
│  Connection open, authenticated        │
│  Waiting to be used                    │
│  Periodic keep-alive pings             │
└────────────────────────────────────────┘
         │
         ↓ Requested by goroutine
         │
State 2: IN USE (executing query)
┌────────────────────────────────────────┐
│  Assigned to goroutine                 │
│  Executing SQL query                   │
│  Cannot be used by others              │
└────────────────────────────────────────┘
         │
         ↓ Query complete
         │
State 3: RETURNED to pool
┌────────────────────────────────────────┐
│  Cleaned (transaction rolled back)     │
│  Ready for next use                    │
│  Back to IDLE state                    │
└────────────────────────────────────────┘
         │
         ↓ After 5 minutes (ConnMaxLifetime)
         │
State 4: CLOSED
┌────────────────────────────────────────┐
│  Connection closed gracefully          │
│  New connection created to replace it  │
│  Prevents stale connections            │
└────────────────────────────────────────┘

Turnover Rate:
┌────────────────────────────────────────────────────────────────┐
│  Connection lifetime: 5 minutes                                │
│  Pool size: 25                                                 │
│  Turnover: 25 connections / 5 minutes = 5 connections/minute   │
│           = 1 connection every 12 seconds                      │
│                                                                │
│  Impact: Negligible overhead                                   │
└────────────────────────────────────────────────────────────────┘
```

---

## Bottlenecks and Solutions {#bottlenecks}

What limits performance at high concurrency?

### Common Bottlenecks

```
┌───────────────────────────────────────────────────────────────────────────┐
│              PERFORMANCE BOTTLENECKS AT 10,000 RPS                        │
└───────────────────────────────────────────────────────────────────────────┘

1. DATABASE CONNECTION POOL
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Symptom:
  Request latency increases from 50ms → 500ms
  High number of goroutines in "waiting for DB connection" state

Diagnosis:
  ┌────────────────────────────────────────────────────────────┐
  │  $ curl http://localhost:6060/debug/pprof/goroutine        │
  │  2000 goroutines in runtime.chanrecv                       │
  │  (waiting on pool channel)                                 │
  └────────────────────────────────────────────────────────────┘

Solution:
  ├─ Increase MaxOpenConns: 25 → 50
  ├─ Add database read replicas
  ├─ Implement caching (Redis)
  └─ Optimize slow queries

2. FILE DESCRIPTOR LIMIT
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Symptom:
  Server stops accepting connections
  Error: "too many open files"

Diagnosis:
  ┌────────────────────────────────────────────────────────────┐
  │  $ ulimit -n                                               │
  │  1024  ← Default limit, too low!                           │
  │                                                            │
  │  $ lsof -p <pid> | wc -l                                   │
  │  1023  ← At limit!                                         │
  └────────────────────────────────────────────────────────────┘

Calculation:
  Per connection: 1 socket FD
  2,000 connections = 2,000 FDs
  + DB connections: 25 FDs
  + Open log files: 10 FDs
  + Other: 100 FDs
  Total needed: ~2,200 FDs

Solution:
  ┌────────────────────────────────────────────────────────────┐
  │  # /etc/security/limits.conf                               │
  │  * soft nofile 65536                                       │
  │  * hard nofile 65536                                       │
  │                                                            │
  │  Or in systemd service file:                               │
  │  LimitNOFILE=65536                                         │
  └────────────────────────────────────────────────────────────┘

3. CPU SATURATION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Symptom:
  All CPU cores at 100%
  Latency increases
  Goroutines wait in run queues

Diagnosis:
  ┌────────────────────────────────────────────────────────────┐
  │  $ top                                                     │
  │  CPU: 800% (8 cores × 100%)                                │
  │                                                            │
  │  HTTP pprof:                                               │
  │  Lots of goroutines in "runnable" state                    │
  └────────────────────────────────────────────────────────────┘

Solutions:
  ├─ Horizontal scaling: Add more servers
  ├─ Optimize hot code paths
  ├─ Reduce JSON encoding overhead
  ├─ Use faster serialization (protobuf)
  └─ Profile with pprof to find hotspots

4. MEMORY EXHAUSTION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Symptom:
  Server OOM killed
  GC pauses increase
  Allocation rate too high

Diagnosis:
  ┌────────────────────────────────────────────────────────────┐
  │  $ curl http://localhost:6060/debug/pprof/heap             │
  │  Total allocation: 2GB                                     │
  │  Active goroutines: 10,000 (should be 2,000)               │
  │  → Goroutine leak!                                         │
  └────────────────────────────────────────────────────────────┘

Solutions:
  ├─ Fix goroutine leaks
  ├─ Implement request size limits
  ├─ Use sync.Pool for frequent allocations
  └─ Tune GOGC environment variable

5. NETWORK BANDWIDTH
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Symptom:
  10,000 RPS achieved but requests timeout
  Network card at 100%

Diagnosis:
  ┌────────────────────────────────────────────────────────────┐
  │  $ ifstat                                                  │
  │  eth0: 950 Mbps (network at capacity)                      │
  │                                                            │
  │  Request/response size too large                           │
  └────────────────────────────────────────────────────────────┘

Solutions:
  ├─ Enable compression (gzip)
  ├─ Reduce response payload size
  ├─ Use CDN for static assets
  └─ Upgrade network interface (10 Gbps)
```

### Monitoring Dashboard

```
┌───────────────────────────────────────────────────────────────────────────┐
│              HEALTH MONITORING AT 10,000 RPS                              │
└───────────────────────────────────────────────────────────────────────────┘

Key Metrics to Watch:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. Request Rate
   ┌────────────────────────────────────────┐
   │  RPS: 10,000  ██████████████████ 100%  │
   │  Target: 10,000                        │
   │  Status: ✓ HEALTHY                     │
   └────────────────────────────────────────┘

2. Response Time (Latency)
   ┌────────────────────────────────────────┐
   │  P50:   50ms  ✓                        │
   │  P95:  150ms  ✓                        │
   │  P99:  300ms  ⚠ Acceptable             │
   │  P99.9: 1s    ✗ Too high!              │
   └────────────────────────────────────────┘

3. Goroutines
   ┌────────────────────────────────────────┐
   │  Active: 2,000 ████████████████  ✓    │
   │  Target: 2,000                         │
   │  Leaking: No                           │
   └────────────────────────────────────────┘

4. CPU Usage
   ┌────────────────────────────────────────┐
   │  Core 0-7: 85-92%  ████████████████    │
   │  Average: 89%                          │
   │  Status: ✓ HEALTHY                     │
   └────────────────────────────────────────┘

5. Memory
   ┌────────────────────────────────────────┐
   │  Heap: 150MB  ████████         ✓       │
   │  GC Pause: 1ms                         │
   │  GC Frequency: Every 2s                │
   └────────────────────────────────────────┘

6. Database Pool
   ┌────────────────────────────────────────┐
   │  In Use: 23/25  ████████████████       │
   │  Wait Queue: 5  ⚠                     │
   │  Status: Near capacity                 │
   └────────────────────────────────────────┘

7. Error Rate
   ┌────────────────────────────────────────┐
   │  Success: 99.95%  ✓                    │
   │  Errors: 0.05% (5 errors/sec)          │
   │  Status: ✓ HEALTHY                     │
   └────────────────────────────────────────┘
```

---

## Real-World Load Test {#load-test}

Let's run an actual load test and analyze results.

### Load Test Configuration

```go
// Load test server
package main

import (
    "database/sql"
    "encoding/json"
    "fmt"
    "log"
    "net/http"
    "time"
    
    _ "github.com/lib/pq"
)

type Server struct {
    db *sql.DB
}

func main() {
    // Database connection pool
    db, err := sql.Open("postgres", connString)
    if err != nil {
        log.Fatal(err)
    }
    
    db.SetMaxOpenConns(25)
    db.SetMaxIdleConns(25)
    db.SetConnMaxLifetime(5 * time.Minute)
    
    s := &Server{db: db}
    
    // Endpoints
    http.HandleFunc("/health", s.healthHandler)
    http.HandleFunc("/users", s.usersHandler)
    http.HandleFunc("/heavy", s.heavyHandler)
    
    // Server config
    server := &http.Server{
        Addr:         ":8080",
        ReadTimeout:  10 * time.Second,
        WriteTimeout: 10 * time.Second,
        IdleTimeout:  60 * time.Second,
    }
    
    log.Println("Server starting on :8080")
    log.Fatal(server.ListenAndServe())
}

func (s *Server) usersHandler(w http.ResponseWriter, r *http.Request) {
    // Simulate DB query (100ms)
    var users []User
    err := s.db.QueryContext(r.Context(), 
        "SELECT id, name, email FROM users LIMIT 10")
    if err != nil {
        http.Error(w, err.Error(), 500)
        return
    }
    
    json.NewEncoder(w).Encode(users)
}
```

### Load Test Execution

```bash
# Using wrk for load testing
wrk -t12 -c2000 -d60s --latency http://localhost:8080/users

# Explanation:
# -t12: 12 threads generating load
# -c2000: 2000 concurrent connections
# -d60s: Run for 60 seconds
# --latency: Show latency percentiles
```

### Load Test Results

```
┌───────────────────────────────────────────────────────────────────────────┐
│              LOAD TEST RESULTS: 60 SECOND RUN                             │
└───────────────────────────────────────────────────────────────────────────┘

Running 60s test @ http://localhost:8080/users
  12 threads and 2000 connections

Results:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    165.25ms  45.12ms  850.00ms   82.35%
    Req/Sec    851.33    125.42    1.20k    71.23%
    
  Latency Distribution:
     50%   155ms  ← Median: Half of requests faster than this
     75%   185ms
     90%   215ms
     95%   245ms  ← 95% of requests faster than this
     99%   385ms
    99.9%  650ms  ← Outliers
    
  612,442 requests in 60.00s, 125.45MB read
  Requests/sec: 10,207.37  ✓ Achieved target!
  Transfer/sec:   2.09MB

Success Rate: 99.98% (12 errors out of 612,442 requests)

Breakdown:
┌────────────────────────────────────────────────────────────────┐
│  Total Requests: 612,442                                       │
│  Successful: 612,430 (99.98%)                                  │
│  Failed: 12 (0.02%)                                            │
│  Average RPS: 10,207                                           │
│  Peak RPS: 11,500 (at 30s mark)                                │
└────────────────────────────────────────────────────────────────┘

Latency Over Time:
┌────────────────────────────────────────────────────────────────┐
│ 400ms├                              ╱╲                         │
│      │                            ╱    ╲                       │
│ 300ms├                          ╱        ╲                     │
│      │                        ╱            ╲                   │
│ 200ms├──────────────────────╱                ╲───────────────  │
│      │                    ╱  Steady state     ╲                │
│ 100ms├──────────────────╱       ~165ms          ╲────────────  │
│      │              ╱ Ramp up                      Ramp down   │
│   0ms├────────────╱──────────────────────────────────────╲───  │
│      0s        10s     20s    30s    40s    50s    60s         │
└────────────────────────────────────────────────────────────────┘
```

### System Metrics During Load Test

```
┌───────────────────────────────────────────────────────────────────────────┐
│              SYSTEM METRICS DURING LOAD TEST                              │
└───────────────────────────────────────────────────────────────────────────┘

CPU Usage (8 cores):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  Core 0: ████████████████████████████ 91%
  Core 1: ███████████████████████████  89%
  Core 2: ████████████████████████████ 92%
  Core 3: ███████████████████████████  88%
  Core 4: ████████████████████████████ 93%
  Core 5: ███████████████████████████  87%
  Core 6: ████████████████████████████ 90%
  Core 7: ███████████████████████████  89%
  
  Average: 89.9% (well balanced)

Memory Usage:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  Start:  50MB
  Peak:   220MB (at 30s)
  End:    180MB (after GC)
  
  ┌────────────────────────────────────────────────────────────┐
  │ 250MB├─────────────────────────╱╲────────────────────────│
  │      │                       ╱    ╲                      │
  │ 200MB├─────────────────────╱   Peak ╲───────────────────│
  │      │                   ╱            ╲╲                 │
  │ 150MB├─────────────────╱                ╲╲───────────────│
  │      │               ╱                    ╲╲  GC cycles  │
  │ 100MB├─────────────╱                        ╲╲───────────│
  │      │           ╱  Ramp up                   ╲╲         │
  │  50MB├─────────╱──────────────────────────────────╲╲─────│
  │      0s     10s    20s    30s    40s    50s    60s       │
  └────────────────────────────────────────────────────────────┘

Goroutines:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  T=0s:    10 (initial)
  T=10s:   1,800 (ramping up)
  T=20s:   2,050 (steady state)
  T=30s:   2,100 (peak)
  T=40s:   2,000 (steady state)
  T=60s:   50 (ramping down)

Database Connections:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  Idle: 2-5 (fluctuating)
  In Use: 20-23 (steady)
  Max: 25
  Wait Queue: 50-100 goroutines (brief spikes)

Network:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  Incoming: 12 MB/s (requests)
  Outgoing: 24 MB/s (responses)
  Total: 36 MB/s = 288 Mbps
  Capacity: 1 Gbps
  Utilization: 28.8%
```

---

## Optimization Strategies {#optimization}

How to improve performance further?

### Optimization Checklist

```
┌───────────────────────────────────────────────────────────────────────────┐
│              OPTIMIZATION STRATEGIES FOR 10,000+ RPS                      │
└───────────────────────────────────────────────────────────────────────────┘

1. CONNECTION REUSE (HTTP Keep-Alive)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Before: Each request = new TCP connection
  - TCP handshake: 3ms
  - TLS handshake: 20ms (if HTTPS)
  - Total overhead: 23ms per request

After: Keep-Alive enabled (HTTP/1.1 default)
  - First request: 23ms overhead
  - Subsequent requests: 0ms overhead
  - One connection serves 100+ requests

Impact:
  ┌────────────────────────────────────────────────────────┐
  │  Latency reduction: 23ms → 0ms                         │
  │  Throughput increase: 2x-3x                            │
  │  CPU usage: -20%                                       │
  └────────────────────────────────────────────────────────┘

2. RESPONSE COMPRESSION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Middleware:
  func gzipMiddleware(next http.Handler) http.Handler {
      return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
          if strings.Contains(r.Header.Get("Accept-Encoding"), "gzip") {
              gw := gzip.NewWriter(w)
              defer gw.Close()
              
              w.Header().Set("Content-Encoding", "gzip")
              next.ServeHTTP(gzipResponseWriter{Writer: gw, ResponseWriter: w}, r)
          } else {
              next.ServeHTTP(w, r)
          }
      })
  }

Impact:
  ┌────────────────────────────────────────────────────────┐
  │  Response size: 10KB → 2KB (5x smaller)                │
  │  Network bandwidth: -80%                               │
  │  Latency: Varies (CPU trade-off)                       │
  │  - Fast network: +5ms CPU, -10ms network = -5ms total  │
  │  - Slow network: +5ms CPU, -50ms network = -45ms total │
  └────────────────────────────────────────────────────────┘

3. DATABASE QUERY OPTIMIZATION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Add indexes:
  CREATE INDEX idx_users_email ON users(email);
  CREATE INDEX idx_orders_user_id ON orders(user_id);

Use prepared statements:
  stmt, _ := db.Prepare("SELECT * FROM users WHERE id = $1")
  defer stmt.Close()
  // Reuse stmt for multiple queries

Connection pooling optimization:
  db.SetMaxOpenConns(50)        // Increase from 25
  db.SetMaxIdleConns(50)        
  db.SetConnMaxIdleTime(1 * time.Minute)

Impact:
  ┌────────────────────────────────────────────────────────┐
  │  Query time: 100ms → 15ms                              │
  │  Database CPU: -85%                                    │
  │  Throughput: 10,000 → 60,000 RPS                       │
  └────────────────────────────────────────────────────────┘

4. CACHING LAYER (Redis)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Pattern:
  func (s *Server) getUser(id string) (*User, error) {
      // Try cache first
      if cached, err := s.redis.Get("user:" + id); err == nil {
          var user User
          json.Unmarshal([]byte(cached), &user)
          return &user, nil
      }
      
      // Cache miss - query database
      user, err := s.db.QueryUser(id)
      if err != nil {
          return nil, err
      }
      
      // Store in cache
      data, _ := json.Marshal(user)
      s.redis.Set("user:"+id, data, 5*time.Minute)
      
      return user, nil
  }

Impact (80% cache hit rate):
  ┌────────────────────────────────────────────────────────┐
  │  Average latency: 150ms → 25ms                         │
  │  Database load: -80%                                   │
  │  Capacity: 10,000 → 40,000 RPS                         │
  └────────────────────────────────────────────────────────┘

5. JSON OPTIMIZATION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Use faster JSON library:
  import "github.com/bytedance/sonic"  // 2-3x faster
  
  // Instead of:
  json.Marshal(data)
  
  // Use:
  sonic.Marshal(data)

Or use sync.Pool for encoders:
  var encoderPool = sync.Pool{
      New: func() interface{} {
          return json.NewEncoder(nil)
      },
  }

Impact:
  ┌────────────────────────────────────────────────────────┐
  │  JSON encoding: 5ms → 2ms per request                  │
  │  CPU usage: -15%                                       │
  │  Capacity: 10,000 → 11,500 RPS                         │
  └────────────────────────────────────────────────────────┘
```

### Final Optimized Architecture

```
┌───────────────────────────────────────────────────────────────────────────┐
│              OPTIMIZED ARCHITECTURE FOR 50,000 RPS                        │
└───────────────────────────────────────────────────────────────────────────┘

                           Load Balancer
                          (HAProxy/Nginx)
                                │
                    ┌───────────┼───────────┐
                    │           │           │
                    ↓           ↓           ↓
              ┌─────────┐ ┌─────────┐ ┌─────────┐
              │ Go      │ │ Go      │ │ Go      │
              │ Server 1│ │ Server 2│ │ Server 3│  ← 3 instances
              │ 17K RPS │ │ 17K RPS │ │ 17K RPS │    50K RPS total
              └────┬────┘ └────┬────┘ └────┬────┘
                   │           │           │
                   └───────────┼───────────┘
                               │
                    ┌──────────┼──────────┐
                    │          │          │
                    ↓          ↓          ↓
              ┌─────────┐ ┌─────────┐ ┌─────────┐
              │ Redis   │ │ Redis   │ │ Redis   │  ← Cache layer
              │ Master  │ │ Replica1│ │ Replica2│    80% hit rate
              └─────────┘ └─────────┘ └─────────┘
                    │
                    ↓
              ┌─────────────────┐
              │ PostgreSQL      │  ← Database
              │ (Primary)       │    10K queries/sec
              └────────┬────────┘
                       │
              ┌────────┴────────┐
              │                 │
              ↓                 ↓
        ┌─────────┐       ┌─────────┐
        │ Replica1│       │ Replica2│  ← Read replicas
        └─────────┘       └─────────┘    Distribute load

Performance:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
  Throughput: 50,000 RPS
  Latency P50: 15ms
  Latency P95: 45ms
  Latency P99: 85ms
  Success rate: 99.99%
  CPU usage: 70% per server
  Memory: 300MB per server
```

This completes the comprehensive guide on handling high concurrency with Go servers!