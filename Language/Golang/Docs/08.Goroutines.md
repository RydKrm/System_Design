# A Comprehensive Guide to Go Goroutines

## Table of Contents

1. [What are Goroutines?](#what-are-goroutines)
2. [Why We Need Goroutines](#why-we-need-goroutines)
3. [How Goroutines Work](#how-goroutines-work)
4. [Memory Storage and Allocation](#memory-storage-and-allocation)
5. [Complete Workflow: From Creation to Execution](#complete-workflow)
6. [All Goroutine Functionality](#all-goroutine-functionality)
7. [Real-World Examples](#real-world-examples)

---

## What are Goroutines?

Imagine you're cooking dinner. You put water on the stove to boil, while it's boiling you chop vegetables, and while chopping you occasionally stir the pot. You're not doing these tasks in strict sequence (boil water completely, then chop vegetables, then stir), but rather you're switching between them as needed. This is essentially what goroutines do in programming.

A goroutine is a lightweight thread of execution managed by the Go runtime. Unlike traditional operating system threads, goroutines are much lighter and more efficient. Think of them as mini-workers that can perform tasks concurrently within your program.

### The Fundamental Concept

In traditional programming, your code executes line by line, sequentially. If you have three functions to call, they execute one after another:

```
Function A executes → completes → Function B executes → completes → Function C executes
```

With goroutines, you can run these functions concurrently:

```
Function A ──────────────────────→
Function B ─────────────→
Function C ──────────────────────────────→
            (all running at the same time)
```

### Syntax and Basic Usage

Creating a goroutine is remarkably simple. You just add the keyword `go` before a function call:

```go
package main

import (
    "fmt"
    "time"
)

func sayHello() {
    fmt.Println("Hello from goroutine")
}

func main() {
    // Regular function call - executes synchronously
    sayHello()
    
    // Goroutine - executes concurrently
    go sayHello()
    
    // Give goroutine time to execute
    time.Sleep(time.Second)
}
```

---

## Why We Need Goroutines

### The Problem They Solve

Let's understand this through a real-world scenario. Imagine you're building a web server that handles user requests. Without concurrency, your server would work like this:

```
Request 1 arrives → Process (5 seconds) → Respond → 
Request 2 arrives → Process (5 seconds) → Respond →
Request 3 arrives → Process (5 seconds) → Respond
```

If you have 1000 users, and each request takes 5 seconds, the last user would wait nearly 1.4 hours! This is clearly unacceptable.

### How Goroutines Address This

With goroutines, each request can be handled concurrently:

```
Request 1 arrives → go handleRequest(1) ──────────→
Request 2 arrives → go handleRequest(2) ──────────→
Request 3 arrives → go handleRequest(3) ──────────→
(All processing simultaneously)
```

Now all 1000 users get their responses in approximately 5 seconds (assuming sufficient resources).

### Key Advantages of Goroutines

**Lightweight Memory Footprint**: Traditional operating system threads typically require 1-2 MB of memory each. If you want to handle 10,000 concurrent connections, you'd need 10-20 GB just for thread stacks! Goroutines, on the other hand, start with only 2 KB of stack space. This means you can easily run hundreds of thousands of goroutines simultaneously without exhausting your system's memory.

**Fast Creation and Destruction**: Creating an OS thread involves system calls and kernel involvement, which is expensive. Creating a goroutine is a simple memory allocation in user space, managed entirely by the Go runtime. This makes goroutines much faster to create and destroy.

**Efficient Context Switching**: When the operating system switches between threads, it must save and restore a large amount of state information (registers, program counter, stack pointer, etc.). This context switch can take thousands of CPU cycles. Goroutines are managed by the Go scheduler in user space, and switching between them is much faster because the Go runtime knows exactly what needs to be saved and restored.

**Built-in Communication Mechanisms**: Go provides channels, which are type-safe conduits for communication between goroutines. This makes it much easier to write correct concurrent programs compared to traditional thread programming with mutexes and condition variables.

### Diagram: Goroutines vs Traditional Threads

```
Traditional Threading Model:
┌─────────────────────────────────────┐
│     Operating System Kernel         │
├─────────────────────────────────────┤
│  Thread 1  │  Thread 2  │  Thread 3 │
│  (1-2 MB)  │  (1-2 MB)  │  (1-2 MB) │
│     ↓      │     ↓      │     ↓     │
│   Task A   │   Task B   │   Task C  │
└─────────────────────────────────────┘
        Heavy, expensive to manage

Go Goroutine Model:
┌─────────────────────────────────────────────┐
│         Go Runtime Scheduler                │
├─────────────────────────────────────────────┤
│ Goroutine 1 │ Goroutine 2 │ ... │ Goroutine N │
│   (2 KB)    │   (2 KB)    │     │   (2 KB)    │
├─────────────────────────────────────────────┤
│     OS Thread 1  │  OS Thread 2  │ ... │    │
└─────────────────────────────────────────────┘
    Lightweight, efficiently multiplexed
```

---

## How Goroutines Work

### The Go Scheduler: The Orchestra Conductor

The Go scheduler is the brain behind goroutines. It's responsible for managing how goroutines are executed on the available CPU cores. The scheduler uses a model called **M:N scheduling**, which means M goroutines are multiplexed onto N OS threads.

### The GMP Model

Go's scheduler is built on three fundamental concepts, known as the GMP model:

**G (Goroutine)**: This represents an individual goroutine. Each G contains the stack, instruction pointer, and other information needed to track the goroutine's execution state. When you write `go myFunction()`, the Go runtime creates a G structure to represent this goroutine.

**M (Machine/OS Thread)**: This represents an actual OS thread. The M is what actually runs on a CPU core. Think of M as the physical worker that executes code. The Go runtime creates a pool of M's based on your system's capabilities.

**P (Processor/Context)**: This is a resource that an M needs to execute goroutines. You can think of P as a scheduling context that holds a queue of runnable goroutines. The number of P's is typically equal to the number of CPU cores (set by `GOMAXPROCS`).

### Visualization of the GMP Model

```
┌─────────────────────────────────────────────────────────┐
│                    Go Runtime                           │
│                                                         │
│  P (Processor 1)              P (Processor 2)           │
│  ┌──────────────┐             ┌──────────────┐         │
│  │ Run Queue    │             │ Run Queue    │         │
│  │ [G1][G2][G3] │             │ [G4][G5][G6] │         │
│  └──────┬───────┘             └──────┬───────┘         │
│         │                            │                 │
│         ↓                            ↓                 │
│  M (OS Thread 1)              M (OS Thread 2)          │
│  ┌──────────────┐             ┌──────────────┐         │
│  │ Currently    │             │ Currently    │         │
│  │ Running: G1  │             │ Running: G4  │         │
│  └──────┬───────┘             └──────┬───────┘         │
│         │                            │                 │
└─────────┼────────────────────────────┼─────────────────┘
          ↓                            ↓
    ┌──────────┐                 ┌──────────┐
    │  CPU 1   │                 │  CPU 2   │
    └──────────┘                 └──────────┘

Global Run Queue: [G7][G8][G9]... (overflow queue)
```

### The Scheduling Process: Step by Step

When you launch a goroutine, here's what happens behind the scenes:

**Step 1: Goroutine Creation** When you call `go myFunction()`, the Go runtime allocates a G structure. This structure is lightweight, starting with a small stack (typically 2 KB). The G is initially marked as runnable and needs to be scheduled for execution.

**Step 2: Queue Assignment** The scheduler tries to place the new G in a local run queue associated with a P. Each P has its own local run queue that can hold up to 256 goroutines. If the local queue is full, the goroutine goes into a global run queue that all P's can access.

**Step 3: M-P Binding** For a goroutine to actually run, an M (OS thread) must be bound to a P (processor context). The M picks up a goroutine from the P's local run queue and starts executing it.

**Step 4: Execution** The M runs the goroutine's code. The goroutine continues executing until one of several things happens:

- It completes and exits
- It makes a blocking system call
- It's preempted by the scheduler
- It explicitly yields using `runtime.Gosched()`

**Step 5: Context Switching** When a goroutine can't continue (perhaps it's waiting for I/O), the scheduler performs a context switch. It saves the current goroutine's state and loads another goroutine from the run queue. This happens entirely in user space and is very fast.

### Work Stealing Algorithm

One of the clever features of Go's scheduler is work stealing. If a P's local queue becomes empty, it doesn't just sit idle. Instead, it tries to "steal" goroutines from other P's queues or from the global queue. This helps balance the workload across all CPU cores.

```
P1 has work:  [G1][G2][G3][G4][G5]
P2 is idle:   []

Work Stealing:
P1: [G1][G2]
P2: [G3][G4][G5]  ← Stole half of P1's work
```

### Blocking Operations

When a goroutine makes a blocking system call (like reading from a file or network), something interesting happens:

1. The M (OS thread) becomes blocked at the OS level
2. The Go runtime detects this and detaches the P from the blocked M
3. The P is then attached to another M (or a new M is created if needed)
4. Other goroutines continue running without interruption

```
Before blocking call:
M1 ← P1 (running G1)

G1 makes blocking syscall:
M1 (blocked with G1)
M2 ← P1 (continues with other goroutines)

After syscall completes:
G1 is placed back in run queue
M1 returns to thread pool
```

---

## Memory Storage and Allocation

### The Goroutine Stack

Every goroutine has its own stack, which is a region of memory used for local variables, function calls, and return addresses. Understanding how this stack works is crucial to understanding goroutines' efficiency.

**Initial Stack Size**: When a goroutine is created, the Go runtime allocates a small stack for it, typically 2 KB (2048 bytes). This is remarkably small compared to traditional thread stacks which start at 1-2 MB. This small initial size is possible because Go uses a technique called "stack growth" or "segmented stacks."

**Stack Growth Mechanism**: As a goroutine executes and needs more stack space (for example, when calling deeply nested functions), the Go runtime can automatically grow the stack. Here's how this works:

1. Each function call checks if there's enough stack space remaining
2. If not, the runtime allocates a new, larger stack (typically double the size)
3. The contents of the old stack are copied to the new stack
4. Pointers are adjusted to point to the new locations
5. The old stack is freed

This means a goroutine starts small but can grow to accommodate its needs, up to a maximum limit (typically 1 GB on 64-bit systems).

### Diagram: Goroutine Stack Growth

```
Initial Creation (2 KB):
┌─────────────────┐
│   Goroutine G1  │
│   Stack: 2 KB   │
│  ┌───────────┐  │
│  │func main  │  │
│  └───────────┘  │
└─────────────────┘

After Deep Function Calls (4 KB):
┌─────────────────────┐
│    Goroutine G1     │
│    Stack: 4 KB      │
│  ┌──────────────┐   │
│  │ func nested3 │   │
│  ├──────────────┤   │
│  │ func nested2 │   │
│  ├──────────────┤   │
│  │ func nested1 │   │
│  ├──────────────┤   │
│  │  func main   │   │
│  └──────────────┘   │
└─────────────────────┘

Stack grows automatically as needed
```

### Goroutine Memory Layout

A goroutine's complete memory footprint includes several components:

**The G Structure**: This is the goroutine descriptor maintained by the runtime. It's approximately 376 bytes and contains metadata about the goroutine including its stack bounds, current stack pointer, goroutine ID, status (running, runnable, waiting), and scheduling information.

**The Stack**: As discussed, this starts at 2 KB and grows as needed. The stack contains local variables, function parameters, return addresses, and saved register values.

**The Defer/Panic Information**: If your goroutine uses `defer` statements or has panic recovery, additional structures are allocated to track this information.

### Where Goroutines Live in Memory

```
┌───────────────────────────────────────────────────────┐
│                    Process Memory Space               │
├───────────────────────────────────────────────────────┤
│  Heap (managed by Go's garbage collector)             │
│  ┌──────────────────────────────────────────────┐     │
│  │ G Structures (goroutine metadata)            │     │
│  │ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐              │     │
│  │ │ G1  │ │ G2  │ │ G3  │ │ G4  │ ...          │     │
│  │ └─────┘ └─────┘ └─────┘ └─────┘              │     │
│  │                                              │     │
│  │ Other heap-allocated objects                 │     │
│  └──────────────────────────────────────────────┘     │
│                                                       │
│  Goroutine Stacks (separate memory regions)           │
│  ┌──────┐  ┌──────┐  ┌──────┐  ┌──────┐               │
│  │Stack1│  │Stack2│  │Stack3│  │Stack4│  ...          │
│  │ 2KB  │  │ 4KB  │  │ 2KB  │  │ 8KB  │               │
│  └──────┘  └──────┘  └──────┘  └──────┘               │
│                                                       │
│  OS Threads (M structures)                            │
│  ┌──────────┐  ┌──────────┐                           │
│  │ Thread 1 │  │ Thread 2 │  ...                      │
│  └──────────┘  └──────────┘                           │
└───────────────────────────────────────────────────────┘
```

### Memory Allocation Process

When you create a new goroutine, here's the detailed memory allocation sequence:

**Phase 1: G Structure Allocation** The runtime first needs to get a G structure. It maintains a pool of free G structures to avoid repeated allocations. If a G is available in the pool, it's reused. If not, the runtime allocates a new G structure from the heap. This G structure is approximately 376 bytes.

**Phase 2: Stack Allocation** Next, the runtime allocates the initial stack. This is done from a special stack allocator that manages stack memory separately from the general heap. The stack allocator uses a cache to make allocation fast. The initial 2 KB stack is allocated from this cache if available, or from the OS if the cache is empty.

**Phase 3: Initialization** The G structure is initialized with:

- Stack bounds (start and end addresses)
- Program counter (pointing to the goroutine's starting function)
- Stack pointer (initially at the top of the stack)
- Goroutine ID
- Status (set to runnable)

**Phase 4: Queue Insertion** Finally, the G is placed in a run queue (either the local queue of a P or the global queue) and is ready to be scheduled.

### Memory Efficiency Example

Let's compare memory usage for handling 10,000 concurrent tasks:

```
Traditional Threading Approach:
10,000 threads × 2 MB per thread = 20,000 MB (≈20 GB)

Goroutine Approach:
10,000 goroutines × 2 KB per goroutine = 20,000 KB (≈20 MB)

Memory savings: ~1000x less memory!
```

This dramatic difference is why Go can handle hundreds of thousands of concurrent connections on modest hardware, while traditional threaded servers would struggle with just thousands.

---

## Complete Workflow: From Creation to Execution

Let's trace the entire journey of a goroutine from the moment you write `go myFunction()` to when it completes execution. This will give you a complete picture of everything that happens under the hood.

### Phase 1: Source Code to Runtime Call

When you write this code:

```go
func main() {
    go printNumbers()
}

func printNumbers() {
    for i := 0; i < 5; i++ {
        fmt.Println(i)
    }
}
```

The Go compiler transforms `go printNumbers()` into a call to the runtime function `runtime.newproc`. This function is responsible for creating and scheduling the new goroutine.

### Phase 2: Obtaining a G Structure

The runtime first needs to get a G structure to represent the new goroutine. It follows this process:

**Step 2a: Check the P's Free List** Each P (processor context) maintains a small cache of free G structures. The runtime first checks this cache. If a free G is available, it's reused. This is much faster than allocating new memory.

**Step 2b: Check the Global Free List** If the P's cache is empty, the runtime checks a global list of free G structures. It tries to grab a batch of free G's from this global list to refill the P's local cache.

**Step 2c: Allocate New G** If there are no free G structures available anywhere, the runtime allocates a new G from the heap. This G structure is approximately 376 bytes and will be reused when the goroutine completes.

### Phase 3: Stack Allocation

Now the runtime needs to allocate a stack for the goroutine:

**Step 3a: Determine Stack Size** The runtime determines the initial stack size needed. For most goroutines, this is 2 KB (2048 bytes). However, certain special goroutines (like those used by the garbage collector) may get larger initial stacks.

**Step 3b: Allocate from Stack Cache** Each P maintains a cache of stack spans of various sizes (2 KB, 4 KB, 8 KB, 16 KB, 32 KB). The runtime tries to allocate from this cache first.

**Step 3c: Allocate from Heap or OS** If the cache doesn't have a suitable stack span, the runtime allocates memory from the heap or directly from the operating system.

### Phase 4: Goroutine Initialization

With the G structure and stack allocated, the runtime initializes the goroutine:

```
Initialization Steps:

1. Set stack bounds:
   G.stackguard0 = stack_start + stack_guard_size
   G.stack.lo = stack_start
   G.stack.hi = stack_end

2. Set function to execute:
   G.startpc = address_of(printNumbers)

3. Set initial stack pointer:
   G.sched.sp = stack_end - frame_size

4. Set status:
   G.atomicstatus = _Grunnable

5. Assign goroutine ID:
   G.goid = next_goroutine_id++

6. Copy arguments onto stack:
   (if function has parameters)
```

### Phase 5: Scheduling the Goroutine

The newly created goroutine needs to be scheduled for execution:

**Step 5a: Try Local Queue** The runtime first tries to add the G to the current P's local run queue. Each P can hold up to 256 goroutines in its local queue. This is the fast path.

```
Current P's Local Queue:
┌─────────────────────────────────┐
│ [G1][G2][G3][G4] ... [New G]    │ ← Add new goroutine here
└─────────────────────────────────┘
    (capacity: 256 goroutines)
```

**Step 5b: If Local Queue Full, Use Global Queue** If the local queue is full, the runtime moves half of the local queue's goroutines to the global queue and then adds the new goroutine. This balances the load across all processors.

```
Before:
P1 Local Queue: [G1][G2][G3]...[G256] (Full)

After:
P1 Local Queue: [G1][G2]...[G128][New G]
Global Queue: [G129][G130]...[G256]
```

### Phase 6: Execution by M (OS Thread)

Now we need an M (OS thread) to actually run the goroutine:

**Step 6a: M-P Association** An M must be associated with a P to execute goroutines. When the program starts, the runtime creates GOMAXPROCS number of P's (typically equal to the number of CPU cores) and a pool of M's.

**Step 6b: M Picks Up Goroutine** An M that's associated with a P looks for work in this order:

1. Check the P's local run queue (fast path)
2. Check the global run queue
3. Try to steal from other P's queues (work stealing)
4. Check for network poller goroutines
5. If nothing found, the M goes to sleep

**Step 6c: Context Switch** When the M finds a goroutine to run, it performs a lightweight context switch:

```
Context Switch Process:

1. Save current goroutine's state (if any):
   - Save stack pointer
   - Save program counter
   - Save register values

2. Load new goroutine's state:
   - Restore stack pointer
   - Restore program counter
   - Restore register values

3. Jump to goroutine's code:
   - Start executing from program counter
```

### Phase 7: Goroutine Execution

The goroutine is now running on an M. During execution, several events can occur:

**Event A: Function Calls** Each function call checks if there's enough stack space. If the stack is near full, the runtime triggers a stack growth:

```
Stack Growth Process:

1. Check: if (SP < G.stackguard0)
   
2. Allocate new larger stack:
   new_size = old_size * 2
   new_stack = allocate(new_size)
   
3. Copy old stack to new stack:
   copy(new_stack, old_stack, old_size)
   
4. Adjust pointers:
   for each_pointer in stack:
       pointer = pointer + (new_stack - old_stack)
   
5. Update G structure:
   G.stack.lo = new_stack_start
   G.stack.hi = new_stack_end
   
6. Free old stack:
   free(old_stack)
```

**Event B: Blocking System Calls** If the goroutine makes a blocking system call (like reading from a file or network):

```
Blocking Call Handling:

1. M enters system call with G
   M.status = _Msyscall
   
2. Runtime detects blocking call
   (either immediate or after a timeout)
   
3. P is detached from M:
   P.m = nil
   M.p = nil
   
4. P is given to another M:
   if (idle_m exists):
       idle_m.p = P
       wake_up(idle_m)
   else:
       new_m = create_new_thread()
       new_m.p = P
       start(new_m)
   
5. Original M blocks in system call
   
6. When syscall completes:
   - Try to reacquire P
   - If no P available, put G in global queue
   - M goes to idle thread pool
```

**Event C: Channel Operations** If the goroutine performs a channel operation that blocks:

```
Channel Blocking:

1. Goroutine tries to send/receive on channel
   
2. Channel is not ready:
   - For send: channel buffer full or no receiver
   - For receive: channel empty or no sender
   
3. Goroutine is parked:
   G.status = _Gwaiting
   G is added to channel's wait queue
   
4. M picks up another goroutine from run queue
   
5. When channel becomes ready:
   - Another goroutine unblocks the waiting G
   - G is placed back in run queue
   - G.status = _Grunnable
```

**Event D: Scheduler Preemption** The Go scheduler can preempt a long-running goroutine to ensure fairness:

```
Preemption Process:

1. Scheduler checks: has G run for > 10ms?
   
2. If yes, mark for preemption:
   G.stackguard0 = stackPreempt
   
3. At next function call:
   - G checks stackguard0
   - Sees preemption mark
   - Calls runtime.morestack
   
4. Runtime scheduler takes over:
   - Save G's state
   - Set G.status = _Grunnable
   - Put G back in run queue
   - Pick next G to run
```

### Phase 8: Goroutine Completion

When a goroutine finishes execution:

**Step 8a: Function Returns** The goroutine's function completes and returns. The runtime's exit code is executed.

**Step 8b: Cleanup** The runtime performs cleanup:

```
Cleanup Process:

1. Set G status:
   G.status = _Gdead
   
2. Clear G fields:
   G.m = nil
   G.lockedm = nil
   G.waitreason = ""
   
3. Return stack to cache:
   add_to_stack_cache(G.stack)
   
4. Return G to free list:
   if (P.gFree.n < 64):
       add_to_P_local_free_list(G)
   else:
       add_to_global_free_list(G)
```

**Step 8c: Next Goroutine** The M immediately looks for another goroutine to run, continuing the cycle.

### Complete Workflow Diagram

```
┌──────────────────────────────────────────────────────────┐
│  go printNumbers()                                       │
└────────────────┬─────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────┐
│  runtime.newproc(printNumbers)                           │
└────────────────┬─────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────┐
│  Get G Structure (from cache or allocate)                │
│  ├─ Check P's free list                                  │
│  ├─ Check global free list                               │
│  └─ Allocate new if needed                               │
└────────────────┬─────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────┐
│  Allocate Stack (2 KB initial)                           │
│  ├─ Try stack cache                                      │
│  └─ Allocate from heap/OS if needed                      │
└────────────────┬─────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────┐
│  Initialize G                                            │
│  ├─ Set stack bounds                                     │
│  ├─ Set starting function (printNumbers)                 │
│  ├─ Set status to Runnable                               │
│  └─ Assign goroutine ID                                  │
└────────────────┬─────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────┐
│  Add to Run Queue                                        │
│  ├─ Try P's local queue (fast path)                      │
│  └─ Use global queue if local full                       │
└────────────────┬─────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────┐
│  M (OS Thread) Picks Up G                                │
│  ├─ M-P association                                      │
│  ├─ Find runnable G                                      │
│  └─ Context switch to G                                  │
└────────────────┬─────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────┐
│  Execute Goroutine                                       │
│  ├─ Run printNumbers() code                              │
│  ├─ Handle stack growth if needed                        │
│  ├─ Handle blocking (syscalls, channels)                 │
│  └─ Subject to preemption                                │
└────────────────┬─────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────┐
│  Goroutine Completes                                     │
│  ├─ Set status to Dead                                   │
│  ├─ Return stack to cache                                │
│  └─ Return G to free list                                │
└────────────────┬─────────────────────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────────────────────┐
│  M Looks for Next G to Run                               │
└──────────────────────────────────────────────────────────┘
```

This complete workflow shows every step from when you write `go myFunction()` to when the goroutine completes execution, including all memory allocations, scheduling decisions, and runtime operations.

---

## All Goroutine Functionality

### Core Goroutine Operations

Goroutines provide a rich set of functionality for concurrent programming. Let's explore all the capabilities in detail.

### 1. Anonymous Goroutines

You don't always need to define a separate function to create a goroutine. You can use anonymous functions (closures) directly:

```go
package main

import (
    "fmt"
    "time"
)

func main() {
    message := "Hello"
    
    // Anonymous goroutine
    go func() {
        fmt.Println(message, "from anonymous goroutine")
    }()
    
    // Anonymous goroutine with parameters
    go func(msg string) {
        fmt.Println(msg)
    }("Hello with parameter")
    
    time.Sleep(time.Second)
}
```

Anonymous goroutines are particularly useful for capturing variables from the surrounding scope. However, you must be careful about variable capture, especially in loops.

### 2. Goroutine Communication via Channels

Channels are the primary way goroutines communicate with each other. Think of a channel as a pipe through which you can send and receive values.

**Unbuffered Channels**: These channels have no capacity. A send operation blocks until another goroutine is ready to receive, and vice versa. This provides synchronization between goroutines.

```go
func main() {
    ch := make(chan string) // unbuffered channel
    
    go func() {
        // This send will block until main() receives
        ch <- "Hello from goroutine"
    }()
    
    // This receive will block until the goroutine sends
    message := <-ch
    fmt.Println(message)
}
```

**Buffered Channels**: These channels have a specified capacity. A send blocks only when the buffer is full, and a receive blocks only when the buffer is empty.

```go
func main() {
    ch := make(chan int, 3) // buffered channel with capacity 3
    
    // These sends won't block because buffer has space
    ch <- 1
    ch <- 2
    ch <- 3
    
    // This would block: ch <- 4
    
    // Receive values
    fmt.Println(<-ch) // 1
    fmt.Println(<-ch) // 2
    fmt.Println(<-ch) // 3
}
```

### 3. Channel Directions

You can specify whether a channel can be used for sending, receiving, or both. This helps prevent misuse:

```go
// Send-only channel
func sendOnly(ch chan<- int) {
    ch <- 42
    // Cannot receive: value := <-ch  // This would cause compile error
}

// Receive-only channel
func receiveOnly(ch <-chan int) {
    value := <-ch
    // Cannot send: ch <- 42  // This would cause compile error
}

// Bidirectional channel (can send and receive)
func bidirectional(ch chan int) {
    ch <- 42
    value := <-ch
    fmt.Println(value)
}
```

### 4. Closing Channels

A sender can close a channel to indicate that no more values will be sent. Receivers can check if a channel is closed:

```go
func producer(ch chan int) {
    for i := 0; i < 5; i++ {
        ch <- i
    }
    close(ch) // Signal that we're done sending
}

func consumer(ch chan int) {
    // Method 1: Check if channel is closed
    for {
        value, ok := <-ch
        if !ok {
            break // Channel is closed
        }
        fmt.Println(value)
    }
    
    // Method 2: Range over channel (automatically stops when closed)
    for value := range ch {
        fmt.Println(value)
    }
}
```

### 5. Select Statement

The `select` statement lets a goroutine wait on multiple channel operations. It's like a switch statement for channels:

```go
func main() {
    ch1 := make(chan string)
    ch2 := make(chan string)
    
    go func() {
        time.Sleep(1 * time.Second)
        ch1 <- "from ch1"
    }()
    
    go func() {
        time.Sleep(2 * time.Second)
        ch2 <- "from ch2"
    }()
    
    for i := 0; i < 2; i++ {
        select {
        case msg1 := <-ch1:
            fmt.Println("Received", msg1)
        case msg2 := <-ch2:
            fmt.Println("Received", msg2)
        case <-time.After(3 * time.Second):
            fmt.Println("Timeout")
        }
    }
}
```

The `select` statement blocks until one of its cases can proceed, then it executes that case. If multiple cases are ready, it chooses one at random.

### 6. Non-blocking Channel Operations

You can make channel operations non-blocking using `select` with a `default` case:

```go
func main() {
    ch := make(chan int, 1)
    
    // Non-blocking send
    select {
    case ch <- 42:
        fmt.Println("Sent value")
    default:
        fmt.Println("Channel full, couldn't send")
    }
    
    // Non-blocking receive
    select {
    case value := <-ch:
        fmt.Println("Received", value)
    default:
        fmt.Println("Channel empty, couldn't receive")
    }
}
```

### 7. WaitGroups for Synchronization

The `sync.WaitGroup` type is used to wait for a collection of goroutines to finish executing:

```go
import (
    "fmt"
    "sync"
    "time"
)

func worker(id int, wg *sync.WaitGroup) {
    defer wg.Done() // Signal completion when function returns
    
    fmt.Printf("Worker %d starting\n", id)
    time.Sleep(time.Second)
    fmt.Printf("Worker %d done\n", id)
}

func main() {
    var wg sync.WaitGroup
    
    for i := 1; i <= 5; i++ {
        wg.Add(1) // Increment counter
        go worker(i, &wg)
    }
    
    wg.Wait() // Block until counter is zero
    fmt.Println("All workers completed")
}
```

### 8. Mutexes for Shared State

When multiple goroutines need to access shared data, you need synchronization to prevent race conditions. The `sync.Mutex` type provides mutual exclusion:

```go
import (
    "fmt"
    "sync"
)

type SafeCounter struct {
    mu    sync.Mutex
    count int
}

func (c *SafeCounter) Increment() {
    c.mu.Lock()         // Acquire lock
    defer c.mu.Unlock() // Release lock when function returns
    c.count++
}

func (c *SafeCounter) Value() int {
    c.mu.Lock()
    defer c.mu.Unlock()
    return c.count
}

func main() {
    counter := SafeCounter{}
    var wg sync.WaitGroup
    
    for i := 0; i < 1000; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            counter.Increment()
        }()
    }
    
    wg.Wait()
    fmt.Println("Final count:", counter.Value())
}
```

### 9. Context for Cancellation and Timeouts

The `context` package provides a way to carry deadlines, cancellation signals, and other request-scoped values across goroutines:

```go
import (
    "context"
    "fmt"
    "time"
)

func worker(ctx context.Context, id int) {
    for {
        select {
        case <-ctx.Done():
            fmt.Printf("Worker %d cancelled\n", id)
            return
        default:
            fmt.Printf("Worker %d working...\n", id)
            time.Sleep(500 * time.Millisecond)
        }
    }
}

func main() {
    // Create context with timeout
    ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
    defer cancel() // Always call cancel to release resources
    
    for i := 1; i <= 3; i++ {
        go worker(ctx, i)
    }
    
    // Wait for context to be done
    <-ctx.Done()
    fmt.Println("Main function exiting")
    time.Sleep(time.Second) // Give goroutines time to finish
}
```

### 10. Once for One-Time Initialization

The `sync.Once` type ensures that a piece of code is executed only once, no matter how many goroutines call it:

```go
import (
    "fmt"
    "sync"
)

var (
    instance *Database
    once     sync.Once
)

type Database struct {
    connection string
}

func GetDatabase() *Database {
    once.Do(func() {
        fmt.Println("Creating database instance")
        instance = &Database{connection: "db://localhost"}
    })
    return instance
}

func main() {
    var wg sync.WaitGroup
    
    for i := 0; i < 10; i++ {
        wg.Add(1)
        go func() {
            defer wg.Done()
            db := GetDatabase()
            fmt.Println(db.connection)
        }()
    }
    
    wg.Wait()
    // "Creating database instance" prints only once
}
```

### 11. Runtime Control Functions

Go provides several runtime functions to control goroutine behavior:

**runtime.GOMAXPROCS**: Sets the maximum number of CPUs that can execute simultaneously.

```go
import "runtime"

func main() {
    // Get current setting
    n := runtime.GOMAXPROCS(0)
    fmt.Println("Current GOMAXPROCS:", n)
    
    // Set to 4 CPUs
    runtime.GOMAXPROCS(4)
}
```

**runtime.Gosched**: Yields the processor to other goroutines.

```go
func main() {
    go func() {
        for i := 0; i < 5; i++ {
            fmt.Println("Goroutine")
            runtime.Gosched() // Yield to main
        }
    }()
    
    for i := 0; i < 5; i++ {
        fmt.Println("Main")
        runtime.Gosched() // Yield to goroutine
    }
}
```

**runtime.NumGoroutine**: Returns the number of goroutines currently running.

```go
func main() {
    fmt.Println("Goroutines:", runtime.NumGoroutine()) // 1
    
    for i := 0; i < 10; i++ {
        go func() {
            time.Sleep(time.Second)
        }()
    }
    
    fmt.Println("Goroutines:", runtime.NumGoroutine()) // 11
}
```

### Diagram: Goroutine Communication Patterns

```
1. Pipeline Pattern:
   Generator → Processor → Consumer
   [G1] ─────→ [G2] ─────→ [G3]
        ch1         ch2

2. Fan-Out Pattern (one producer, multiple consumers):
          ┌─→ [Worker 1]
   [Producer] ─→ [Worker 2]
          └─→ [Worker 3]

3. Fan-In Pattern (multiple producers, one consumer):
   [Producer 1] ┐
   [Producer 2] ├─→ [Consumer]
   [Producer 3] ┘

4. Worker Pool Pattern:
   [Task Queue] → [Worker 1]
                  [Worker 2]
                  [Worker 3]
                  [Worker 4]
```

---

## Real-World Examples

Let's explore practical, real-world examples that demonstrate how goroutines solve actual problems you'll encounter in production applications.

### Example 1: Concurrent Web Scraper

Imagine you need to fetch data from 100 different websites. Doing this sequentially would be extremely slow. Goroutines make this fast and efficient:

```go
package main

import (
    "fmt"
    "io"
    "net/http"
    "sync"
    "time"
)

type Result struct {
    URL      string
    Status   int
    Duration time.Duration
    Error    error
}

func fetchURL(url string, results chan<- Result, wg *sync.WaitGroup) {
    defer wg.Done()
    
    start := time.Now()
    resp, err := http.Get(url)
    duration := time.Since(start)
    
    result := Result{
        URL:      url,
        Duration: duration,
    }
    
    if err != nil {
        result.Error = err
        results <- result
        return
    }
    defer resp.Body.Close()
    
    result.Status = resp.StatusCode
    results <- result
}

func main() {
    urls := []string{
        "https://google.com",
        "https://github.com",
        "https://stackoverflow.com",
        "https://reddit.com",
        "https://twitter.com",
    }
    
    results := make(chan Result, len(urls))
    var wg sync.WaitGroup
    
    // Start time
    start := time.Now()
    
    // Launch goroutine for each URL
    for _, url := range urls {
        wg.Add(1)
        go fetchURL(url, results, &wg)
    }
    
    // Wait for all goroutines to complete
    wg.Wait()
    close(results)
    
    // Print results
    fmt.Printf("Total time: %v\n\n", time.Since(start))
    
    for result := range results {
        if result.Error != nil {
            fmt.Printf("❌ %s failed: %v (took %v)\n", 
                result.URL, result.Error, result.Duration)
        } else {
            fmt.Printf("✓ %s returned %d (took %v)\n", 
                result.URL, result.Status, result.Duration)
        }
    }
}
```

**How This Works**: Instead of fetching URLs one by one (which would take 5 × average response time), all URLs are fetched concurrently. If each request takes 2 seconds, the sequential approach takes 10 seconds, while the concurrent approach takes only about 2 seconds.

**Memory Considerations**: Each goroutine in this example uses approximately 2-4 KB of memory initially. For 100 URLs, that's only 200-400 KB, compared to 100-200 MB if we used OS threads.

### Example 2: Worker Pool Pattern

A worker pool is a common pattern where you have a fixed number of workers processing tasks from a queue. This is useful for rate limiting and resource management:

```go
package main

import (
    "fmt"
    "math/rand"
    "sync"
    "time"
)

type Task struct {
    ID       int
    Data     string
}

type Result struct {
    TaskID   int
    Output   string
    Duration time.Duration
}

// Worker function processes tasks from the jobs channel
func worker(id int, jobs <-chan Task, results chan<- Result, wg *sync.WaitGroup) {
    defer wg.Done()
    
    for job := range jobs {
        start := time.Now()
        
        // Simulate work with random duration
        processingTime := time.Duration(rand.Intn(1000)) * time.Millisecond
        time.Sleep(processingTime)
        
        result := Result{
            TaskID:   job.ID,
            Output:   fmt.Sprintf("Processed by worker %d: %s", id, job.Data),
            Duration: time.Since(start),
        }
        
        results <- result
    }
    
    fmt.Printf("Worker %d finished\n", id)
}

func main() {
    const numWorkers = 3
    const numJobs = 10
    
    jobs := make(chan Task, numJobs)
    results := make(chan Result, numJobs)
    
    var wg sync.WaitGroup
    
    // Start workers
    fmt.Printf("Starting %d workers...\n", numWorkers)
    for i := 1; i <= numWorkers; i++ {
        wg.Add(1)
        go worker(i, jobs, results, &wg)
    }
    
    // Send jobs
    fmt.Printf("Sending %d jobs...\n\n", numJobs)
    for i := 1; i <= numJobs; i++ {
        jobs <- Task{
            ID:   i,
            Data: fmt.Sprintf("Task-%d", i),
        }
    }
    close(jobs) // No more jobs will be sent
    
    // Close results channel when all workers are done
    go func() {
        wg.Wait()
        close(results)
    }()
    
    // Collect results
    for result := range results {
        fmt.Printf("Task %d completed in %v: %s\n", 
            result.TaskID, result.Duration, result.Output)
    }
    
    fmt.Println("\nAll tasks completed!")
}
```

**Why This Pattern Is Useful**: Instead of creating a goroutine for every task (which could be thousands), we create a fixed number of workers. This:

- Limits resource consumption
- Prevents overwhelming external services
- Makes the system more predictable and controllable

**Real-World Use Case**: This pattern is perfect for scenarios like processing uploaded files, sending emails, making API calls to external services with rate limits, or processing database records in batches.

### Example 3: Real-time Data Aggregator

This example simulates a system that collects data from multiple sensors concurrently and aggregates the results:

```go
package main

import (
    "fmt"
    "math/rand"
    "sync"
    "time"
)

type SensorReading struct {
    SensorID    int
    Temperature float64
    Humidity    float64
    Timestamp   time.Time
}

type AggregatedData struct {
    AvgTemperature float64
    AvgHumidity    float64
    ReadingCount   int
}

// Simulates a sensor that sends periodic readings
func sensor(id int, readings chan<- SensorReading, stop <-chan bool, wg *sync.WaitGroup) {
    defer wg.Done()
    
    ticker := time.NewTicker(500 * time.Millisecond)
    defer ticker.Stop()
    
    for {
        select {
        case <-stop:
            fmt.Printf("Sensor %d stopped\n", id)
            return
        case <-ticker.C:
            reading := SensorReading{
                SensorID:    id,
                Temperature: 20.0 + rand.Float64()*10.0, // 20-30°C
                Humidity:    50.0 + rand.Float64()*30.0, // 50-80%
                Timestamp:   time.Now(),
            }
            readings <- reading
        }
    }
}

// Aggregates sensor readings
func aggregator(readings <-chan SensorReading, stop <-chan bool) {
    var (
        totalTemp     float64
        totalHumidity float64
        count         int
    )
    
    ticker := time.NewTicker(2 * time.Second)
    defer ticker.Stop()
    
    for {
        select {
        case <-stop:
            fmt.Println("Aggregator stopped")
            return
            
        case reading := <-readings:
            totalTemp += reading.Temperature
            totalHumidity += reading.Humidity
            count++
            fmt.Printf("Sensor %d: Temp=%.1f°C, Humidity=%.1f%%\n", 
                reading.SensorID, reading.Temperature, reading.Humidity)
            
        case <-ticker.C:
            if count > 0 {
                fmt.Printf("\n=== Aggregated Data ===\n")
                fmt.Printf("Average Temperature: %.2f°C\n", totalTemp/float64(count))
                fmt.Printf("Average Humidity: %.2f%%\n", totalHumidity/float64(count))
                fmt.Printf("Total Readings: %d\n\n", count)
            }
        }
    }
}

func main() {
    const numSensors = 3
    
    readings := make(chan SensorReading, 10)
    stopSensors := make(chan bool)
    stopAggregator := make(chan bool)
    
    var wg sync.WaitGroup
    
    // Start sensors
    fmt.Printf("Starting %d sensors...\n\n", numSensors)
    for i := 1; i <= numSensors; i++ {
        wg.Add(1)
        go sensor(i, readings, stopSensors, &wg)
    }
    
    // Start aggregator
    go aggregator(readings, stopAggregator)
    
    // Run for 10 seconds
    time.Sleep(10 * time.Second)
    
    // Stop sensors
    fmt.Println("\nStopping sensors...")
    close(stopSensors)
    wg.Wait()
    
    // Stop aggregator
    close(stopAggregator)
    close(readings)
    
    time.Sleep(time.Second) // Let aggregator finish
    fmt.Println("System shutdown complete")
}
```

**Real-World Applications**: This pattern is used in:

- IoT systems collecting data from multiple devices
- Monitoring systems aggregating metrics from multiple servers
- Financial trading systems collecting price feeds from multiple exchanges
- Log aggregation systems collecting logs from multiple services

### Example 4: Parallel File Processing

This example demonstrates processing multiple files concurrently, a common task in data processing pipelines:

```go
package main

import (
    "fmt"
    "math/rand"
    "sync"
    "time"
)

type File struct {
    Name string
    Size int // in MB
}

type ProcessResult struct {
    FileName     string
    Success      bool
    Duration     time.Duration
    RecordsProcessed int
    Error        error
}

func processFile(file File, results chan<- ProcessResult, wg *sync.WaitGroup) {
    defer wg.Done()
    
    start := time.Now()
    
    // Simulate file processing (100ms per MB)
    processingTime := time.Duration(file.Size*100) * time.Millisecond
    time.Sleep(processingTime)
    
    // Simulate occasional errors
    success := rand.Float32() > 0.1 // 90% success rate
    
    result := ProcessResult{
        FileName:         file.Name,
        Success:          success,
        Duration:         time.Since(start),
        RecordsProcessed: file.Size * 1000, // Simulate 1000 records per MB
    }
    
    if !success {
        result.Error = fmt.Errorf("processing error in file %s", file.Name)
    }
    
    results <- result
}

func main() {
    files := []File{
        {"data_2024_01.csv", 50},
        {"data_2024_02.csv", 75},
        {"data_2024_03.csv", 100},
        {"data_2024_04.csv", 60},
        {"data_2024_05.csv", 80},
        {"data_2024_06.csv", 90},
    }
    
    results := make(chan ProcessResult, len(files))
    var wg sync.WaitGroup
    
    fmt.Println("Starting parallel file processing...\n")
    start := time.Now()
    
    // Process all files concurrently
    for _, file := range files {
        wg.Add(1)
        go processFile(file, results, &wg)
        fmt.Printf("Started processing: %s (%d MB)\n", file.Name, file.Size)
    }
    
    // Wait for all processing to complete
    wg.Wait()
    close(results)
    
    totalDuration := time.Since(start)
    
    // Collect and display results
    fmt.Println("\n=== Processing Results ===")
    successCount := 0
    totalRecords := 0
    
    for result := range results {
        if result.Success {
            successCount++
            totalRecords += result.RecordsProcessed
            fmt.Printf("✓ %s: %d records in %v\n", 
                result.FileName, result.RecordsProcessed, result.Duration)
        } else {
            fmt.Printf("✗ %s: FAILED - %v\n", result.FileName, result.Error)
        }
    }
    
    fmt.Printf("\n=== Summary ===\n")
    fmt.Printf("Total time: %v\n", totalDuration)
    fmt.Printf("Files processed: %d/%d\n", successCount, len(files))
    fmt.Printf("Total records: %d\n", totalRecords)
    
    // Calculate what sequential processing would have taken
    var sequentialTime time.Duration
    for _, file := range files {
        sequentialTime += time.Duration(file.Size*100) * time.Millisecond
    }
    fmt.Printf("Sequential time would have been: %v\n", sequentialTime)
    fmt.Printf("Speedup: %.2fx\n", float64(sequentialTime)/float64(totalDuration))
}
```

**Key Insight**: In this example, if we have 6 files totaling 455 MB, sequential processing would take about 45.5 seconds. With concurrent processing (assuming 6 CPU cores), it takes about 10 seconds (the time of the largest file). This is a 4.5x speedup!

### Example 5: Rate-Limited API Client

This example shows how to make concurrent API calls while respecting rate limits:

```go
package main

import (
    "fmt"
    "sync"
    "time"
)

type APIRequest struct {
    ID       int
    Endpoint string
}

type APIResponse struct {
    RequestID  int
    StatusCode int
    Duration   time.Duration
}

// Rate limiter using a ticker
func rateLimiter(requestsPerSecond int) <-chan time.Time {
    interval := time.Second / time.Duration(requestsPerSecond)
    ticker := time.NewTicker(interval)
    return ticker.C
}

func makeAPICall(req APIRequest) APIResponse {
    start := time.Now()
    
    // Simulate API call
    time.Sleep(100 * time.Millisecond)
    
    return APIResponse{
        RequestID:  req.ID,
        StatusCode: 200,
        Duration:   time.Since(start),
    }
}

func worker(id int, requests <-chan APIRequest, responses chan<- APIResponse, 
            limiter <-chan time.Time, wg *sync.WaitGroup) {
    defer wg.Done()
    
    for req := range requests {
        // Wait for rate limiter
        <-limiter
        
        fmt.Printf("Worker %d processing request %d\n", id, req.ID)
        response := makeAPICall(req)
        responses <- response
    }
}

func main() {
    const (
        numWorkers = 3
        numRequests = 20
        rateLimit = 5 // requests per second
    )
    
    requests := make(chan APIRequest, numRequests)
    responses := make(chan APIResponse, numRequests)
    limiter := rateLimiter(rateLimit)
    
    var wg sync.WaitGroup
    
    // Start workers
    fmt.Printf("Starting %d workers with rate limit of %d req/s\n\n", 
        numWorkers, rateLimit)
    
    for i := 1; i <= numWorkers; i++ {
        wg.Add(1)
        go worker(i, requests, responses, limiter, &wg)
    }
    
    // Send requests
    start := time.Now()
    for i := 1; i <= numRequests; i++ {
        requests <- APIRequest{
            ID:       i,
            Endpoint: fmt.Sprintf("/api/endpoint-%d", i),
        }
    }
    close(requests)
    
    // Close responses after all workers complete
    go func() {
        wg.Wait()
        close(responses)
    }()
    
    // Collect responses
    successCount := 0
    for response := range responses {
        successCount++
        fmt.Printf("Request %d completed: Status %d (took %v)\n", 
            response.RequestID, response.StatusCode, response.Duration)
    }
    
    totalDuration := time.Since(start)
    
    fmt.Printf("\n=== Summary ===\n")
    fmt.Printf("Total requests: %d\n", successCount)
    fmt.Printf("Total time: %v\n", totalDuration)
    fmt.Printf("Actual rate: %.2f req/s\n", 
        float64(successCount)/totalDuration.Seconds())
}
```

**Real-World Application**: This pattern is essential when working with third-party APIs that have rate limits. It ensures you maximize throughput while staying within the API's constraints.

These examples demonstrate how goroutines solve real-world problems efficiently. The key takeaway is that goroutines make concurrent programming accessible and practical, allowing you to write performant applications that can handle thousands of concurrent operations with minimal resource usage.

---

## Conclusion

Goroutines represent one of Go's most powerful features, making concurrent programming simple and efficient. Through this comprehensive guide, we've explored:

- **What goroutines are**: Lightweight threads managed by the Go runtime
- **Why we need them**: To handle concurrent operations efficiently with minimal resource overhead
- **How they work**: Through the sophisticated GMP scheduler model
- **Memory management**: How goroutines are stored and managed in memory with small initial footprints
- **Complete workflow**: Every step from creation to execution and cleanup
- **All functionality**: Channels, WaitGroups, Mutexes, Context, and more
- **Real-world applications**: Practical examples that solve actual problems

The beauty of goroutines lies in their simplicity. With just the `go` keyword, you can create concurrent programs that would require complex threading code in other languages. Combined with channels for communication and synchronization primitives for coordination, goroutines provide a complete toolkit for building scalable, concurrent applications.