# The Complete Guide to Goroutines: From Basics to Orchestra Orchestration

## Table of Contents

1. [Introduction - The Power of Goroutines](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#introduction)
2. [What are Goroutines?](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#what-are-goroutines)
3. [Creating and Starting Goroutines](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#creating-goroutines)
4. [Goroutine Lifecycle](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#goroutine-lifecycle)
5. [How Goroutines Work Internally](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#how-goroutines-work)
6. [Multiple Goroutines Working Together](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#multiple-goroutines)
7. [Coordination Patterns](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#coordination-patterns)
8. [Visualization of Goroutine Execution](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#visualization)
9. [Common Patterns and Architectures](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#patterns)
10. [Debugging and Troubleshooting](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#debugging)
11. [Best Practices](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#best-practices)

---

## Introduction - The Power of Goroutines {#introduction}

Imagine conducting an orchestra. You have dozens of musicians - violins, cellos, trumpets, drums - all playing different parts of the same symphony. Each musician reads their own sheet music and plays their instrument independently. Yet somehow, through careful coordination, they produce a harmonious whole that's greater than the sum of its parts.

This is exactly what goroutines enable in Go programs. Goroutines are like the musicians in an orchestra:

**Independence**: Each goroutine executes its own code independently, like a musician playing their part.

**Coordination**: Goroutines coordinate through channels and synchronization primitives, like musicians following the conductor's baton and listening to each other.

**Concurrency**: Multiple goroutines make progress concurrently, like musicians playing simultaneously.

**Efficiency**: Goroutines are incredibly lightweight - you can have thousands of them, like a massive orchestra with hundreds of musicians.

**Harmony**: When properly coordinated, goroutines work together to accomplish complex tasks that would be difficult or impossible with sequential code.

But unlike an orchestra where coordination is handled by a human conductor, goroutines need explicit coordination mechanisms in code. This guide teaches you how to be the conductor of your goroutine orchestra - how to start goroutines, coordinate their work, handle their completion, and debug issues when they get out of sync.

### The Restaurant Kitchen Revisited

Let's use a detailed restaurant kitchen analogy throughout this guide:

**Sequential Cooking (No Goroutines)**: One chef prepares an entire meal from start to finish. They dice vegetables, cook the main course, prepare the sauce, and plate everything. For one order, this works. For 50 simultaneous orders, the kitchen grinds to a halt. Customers wait hours for food.

**Concurrent Cooking (With Goroutines)**: Multiple chefs work simultaneously:

- Chef A (goroutine 1) handles all appetizers
- Chef B (goroutine 2) handles all main courses
- Chef C (goroutine 3) handles all desserts
- Chef D (goroutine 4) handles all sauces

Each chef works independently on their specialty. When Chef B needs sauce, Chef D has it ready. When all parts of an order are complete, they're plated together. The kitchen can handle 50 simultaneous orders efficiently because work is distributed and concurrent.

The challenge: coordinating these chefs. How does Chef B know when Chef D's sauce is ready? How do we know when all parts of an order are complete? This is the core challenge of concurrent programming with goroutines.

---

## What are Goroutines? {#what-are-goroutines}

A goroutine is a lightweight thread of execution managed by the Go runtime. It's Go's fundamental unit of concurrency.

### Goroutines vs. OS Threads

Let's understand what makes goroutines special by comparing them to traditional OS threads:

**OS Thread**:

```
┌─────────────────────────────────────────┐
│ OS Thread (e.g., pthread, Windows thread) │
├─────────────────────────────────────────┤
│ Stack: 1-8 MB (fixed size)              │
│ Creation time: ~20 microseconds         │
│ Context switch: ~1-2 microseconds       │
│ Managed by: OS kernel scheduler         │
│ Cost: Expensive                         │
│ Limit: ~10,000 per system               │
└─────────────────────────────────────────┘
```

**Goroutine**:

```
┌─────────────────────────────────────────┐
│ Goroutine (Go's lightweight thread)     │
├─────────────────────────────────────────┤
│ Stack: 2 KB (grows/shrinks dynamically)│
│ Creation time: ~0.2 microseconds        │
│ Context switch: ~0.2 microseconds       │
│ Managed by: Go runtime scheduler        │
│ Cost: Very cheap                        │
│ Limit: Millions per system              │
└─────────────────────────────────────────┘
```

**Key Differences**:

**Memory Footprint**: OS threads start with 1-8MB stacks. Creating 10,000 threads = 10-80GB of memory just for stacks! Goroutines start with 2KB and grow only when needed. Creating 10,000 goroutines = only 20MB. This is why you can have millions of goroutines but only thousands of threads.

**Creation Cost**: Creating an OS thread involves system calls, kernel operations, and significant overhead. Creating a goroutine is just allocating a small stack and a few data structures in user space - 100x faster.

**Scheduling**: OS threads are scheduled by the kernel. Every context switch involves saving/restoring CPU registers, changing memory contexts, and kernel mode transitions. Goroutines are scheduled by the Go runtime in user space - much faster and more efficient.

**Growth**: OS thread stacks are fixed size. Set it too small and stack overflows occur. Set it too large and you waste memory. Goroutine stacks grow and shrink dynamically as needed - you get the right size automatically.

### What Makes Goroutines Lightweight?

**Small Initial Stack**: 2KB is tiny. It holds a few stack frames (function calls) and local variables. For goroutines that do simple work, this is enough - no need to allocate more.

**Stack Copying for Growth**: When a goroutine needs more stack (deep function calls, large local variables), the runtime allocates a bigger stack and copies everything over. This happens transparently. The goroutine's maximum stack can be gigabytes, but it only allocates what's needed.

**User-Space Scheduling**: The Go runtime scheduler (not the OS) decides which goroutine runs on which OS thread. This is much faster than kernel scheduling because it doesn't involve context switches into kernel mode.

**Multiplexing**: The Go runtime multiplexes many goroutines onto a small number of OS threads (typically equal to the number of CPU cores). So you might have 10,000 goroutines but only 8 OS threads. The OS sees 8 threads (cheap), but your program has 10,000 concurrent execution contexts.

### Goroutines are Not Magic

It's important to understand that goroutines don't create more CPU cores or make your computer magically faster. What they do:

**Concurrency, Not Parallelism (Necessarily)**: Goroutines provide concurrent execution structure. Whether they execute in parallel depends on available CPU cores. With 1 CPU core, goroutines run concurrently (interleaved) but not parallel. With 8 cores, up to 8 can run truly in parallel.

**Efficient Use of Resources**: Goroutines excel at I/O-bound tasks. When one goroutine waits for I/O (network, disk, database), another goroutine uses the CPU. Traditional threads often sit idle waiting. Goroutines maximize CPU utilization.

**Simplified Concurrent Programming**: The real magic is that goroutines make concurrent programming accessible. In other languages, concurrency is hard - thread pools, callbacks, complex synchronization. In Go, you just write `go function()` and the runtime handles the complexity.

---

## Creating and Starting Goroutines {#creating-goroutines}

Creating a goroutine is remarkably simple - just add `go` before a function call.

### Basic Syntax

```go
// Regular function call (synchronous)
doWork()  // Caller waits for doWork() to complete

// Goroutine (asynchronous)
go doWork()  // Caller continues immediately, doWork() runs concurrently
```

The `go` keyword says "execute this function concurrently in a new goroutine." The calling goroutine doesn't wait - it continues executing the next line immediately.

### Named Function Goroutines

```go
func printNumbers() {
    for i := 1; i <= 5; i++ {
        fmt.Println(i)
        time.Sleep(100 * time.Millisecond)
    }
}

func main() {
    go printNumbers()  // Start goroutine
    
    // Main goroutine continues here immediately
    fmt.Println("Main goroutine continues")
    
    time.Sleep(1 * time.Second)  // Wait for goroutine to finish
}
```

**What Happens**:

```
Timeline:
T=0ms:    Main starts
T=0ms:    Main calls "go printNumbers()"
          - New goroutine created
          - printNumbers() starts in new goroutine
T=0ms:    Main continues to next line (doesn't wait)
T=0ms:    Main prints "Main goroutine continues"
T=0ms:    Main sleeps (1 second)

Meanwhile, printNumbers() goroutine:
T=0ms:    Prints "1"
T=100ms:  Prints "2"
T=200ms:  Prints "3"
T=300ms:  Prints "4"
T=400ms:  Prints "5"
T=500ms:  Goroutine finishes

T=1000ms: Main wakes up and exits
```

Both the main goroutine and the printNumbers goroutine execute concurrently. They're both making progress "at the same time" (concurrently, potentially in parallel on multi-core systems).

### Anonymous Function Goroutines

Often you use anonymous functions (function literals) with goroutines:

```go
func main() {
    go func() {
        fmt.Println("Hello from goroutine")
    }()  // () at the end calls the function
    
    time.Sleep(100 * time.Millisecond)
}
```

This is very common when you need a quick concurrent operation and don't want to define a separate named function.

### Goroutines with Parameters

You can pass parameters to goroutines:

```go
func processItem(id int, name string) {
    fmt.Printf("Processing item %d: %s\n", id, name)
}

func main() {
    go processItem(1, "apple")
    go processItem(2, "banana")
    
    time.Sleep(100 * time.Millisecond)
}
```

**Critical Gotcha - Variable Capture**:

```go
// WRONG - Common mistake!
for i := 0; i < 5; i++ {
    go func() {
        fmt.Println(i)  // Captures variable i by reference!
    }()
}
// Output: Might print 5, 5, 5, 5, 5 (all goroutines see final value)

// RIGHT - Pass as parameter
for i := 0; i < 5; i++ {
    go func(id int) {
        fmt.Println(id)  // Each goroutine gets its own copy
    }(i)  // Pass i as argument
}
// Output: 0, 1, 2, 3, 4 (in some order)

// ALSO RIGHT - Copy to local variable
for i := 0; i < 5; i++ {
    i := i  // Create new variable in loop scope
    go func() {
        fmt.Println(i)  // Captures the loop-scope i
    }()
}
```

**Why This Happens**: When the anonymous function captures variable `i` by reference, all goroutines share the same `i` variable. By the time goroutines actually execute (they start asynchronously), the loop has finished and `i = 5`. So all goroutines print 5.

By passing `i` as a parameter or creating a new loop-scoped variable, each goroutine gets its own copy of the value.

### The Main Goroutine

Your `main()` function runs in a goroutine called the main goroutine. When `main()` returns, the program exits immediately - all other goroutines are terminated, even if they haven't finished.

```go
func worker() {
    time.Sleep(2 * time.Second)
    fmt.Println("Worker done")  // Never prints!
}

func main() {
    go worker()
    
    fmt.Println("Main done")
    // Main exits immediately, worker goroutine is killed
}
```

**Output**: Just "Main done" - the worker never finishes because the program exits.

This is why you often see `time.Sleep()` or synchronization (WaitGroups, channels) at the end of examples - to keep the main goroutine alive until workers finish.

---

## Goroutine Lifecycle {#goroutine-lifecycle}

A goroutine goes through several states during its lifetime. Understanding this lifecycle helps you reason about goroutine behavior.

### Lifecycle States

```
┌────────────────────────────────────────────────────────────────┐
│                     GOROUTINE LIFECYCLE                        │
└────────────────────────────────────────────────────────────────┘

    ┌─────────┐
    │ Created │  (go func() called)
    └────┬────┘
         │
         ↓
    ┌─────────┐
    │Runnable │  (Waiting to be scheduled)
    └────┬────┘
         │
         ↓
    ┌─────────┐
    │ Running │  (Executing on a CPU)
    └────┬────┘
         │
         ├──────→ Blocks on channel/lock/I/O  → ┌─────────┐
         │                                       │ Waiting │
         │                                       └────┬────┘
         │                                            │
         │  ←─────────────────────────────────────────┘
         │         (Unblocked, back to Runnable)
         │
         ↓
    ┌─────────┐
    │  Dead   │  (Function returned)
    └─────────┘
```

### State Descriptions

**Created**: The moment you call `go func()`, a goroutine is created. The runtime allocates a small stack (2KB) and initializes goroutine metadata (G structure). This happens very quickly (~0.2 microseconds).

**Runnable**: The goroutine is ready to execute but isn't currently running on a CPU. It's in a run queue waiting for the scheduler to assign it to an OS thread. Many goroutines can be runnable simultaneously, but only as many as there are CPU cores can actually run in parallel.

**Running**: The goroutine is executing on a CPU. Its code is actively running. On an 8-core system, up to 8 goroutines can be in the running state simultaneously (true parallelism).

**Waiting (Blocked)**: The goroutine performed an operation that can't complete immediately:

- Sent on a channel that's not ready to receive
- Received from a channel that's empty
- Tried to acquire a locked mutex
- Waiting for I/O (network, disk)
- Sleep timer

The goroutine is parked (removed from CPU) and added to a wait queue. It consumes no CPU while waiting. When the condition is satisfied (channel ready, mutex unlocked, I/O completes, timer fires), the goroutine transitions back to Runnable.

**Dead**: The goroutine's function returned (either normally or via panic without recovery). The goroutine is now dead. Its stack is freed (returned to memory allocator), and the G structure might be reused for a new goroutine (optimization).

### Lifecycle Example

Let's trace a goroutine's lifecycle:

```go
func worker(id int, ch chan int) {
    // 1. Running: executing this code
    
    result := compute(id)  // 2. Running: computing
    
    ch <- result  // 3. Might block here if channel not ready
                  //    Transition to Waiting
                  //    When channel ready, back to Runnable
                  //    Then Running again
    
    // 4. Running: continues after send
    
    time.Sleep(100 * time.Millisecond)  // 5. Waiting (sleep timer)
    
    // 6. Running: resumes after sleep
    
    // 7. Dead: function returns
}

func main() {
    ch := make(chan int)
    
    go worker(1, ch)  // Worker goroutine: Created → Runnable
    
    time.Sleep(50 * time.Millisecond)  // Give worker time to send
    
    <-ch  // Main goroutine might block here if worker hasn't sent
          // Worker unblocks when we receive
}
```

**Timeline**:

```
T=0ms:
- Main: Running (starts)
- Worker: Created → Runnable

T=0ms (scheduler runs worker):
- Main: Waiting (sleep)
- Worker: Running (compute)

T=10ms (worker tries to send):
- Main: Still waiting (sleep)
- Worker: Waiting (blocked on channel send)

T=50ms (main wakes up):
- Main: Runnable → Running (receives from channel)
- Worker: Waiting → Runnable (send can now complete)

T=50ms (worker resumes):
- Main: Dead (exits after receive)
- Worker: Running → Waiting (sleep)

T=150ms (worker wakes):
- Worker: Running → Dead (function returns)
```

### Goroutine Leaks

A goroutine leak occurs when a goroutine stays in the Waiting state forever, never becoming Dead. This is a memory leak - the goroutine's stack and metadata remain allocated but unused.

**Common Causes**:

**Waiting on a channel that's never closed/sent on**:

```go
func leak() {
    ch := make(chan int)
    
    go func() {
        <-ch  // Waits forever! Channel never receives anything
    }()
    
    // Forgot to send or close ch
}
```

**Blocked mutex that's never unlocked**:

```go
var mu sync.Mutex

func leak() {
    go func() {
        mu.Lock()  // If mu is already locked and never unlocked, waits forever
        // ...
    }()
}
```

**Infinite loop that can't be stopped**:

```go
func leak() {
    go func() {
        for {
            // No way to exit this loop from outside
            doWork()
        }
    }()
}
```

Goroutine leaks accumulate over time. If you leak 100 goroutines per minute, after an hour you have 6,000 leaked goroutines consuming memory. Eventually, your program runs out of memory.

**Detection**: Use `runtime.NumGoroutine()` to track goroutine count. If it grows unboundedly, you have a leak.

---

## How Goroutines Work Internally {#how-goroutines-work}

To truly understand goroutines, let's peek under the hood at how the Go runtime implements them.

### The GMP Model Revisited

Remember from the runtime guide: Goroutines (G), Machines (M), and Processors (P).

```
Visual Representation:

┌────────────────────────────────────────────────────────────┐
│                     Go Runtime Scheduler                   │
└────────────────────────────────────────────────────────────┘

Goroutines (G) - What you create with "go func()":
┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐
│G1 │ │G2 │ │G3 │ │G4 │ │G5 │ │G6 │ │G7 │ │G8 │ │G9 │  ... (thousands more)
└───┘ └───┘ └───┘ └───┘ └───┘ └───┘ └───┘ └───┘ └───┘
  │     │     │     │     │     │     │     │     │
  └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴───→ Runnable queue

Processors (P) - Execution contexts:
┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│      P0     │  │      P1     │  │      P2     │  │      P3     │
│  Running: G1│  │  Running: G2│  │  Running: G3│  │  Running: G4│
│  Queue: G5,G│  │  Queue: G6  │  │  Queue: G7  │  │  Queue: G8  │
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       │                │                │                │
       ↓                ↓                ↓                ↓
Machines (M) - OS Threads:
┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│      M0     │  │      M1     │  │      M2     │  │      M3     │
│ OS Thread 1 │  │ OS Thread 2 │  │ OS Thread 3 │  │ OS Thread 4 │
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       ↓                ↓                ↓                ↓
┌─────────────────────────────────────────────────────────────┐
│                      CPU Cores (Hardware)                   │
│              Core 0     Core 1     Core 2     Core 3        │
└─────────────────────────────────────────────────────────────┘

Key: 1000s of goroutines → 4 processors → 4 OS threads → 4 CPU cores
```

**How It Works**:

When you create a goroutine with `go func()`, the runtime creates a G structure and adds it to a run queue (either a P's local queue or the global queue).

Each P (processor) has a local run queue of runnable goroutines. A P runs goroutines one at a time. When the current goroutine blocks or is preempted, the P picks the next goroutine from its queue.

Each M (machine/OS thread) must be bound to a P to execute goroutines. The number of Ps is set by GOMAXPROCS (default: number of CPU cores). So with 8 cores, you have 8 Ps and up to 8 Ms running goroutines in parallel.

When a goroutine blocks (channel, I/O, etc.), its M might hand off the P to another M so the P can continue running other goroutines. The blocked M sleeps until the goroutine unblocks.

This multiplexing is why Go can efficiently handle thousands of goroutines on a handful of OS threads.

### Goroutine Scheduling

When does the scheduler decide to switch goroutines?

**Voluntary Yielding**: The goroutine performs an operation that naturally yields control:

- Channel send/receive
- I/O operation
- Sleeping
- Calling `runtime.Gosched()` explicitly

**Preemption**: (Since Go 1.14) The goroutine has been running for "too long" (typically 10ms). The runtime sends a preemption signal, and at the next safe point (function call, memory allocation), the goroutine is preempted and the scheduler runs another goroutine.

**System Call**: The goroutine makes a system call (like reading a file). The M enters the kernel. The P hands off to another M so it can keep running other goroutines while the first M is blocked in the system call.

This ensures fairness - no single goroutine can monopolize a CPU (even with a tight loop) because it'll be preempted after 10ms.

### Work Stealing

To balance load across Ps, the runtime uses work stealing. When a P's local queue is empty:

1. Check the global run queue for goroutines
2. If still empty, randomly pick another P and steal half its goroutines
3. If still empty, check the network poller for goroutines waiting on I/O

This ensures work is distributed evenly. If one P is overloaded (has 100 goroutines queued) and another is idle, the idle P steals 50 goroutines from the busy P. Now both have work, utilizing both CPUs.

## Multiple Goroutines Working Together {#multiple-goroutines}

The real power of goroutines emerges when multiple goroutines work together to accomplish complex tasks. Let's explore how they coordinate and interact.

### Scenario 1: Simple Producer-Consumer

The simplest multi-goroutine pattern: one goroutine produces data, another consumes it.

```go
func producer(ch chan<- int) {
    for i := 1; i <= 5; i++ {
        fmt.Printf("Producing %d\n", i)
        ch <- i
        time.Sleep(100 * time.Millisecond)
    }
    close(ch)
}

func consumer(ch <-chan int) {
    for value := range ch {
        fmt.Printf("Consuming %d\n", value)
        time.Sleep(200 * time.Millisecond)
    }
}

func main() {
    ch := make(chan int, 2)  // Buffered for flexibility
    
    go producer(ch)
    consumer(ch)  // Runs in main goroutine
}
```

**Visual Timeline**:

```
Time    Producer Goroutine          Channel Buffer    Consumer Goroutine (Main)
────────────────────────────────────────────────────────────────────────────────
T=0ms   Produces 1
        Sends to channel ────────→  [1, _]           Receives 1
                                                      ↓
T=100ms Produces 2                                   Consuming 1...
        Sends to channel ────────→  [2, _]           

T=200ms Produces 3                  [2, 3]           Receives 2
        Sends to channel ────────→                   ↓
                                                      Consuming 2...
T=300ms Produces 4                  [3, 4]           
        Blocks (buffer full) ─X                      
                                                      
T=400ms Still blocked               [3, 4]           Receives 3
                                    ↓                ↓
T=400ms Unblocks, sends 4 ────────→ [4]             Consuming 3...
        
T=500ms Produces 5                  [4, 5]
        Sends to channel ────────→                   
        Closes channel                               
        Goroutine ends                               
                                                      
T=600ms                             [5]              Receives 4
                                    ↓                ↓
                                                     Consuming 4...
                                    
T=800ms                             []               Receives 5
                                    (closed)         ↓
                                                     Consuming 5...
                                                     
T=1000ms                                             Range loop exits
                                                     Program ends
```

**Key Observations**:

The producer runs ahead initially, filling the buffer. When the buffer fills (at T=300ms), the producer blocks, waiting for the consumer to catch up. This is automatic **backpressure** - the fast producer is throttled to match the slow consumer.

When the channel closes, the consumer's `range` loop receives all buffered values before exiting. No data is lost.

Both goroutines run concurrently. While the producer is producing, the consumer is consuming. They coordinate only through the channel.

### Scenario 2: Multiple Producers, Single Consumer

Now let's scale up: three producers, one consumer.

```go
func producer(id int, ch chan<- int) {
    for i := 1; i <= 3; i++ {
        value := id*100 + i
        fmt.Printf("Producer %d: sending %d\n", id, value)
        ch <- value
        time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
    }
}

func consumer(ch <-chan int) {
    for value := range ch {
        fmt.Printf("Consumer: received %d\n", value)
    }
}

func main() {
    ch := make(chan int, 5)
    
    // Start 3 producers
    var wg sync.WaitGroup
    for i := 1; i <= 3; i++ {
        wg.Add(1)
        go func(id int) {
            defer wg.Done()
            producer(id, ch)
        }(i)
    }
    
    // Wait for all producers, then close channel
    go func() {
        wg.Wait()
        close(ch)
    }()
    
    // Consume
    consumer(ch)
}
```

**Visual Representation**:

```
┌─────────────────────────────────────────────────────────────────┐
│                  Multiple Producers, One Consumer               │
└─────────────────────────────────────────────────────────────────┘

Producer 1 (G1)         Producer 2 (G2)         Producer 3 (G3)
      │                        │                        │
      │ 101                    │ 201                    │ 301
      └─────→ ┐                └─────→ ┐                └─────→ ┐
              │                        │                        │
      │ 102   ├──→ Channel Buffer      ├──→ [101, 201, 301]    │
      └─────→ ┘    (capacity 5)        └──                     │
                                                                 │
      │ 103                    │ 202                    │ 302   │
      └─────→                  └─────→                  └─────→ │
                                                                 ↓
                                              All values sent to channel
                                                                 ↓
                                                        Consumer Goroutine (Main)
                                                        Receives in some order:
                                                        [101, 201, 301, 102, ...]
                                                        
Key Points:
- All producers send to same channel concurrently
- Consumer receives values in arrival order (non-deterministic)
- WaitGroup tracks when all producers finish
- Helper goroutine closes channel after all producers done
```

**Order is Non-Deterministic**: The three producers run concurrently. Which one sends first? It depends on scheduling. The consumer receives values in arrival order, which varies between runs. One run might be [101, 201, 102, 301, ...], another [201, 101, 301, 102, ...].

This non-determinism is inherent to concurrent systems. If you need specific ordering, you must coordinate explicitly (with timestamps, sequence numbers, or serialization).

### Scenario 3: Single Producer, Multiple Consumers

Now flip it: one producer, three consumers sharing work.

```go
func producer(ch chan<- int) {
    for i := 1; i <= 9; i++ {
        ch <- i
        fmt.Printf("Produced %d\n", i)
    }
    close(ch)
}

func consumer(id int, ch <-chan int, wg *sync.WaitGroup) {
    defer wg.Done()
    
    for value := range ch {
        fmt.Printf("Consumer %d: processing %d\n", id, value)
        time.Sleep(100 * time.Millisecond)  // Simulate work
    }
}

func main() {
    ch := make(chan int)
    var wg sync.WaitGroup
    
    // Start 3 consumers
    for i := 1; i <= 3; i++ {
        wg.Add(1)
        go consumer(i, ch, &wg)
    }
    
    // Produce
    producer(ch)
    
    // Wait for consumers
    wg.Wait()
}
```

**Visual Distribution**:

```
┌─────────────────────────────────────────────────────────────────┐
│                 One Producer, Multiple Consumers                │
└─────────────────────────────────────────────────────────────────┘

                        Producer (Main Goroutine)
                                 │
                    Sends: 1, 2, 3, 4, 5, 6, 7, 8, 9
                                 │
                                 ↓
                           Channel (unbuffered)
                                 │
                   ┌─────────────┼─────────────┐
                   │             │             │
                   ↓             ↓             ↓
            Consumer 1      Consumer 2      Consumer 3
            (Goroutine)     (Goroutine)     (Goroutine)
                 │               │               │
            Receives:       Receives:       Receives:
            1, 4, 7         2, 5, 8         3, 6, 9
            
Timeline:
T=0ms:   Producer sends 1 → Consumer 1 receives
T=0ms:   Producer sends 2 → Consumer 2 receives
T=0ms:   Producer sends 3 → Consumer 3 receives
T=100ms: Producer sends 4 → Consumer 1 receives (finished 1)
T=100ms: Producer sends 5 → Consumer 2 receives (finished 2)
T=100ms: Producer sends 6 → Consumer 3 receives (finished 3)
T=200ms: Producer sends 7 → Consumer 1 receives (finished 4)
T=200ms: Producer sends 8 → Consumer 2 receives (finished 5)
T=200ms: Producer sends 9 → Consumer 3 receives (finished 6)
T=300ms: All consumers finish, WaitGroup completes
```

**Automatic Load Balancing**: The channel automatically distributes work to whichever consumer is available. If Consumer 1 finishes first, it gets the next value. If Consumer 3 is slow, it gets fewer values. No manual load balancing code needed - the channel does it automatically.

This pattern is perfect for parallelizing work. One goroutine produces tasks, multiple workers process them in parallel.

---

## Coordination Patterns {#coordination-patterns}

Goroutines working together need coordination. Let's explore the key patterns.

### Pattern 1: Fan-Out, Fan-In

**Fan-Out**: Distribute work from one goroutine to many workers. **Fan-In**: Collect results from many workers into one channel.

```go
// Fan-Out: Start N workers
func fanOut(input <-chan int, n int) []<-chan int {
    outputs := make([]<-chan int, n)
    
    for i := 0; i < n; i++ {
        out := make(chan int)
        outputs[i] = out
        
        go func(ch chan<- int) {
            for value := range input {
                result := process(value)  // Worker processes
                ch <- result
            }
            close(ch)
        }(out)
    }
    
    return outputs
}

// Fan-In: Merge N channels into one
func fanIn(inputs []<-chan int) <-chan int {
    out := make(chan int)
    var wg sync.WaitGroup
    
    for _, input := range inputs {
        wg.Add(1)
        go func(ch <-chan int) {
            defer wg.Done()
            for value := range ch {
                out <- value
            }
        }(input)
    }
    
    go func() {
        wg.Wait()
        close(out)
    }()
    
    return out
}
```

**Visualization**:

```
┌─────────────────────────────────────────────────────────────────┐
│                     Fan-Out, Fan-In Pattern                     │
└─────────────────────────────────────────────────────────────────┘

                            Input Channel
                            [1,2,3,4,5,6,7,8,9]
                                   │
                    ┌──────────────┼──────────────┐
                    │              │              │
            ┌───────▼───────┐ ┌───▼──────┐ ┌────▼─────┐
            │   Worker 1    │ │ Worker 2 │ │ Worker 3 │  FAN-OUT
            │ Processes:    │ │Processes:│ │Processes:│
            │ 1, 4, 7       │ │ 2, 5, 8  │ │ 3, 6, 9  │
            └───────┬───────┘ └───┬──────┘ └────┬─────┘
                    │              │              │
                    │    Output    │    Output    │
                    │   Channel 1  │  Channel 2   │   Output
                    │              │              │  Channel 3
                    └──────────────┼──────────────┘
                                   │
                           ┌───────▼────────┐
                           │   Merge        │  FAN-IN
                           │  (Fan-In)      │
                           │  All results   │
                           │  → one channel │
                           └───────┬────────┘
                                   │
                           Output: [r1,r2,r3,r4,r5,r6,r7,r8,r9]
                           (in completion order)
```

This pattern is ideal for parallelizing independent work and collecting results.

### Pattern 2: Pipeline

A series of stages, each processing data and passing it to the next stage.

```go
func stage1(input <-chan int) <-chan int {
    out := make(chan int)
    go func() {
        for value := range input {
            out <- value * 2  // Double it
        }
        close(out)
    }()
    return out
}

func stage2(input <-chan int) <-chan int {
    out := make(chan int)
    go func() {
        for value := range input {
            out <- value + 1  // Add 1
        }
        close(out)
    }()
    return out
}

func stage3(input <-chan int) <-chan int {
    out := make(chan int)
    go func() {
        for value := range input {
            out <- value * value  // Square it
        }
        close(out)
    }()
    return out
}

func main() {
    // Create input
    input := make(chan int)
    go func() {
        for i := 1; i <= 5; i++ {
            input <- i
        }
        close(input)
    }()
    
    // Connect pipeline
    output := stage3(stage2(stage1(input)))
    
    // Consume results
    for result := range output {
        fmt.Println(result)
    }
}
```

**Visualization**:

```
┌─────────────────────────────────────────────────────────────────┐
│                      Pipeline Pattern                           │
└─────────────────────────────────────────────────────────────────┘

Input: [1, 2, 3, 4, 5]
   │
   ↓
┌──────────────┐
│   Stage 1    │  (Goroutine 1)
│   x * 2      │  Processing: 1→2, 2→4, 3→6, 4→8, 5→10
└──────┬───────┘
       │ [2, 4, 6, 8, 10]
       ↓
┌──────────────┐
│   Stage 2    │  (Goroutine 2)
│   x + 1      │  Processing: 2→3, 4→5, 6→7, 8→9, 10→11
└──────┬───────┘
       │ [3, 5, 7, 9, 11]
       ↓
┌──────────────┐
│   Stage 3    │  (Goroutine 3)
│   x * x      │  Processing: 3→9, 5→25, 7→49, 9→81, 11→121
└──────┬───────┘
       │
       ↓
Output: [9, 25, 49, 81, 121]

Concurrency:
- All 3 stages run simultaneously (3 goroutines)
- Stage 1 processes value 3 while Stage 2 processes value 2
  and Stage 3 processes value 1 (all at the same time)
- Each stage runs at its own pace
- Channels provide automatic buffering and backpressure
```

Pipelines enable efficient streaming data processing. Each stage can run at its own speed, and the system self-regulates through channel blocking.

### Pattern 3: Worker Pool (Revisited for Goroutines)

A fixed number of worker goroutines processing tasks from a queue.

```go
func workerPool(numWorkers int, tasks <-chan Task, results chan<- Result) {
    var wg sync.WaitGroup
    
    for i := 0; i < numWorkers; i++ {
        wg.Add(1)
        go func(workerID int) {
            defer wg.Done()
            
            for task := range tasks {
                fmt.Printf("Worker %d: processing task %d\n", 
                           workerID, task.ID)
                result := process(task)
                results <- result
            }
        }(i)
    }
    
    wg.Wait()
    close(results)
}
```

**Visualization**:

```
┌─────────────────────────────────────────────────────────────────┐
│                      Worker Pool Pattern                        │
└─────────────────────────────────────────────────────────────────┘

                    Task Queue (Channel)
                    [T1, T2, T3, T4, T5, T6, T7, T8, T9, T10]
                              │
              ┌───────────────┼───────────────┬───────────────┐
              │               │               │               │
         ┌────▼─────┐    ┌───▼──────┐   ┌───▼──────┐   ┌───▼──────┐
         │ Worker 1 │    │ Worker 2 │   │ Worker 3 │   │ Worker 4 │
         │          │    │          │   │          │   │          │
         │ T1 → R1  │    │ T2 → R2  │   │ T3 → R3  │   │ T4 → R4  │
         │ T5 → R5  │    │ T6 → R6  │   │ T7 → R7  │   │ T8 → R8  │
         │ T9 → R9  │    │ T10→ R10 │   │          │   │          │
         └────┬─────┘    └───┬──────┘   └───┬──────┘   └───┬──────┘
              │               │               │               │
              └───────────────┼───────────────┴───────────────┘
                              │
                     Results Channel
                     [R1, R2, R3, R4, R5, R6, R7, R8, R9, R10]
                     (order depends on completion time)

Timeline:
T=0ms:   All 4 workers take a task (T1, T2, T3, T4)
T=50ms:  Worker 2 finishes T2, takes T5
T=60ms:  Worker 3 finishes T3, takes T6
T=70ms:  Worker 1 finishes T1, takes T7
T=80ms:  Worker 4 finishes T4, takes T8
T=100ms: Worker 2 finishes T5, takes T9
T=110ms: Worker 3 finishes T6, takes T10
T=120ms: Worker 1 finishes T7 (no more tasks)
T=130ms: Worker 4 finishes T8 (no more tasks)
T=150ms: Worker 2 finishes T9 (no more tasks)
T=160ms: Worker 3 finishes T10 (no more tasks)
T=160ms: All workers done, results channel closed
```

The worker pool provides bounded concurrency (exactly N workers) and automatic load balancing (faster workers get more tasks).

---

## Visualization of Goroutine Execution {#visualization}

Let's visualize how goroutines actually execute on hardware.

### Scenario: 8 Goroutines on 4 CPU Cores

```
System: 4 CPU cores, GOMAXPROCS=4 (4 Ps, 4 Ms)
Goroutines: G1, G2, G3, G4, G5, G6, G7, G8

┌─────────────────────────────────────────────────────────────────┐
│              Goroutine Execution Timeline                       │
└─────────────────────────────────────────────────────────────────┘

Time
│
│   CPU Core 0       CPU Core 1       CPU Core 2       CPU Core 3
│   ────────────     ────────────     ────────────     ────────────
│
0ms │   G1              G2              G3              G4
│   │                                                   
│   │ (running)       (running)       (running)       (running)
│   │
10ms│   G1              G2              G3              G4
│   │ (blocks on ch)  
│   │ ─────X          
│   │   G5              G2              G3              G4
│   │ (scheduled)     (still running)
│   │
20ms│   G5              G2              G3              G4
│   │                 (blocks on lock)
│   │                 ─────X
│   │   G5              G6              G3              G4
│   │                 (scheduled)
│   │
30ms│   G5              G6              G3              G4
│   │                                 (completes)
│   │                                 (dead)
│   │   G5              G6              G7              G4
│   │                                 (scheduled)
│   │
40ms│   G5              G6              G7              G4
│   │                                                 (completes)
│   │                                                 (dead)
│   │   G5              G6              G7              G8
│   │                                                 (scheduled)
│   │
50ms│   G5              G6              G7              G8
│   │ (completes)
│   │ (dead)
│   │   G1              G6              G7              G8
│   │ (unblocked)
│   │ (scheduled)
│   │
60ms│   G1              G6              G7              G8
│   │                 (completes)
│   │                 (dead)
│   │   G1              G2              G7              G8
│   │                 (unblocked)
│   │                 (scheduled)
│   │
│   ... and so on until all goroutines complete

Key:
- At any moment, up to 4 goroutines run in parallel (4 cores)
- When a goroutine blocks, another takes its place
- Blocked goroutines wait off-CPU (shown as ─X)
- Completed goroutines are marked as dead
- The scheduler continuously assigns runnable goroutines to available cores
```

**Key Observations**:

**True Parallelism**: At T=0ms, G1, G2, G3, G4 all execute simultaneously on different cores. This is true parallel execution.

**Blocking and Switching**: When G1 blocks on a channel at T=10ms, CPU Core 0 doesn't sit idle. The scheduler immediately runs G5 on that core. The CPU is always doing useful work.

**Efficient CPU Utilization**: With 8 goroutines and 4 cores, we keep all cores busy almost all the time. If we had only 4 goroutines and one blocked, we'd have idle CPU. Having more goroutines than cores ensures full utilization.

**Fairness**: Goroutines don't run forever. After ~10ms, they're preempted to give others a turn. This prevents one goroutine from monopolizing a CPU.

### Scenario: I/O-Bound Goroutines (Network Requests)

```
System: 4 CPU cores
Goroutines: 100 goroutines making network requests

┌─────────────────────────────────────────────────────────────────┐
│           I/O-Bound Goroutine Execution                         │
└─────────────────────────────────────────────────────────────────┘

Typical goroutine lifecycle:
│
│ Phase 1: CPU work (preparing request)
│ ▓▓ (2ms CPU time)
│
│ Phase 2: Network I/O (waiting for response)
│ ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ (200ms blocked/waiting)
│
│ Phase 3: CPU work (processing response)
│ ▓▓ (2ms CPU time)
│
│ Total: 204ms elapsed, 4ms CPU time, 200ms waiting
│
│ CPU utilization per goroutine: 4/204 = 2%

With 100 such goroutines:

CPU Timeline:
│
│   Core 0         Core 1         Core 2         Core 3
│   ──────────     ──────────     ──────────     ──────────
│
0ms│ G1 ▓▓         G2 ▓▓         G3 ▓▓         G4 ▓▓
│   │ ↓ wait        ↓ wait        ↓ wait        ↓ wait
│   │ G5 ▓▓         G6 ▓▓         G7 ▓▓         G8 ▓▓
│   │ ↓ wait        ↓ wait        ↓ wait        ↓ wait
│   │ G9 ▓▓         G10 ▓▓        G11 ▓▓        G12 ▓▓
│   │ ... all 100 goroutines do CPU work and start waiting ...
│   │
50ms│ (All 100 waiting)
│   │ No CPU work, all cores idle (or doing other work)
│   │
200ms│ Network responses start arriving
│   │ G1 response → G1 unblocked
│   │ G1 ▓▓ (process response)
│   │ G2 response → G2 unblocked
│   │    G2 ▓▓
│   │ ... responses arrive, goroutines process them ...
│   │
250ms│ All goroutines complete

Key Insight:
- 100 goroutines, but only ~10ms total CPU time per goroutine
- Most time spent waiting (I/O)
- 4 CPU cores easily handle 100 concurrent operations
- OS threads (4) are not blocked during I/O wait
- If we used OS threads (1 per operation), we'd need 100 threads!
```

This shows why goroutines are perfect for I/O-bound work. You can have thousands of concurrent I/O operations with just a few OS threads because goroutines waiting for I/O don't consume CPU or OS thread resources.

## Common Patterns and Architectures {#patterns}

Let's explore complete, production-ready patterns for organizing multiple goroutines.

### Pattern 1: Request-Response Server

A server with goroutines handling requests concurrently.

```go
type Server struct {
    requests  chan Request
    responses chan Response
    workers   int
}

func NewServer(workers int) *Server {
    s := &Server{
        requests:  make(chan Request, 100),
        responses: make(chan Response, 100),
        workers:   workers,
    }
    
    // Start worker goroutines
    for i := 0; i < workers; i++ {
        go s.worker(i)
    }
    
    return s
}

func (s *Server) worker(id int) {
    for req := range s.requests {
        // Process request
        response := processRequest(req)
        s.responses <- response
    }
}

func (s *Server) HandleRequest(req Request) Response {
    // Send request to worker pool
    s.requests <- req
    
    // Wait for response
    return <-s.responses
}
```

**Goroutine Structure**:

```
┌─────────────────────────────────────────────────────────────┐
│                Request-Response Server                      │
└─────────────────────────────────────────────────────────────┘

Client Goroutines                Worker Pool              
                                                          
G_client1 ──request──┐                                   
                     │                                   
G_client2 ──request──┤                                   
                     │                                   
G_client3 ──request──┼──→ Request Channel               
                     │         │                        
G_client4 ──request──┤         │                        
                     │         ↓                        
         ...         │    ┌─────────┐                   
                     │    │ Worker 1│  (Goroutine)      
                     │    └─────────┘                   
                     │    ┌─────────┐                   
                     │    │ Worker 2│  (Goroutine)      
                     │    └─────────┘                   
                     │    ┌─────────┐                   
                     │    │ Worker 3│  (Goroutine)      
                     │    └─────────┘                   
                     │         │                        
                     │         ↓                        
                     └────Response Channel              
                              │                         
G_client1 ←─response─────────┤                        
                              │                         
G_client2 ←─response─────────┤                        
                              │                         
G_client3 ←─response─────────┘                        

- Each client goroutine sends request, blocks waiting for response
- Worker goroutines process requests concurrently
- Bounded concurrency (N workers)
- Automatic load balancing
```

### Pattern 2: Pub-Sub (Publisher-Subscriber)

Multiple subscribers receiving messages from publishers.

```go
type PubSub struct {
    mu          sync.RWMutex
    subscribers map[string][]chan Message
    buffer      int
}

func NewPubSub(buffer int) *PubSub {
    return &PubSub{
        subscribers: make(map[string][]chan Message),
        buffer:      buffer,
    }
}

func (ps *PubSub) Subscribe(topic string) <-chan Message {
    ps.mu.Lock()
    defer ps.mu.Unlock()
    
    ch := make(chan Message, ps.buffer)
    ps.subscribers[topic] = append(ps.subscribers[topic], ch)
    return ch
}

func (ps *PubSub) Publish(topic string, msg Message) {
    ps.mu.RLock()
    defer ps.mu.RUnlock()
    
    for _, ch := range ps.subscribers[topic] {
        // Non-blocking send (skip if subscriber slow)
        select {
        case ch <- msg:
        default:
            // Subscriber buffer full, skip
        }
    }
}

// Usage
func main() {
    ps := NewPubSub(10)
    
    // Start subscribers
    for i := 1; i <= 3; i++ {
        ch := ps.Subscribe("news")
        go func(id int) {
            for msg := range ch {
                fmt.Printf("Subscriber %d: %v\n", id, msg)
            }
        }(i)
    }
    
    // Publish messages
    go func() {
        for i := 1; i <= 10; i++ {
            ps.Publish("news", Message{Data: fmt.Sprintf("News %d", i)})
            time.Sleep(100 * time.Millisecond)
        }
    }()
    
    time.Sleep(2 * time.Second)
}
```

**Goroutine Structure**:

```
┌─────────────────────────────────────────────────────────────┐
│                  Publisher-Subscriber Pattern               │
└─────────────────────────────────────────────────────────────┘

Publisher Goroutines         PubSub Broker         Subscriber Goroutines

G_pub1 ─┐                                          ┌─→ G_sub1 (topic: news)
        │                                          │
G_pub2 ─┤──→ Publish("news", msg) ───────────────┼─→ G_sub2 (topic: news)
        │                                          │
G_pub3 ─┘                                          └─→ G_sub3 (topic: news)


G_pub4 ─────→ Publish("sports", msg) ───────────┬─→ G_sub4 (topic: sports)
                                                 │
                                                 └─→ G_sub5 (topic: sports)

Message Flow:
1. Publisher calls Publish(topic, msg)
2. Broker finds all subscribers to that topic
3. Broker sends message to each subscriber's channel (non-blocking)
4. Each subscriber goroutine receives and processes message

Key Features:
- Decoupling: Publishers don't know about subscribers
- Broadcasting: One publish → many subscribers
- Topic filtering: Each subscriber chooses topics
- Non-blocking: Slow subscriber doesn't block publisher
```

### Pattern 3: Supervisor (Restart Failed Goroutines)

A supervisor that restarts goroutines if they panic or exit.

```go
type Supervisor struct {
    workers map[string]WorkerFunc
    mu      sync.Mutex
    quit    chan struct{}
}

type WorkerFunc func() error

func NewSupervisor() *Supervisor {
    return &Supervisor{
        workers: make(map[string]WorkerFunc),
        quit:    make(chan struct{}),
    }
}

func (s *Supervisor) AddWorker(name string, fn WorkerFunc) {
    s.mu.Lock()
    s.workers[name] = fn
    s.mu.Unlock()
    
    go s.supervise(name, fn)
}

func (s *Supervisor) supervise(name string, fn WorkerFunc) {
    for {
        select {
        case <-s.quit:
            return
        default:
            // Run worker with panic recovery
            func() {
                defer func() {
                    if r := recover(); r != nil {
                        fmt.Printf("Worker %s panicked: %v\n", name, r)
                        time.Sleep(1 * time.Second)  // Backoff
                    }
                }()
                
                if err := fn(); err != nil {
                    fmt.Printf("Worker %s error: %v\n", name, err)
                    time.Sleep(1 * time.Second)  // Backoff
                }
            }()
        }
    }
}

func (s *Supervisor) Shutdown() {
    close(s.quit)
}
```

**Goroutine Structure**:

```
┌─────────────────────────────────────────────────────────────┐
│                    Supervisor Pattern                       │
└─────────────────────────────────────────────────────────────┘

                        Supervisor Goroutine
                                │
                ┌───────────────┼───────────────┐
                │               │               │
         ┌──────▼──────┐ ┌─────▼──────┐ ┌─────▼──────┐
         │ Supervisor  │ │ Supervisor │ │ Supervisor │
         │ for Worker 1│ │for Worker 2│ │for Worker 3│
         └──────┬──────┘ └─────┬──────┘ └─────┬──────┘
                │               │               │
         ┌──────▼──────┐ ┌─────▼──────┐ ┌─────▼──────┐
         │  Worker 1   │ │  Worker 2  │ │  Worker 3  │
         │ (Monitored) │ │ (Monitored)│ │ (Monitored)│
         └─────────────┘ └────────────┘ └────────────┘

Worker Lifecycle:
1. Worker runs normally
2. If worker exits (error or panic):
   - Supervisor catches it
   - Waits briefly (backoff)
   - Restarts worker
3. Repeat until supervisor shutdown

Benefits:
- Fault tolerance: Failures don't crash the system
- Self-healing: Workers restart automatically
- Monitoring: Supervisor logs failures
```

---

## Debugging and Troubleshooting {#debugging}

When goroutines misbehave, debugging can be challenging. Here are essential techniques.

### Problem 1: Goroutine Leaks

**Symptom**: Memory usage grows over time. Number of goroutines increases unboundedly.

**Diagnosis**:

```go
import "runtime"

// Periodically check goroutine count
func monitorGoroutines() {
    ticker := time.NewTicker(10 * time.Second)
    for range ticker.C {
        n := runtime.NumGoroutine()
        fmt.Printf("Number of goroutines: %d\n", n)
    }
}

// In main
go monitorGoroutines()
```

If the count grows without bound, you have a leak.

**Finding the Leak**:

Use pprof to see what goroutines are doing:

```go
import _ "net/http/pprof"

func main() {
    go func() {
        log.Println(http.ListenAndServe("localhost:6060", nil))
    }()
    
    // Your program
}
```

Visit `http://localhost:6060/debug/pprof/goroutine?debug=2` to see all goroutines and their stack traces.

**Common Causes**:

```go
// Leak: Goroutine waiting on channel that's never closed/sent
func leak1() {
    ch := make(chan int)
    go func() {
        <-ch  // Waits forever
    }()
}

// Leak: Goroutine with infinite loop and no exit condition
func leak2() {
    go func() {
        for {
            // No way to stop this loop
            doWork()
        }
    }()
}

// Fix: Provide cancellation mechanism
func fixed() {
    quit := make(chan struct{})
    go func() {
        for {
            select {
            case <-quit:
                return  // Exit loop
            default:
                doWork()
            }
        }
    }()
    
    // Later, to stop:
    close(quit)
}
```

### Problem 2: Deadlocks

**Symptom**: Program hangs. All goroutines are blocked.

**Go's Deadlock Detector**: If all goroutines are blocked (none runnable), Go panics with "all goroutines are asleep - deadlock!"

**Example Deadlock**:

```go
func deadlock() {
    ch := make(chan int)
    
    ch <- 42  // Blocks forever (unbuffered channel, no receiver)
    
    // Never reached
    <-ch
}
```

Go detects this and panics. But in complex programs, partial deadlocks (some goroutines blocked, others still running) won't be detected.

**Debugging**:

Enable goroutine stack dumps on deadlock. When your program hangs, send SIGQUIT (Ctrl+\ on Unix):

```bash
kill -QUIT <pid>
```

This prints stack traces of all goroutines, showing what they're waiting on.

**Common Causes**:

```go
// Circular dependency
func circular() {
    ch1 := make(chan int)
    ch2 := make(chan int)
    
    go func() {
        ch1 <- 1
        <-ch2  // Waits for ch2
    }()
    
    go func() {
        ch2 <- 2
        <-ch1  // Waits for ch1
    }()
    
    // Both goroutines block: deadlock
}

// Mutex lock order inversion
var mu1, mu2 sync.Mutex

func order1() {
    mu1.Lock()
    mu2.Lock()
    // work
    mu2.Unlock()
    mu1.Unlock()
}

func order2() {
    mu2.Lock()  // Different order!
    mu1.Lock()
    // work
    mu1.Unlock()
    mu2.Unlock()
}

// If goroutine 1 runs order1() and goroutine 2 runs order2():
// - G1 locks mu1, waits for mu2
// - G2 locks mu2, waits for mu1
// Deadlock!
```

### Problem 3: Race Conditions

**Symptom**: Non-deterministic behavior. Data corruption. Occasional crashes.

**Diagnosis**: Use Go's race detector:

```bash
go run -race myprogram.go
go test -race ./...
```

The race detector instruments your code to detect concurrent accesses to the same memory location where at least one is a write.

**Example Race**:

```go
var counter int

func race() {
    for i := 0; i < 1000; i++ {
        go func() {
            counter++  // RACE: Multiple goroutines write concurrently
        }()
    }
}
```

**Fix**:

```go
var counter int64

func fixed() {
    for i := 0; i < 1000; i++ {
        go func() {
            atomic.AddInt64(&counter, 1)  // Atomic operation
        }()
    }
}
```

### Problem 4: Channel Direction Errors

**Symptom**: Send on receive-only channel (or vice versa) causes compile error.

This is actually a feature! The compiler catches these bugs.

```go
func sender(ch <-chan int) {
    ch <- 42  // Compile error: cannot send on receive-only channel
}

func receiver(ch chan<- int) {
    <-ch  // Compile error: cannot receive from send-only channel
}
```

If you get these errors, check your function signatures. Ensure send-only channels are `chan<-` and receive-only are `<-chan`.

---

## Best Practices {#best-practices}

Essential guidelines for working with goroutines.

### Practice 1: Always Provide a Way to Stop Goroutines

**Bad**: Goroutine with no exit condition

```go
go func() {
    for {
        doWork()  // Runs forever
    }
}()
```

**Good**: Goroutine with cancellation

```go
done := make(chan struct{})

go func() {
    for {
        select {
        case <-done:
            return  // Exits cleanly
        default:
            doWork()
        }
    }
}()

// Later, to stop:
close(done)
```

**Better**: Use context for cancellation

```go
ctx, cancel := context.WithCancel(context.Background())

go func() {
    for {
        select {
        case <-ctx.Done():
            return
        default:
            doWork()
        }
    }
}()

// Later, to stop:
cancel()
```

### Practice 2: Handle Goroutine Panics

Panics in goroutines don't crash just that goroutine - they crash the entire program. Always recover from panics in goroutines:

```go
go func() {
    defer func() {
        if r := recover(); r != nil {
            fmt.Printf("Goroutine panicked: %v\n", r)
            // Log error, maybe restart goroutine
        }
    }()
    
    // Goroutine work (might panic)
    riskyWork()
}()
```

### Practice 3: Don't Start Goroutines in Libraries (Usually)

If you're writing a library, avoid starting goroutines internally. Let the caller decide:

**Bad**:

```go
func DoWork(data []Item) {
    // Library starts goroutine internally
    go processInBackground(data)
}
```

The caller has no control over this goroutine. They can't stop it, can't track it, can't coordinate with it.

**Good**:

```go
func DoWork(data []Item) <-chan Result {
    results := make(chan Result)
    
    // Return channel, let caller decide to run goroutine
    return results
}

// Caller decides:
results := DoWork(data)

go func() {
    for result := range results {
        handle(result)
    }
}()
```

Or provide explicit Start/Stop methods:

```go
type Worker struct {
    // ...
}

func (w *Worker) Start() {
    go w.run()
}

func (w *Worker) Stop() {
    close(w.quit)
}
```

### Practice 4: Size Goroutine Pools Appropriately

**CPU-Bound Work**: Pool size = number of CPU cores

```go
numWorkers := runtime.NumCPU()
```

**I/O-Bound Work**: Pool size = much larger (10-100x CPU cores)

```go
numWorkers := runtime.NumCPU() * 10
```

**External Rate Limits**: Pool size = rate limit

```go
numWorkers := 100  // API allows 100 concurrent requests
```

### Practice 5: Use Buffered Channels Judiciously

**Unbuffered**: For synchronization, strict rendezvous

```go
done := make(chan struct{})  // Signal channel
```

**Small Buffer**: Avoid blocking on temporary bursts

```go
ch := make(chan Task, 10)  // Absorb small bursts
```

**Buffer Size = Number of Producers**: Avoid producer blocking

```go
results := make(chan Result, numWorkers)  // Workers never block on send
```

**Don't**: Use huge buffers to hide design problems

```go
ch := make(chan Task, 1000000)  // Bad! Hiding deadlock or backpressure issue
```

### Practice 6: Always Clean Up (Close Channels, Cancel Contexts)

```go
func worker(tasks <-chan Task, done chan<- struct{}) {
    defer func() {
        close(done)  // Signal completion
    }()
    
    for task := range tasks {
        process(task)
    }
}

func main() {
    tasks := make(chan Task)
    done := make(chan struct{})
    
    go worker(tasks, done)
    
    // Send tasks
    for _, task := range allTasks {
        tasks <- task
    }
    
    close(tasks)  // Signal no more tasks
    <-done        // Wait for worker to finish
}
```

### Practice 7: Document Goroutine Ownership

Make it clear which goroutine owns resources:

```go
// NewServer creates a server.
// The server starts background goroutines that are stopped when Stop() is called.
// The caller is responsible for calling Stop() to avoid goroutine leaks.
func NewServer() *Server {
    s := &Server{...}
    go s.worker()  // Document: caller must call Stop()
    return s
}

func (s *Server) Stop() {
    close(s.quit)  // Stops background goroutines
}
```

---

## Summary: The Goroutine Orchestra

Goroutines are Go's superpower - lightweight, efficient, and simple to use. But like an orchestra, their true power emerges when they work together in harmony.

**Key Principles**:

**Independence**: Each goroutine is an independent unit of execution. It has its own stack, executes its own code, and doesn't directly interfere with other goroutines.

**Coordination**: Goroutines coordinate through channels (communication) and synchronization primitives (mutexes, waitgroups). This coordination is explicit in code, making concurrent logic clear.

**Efficiency**: Goroutines are cheap enough that you can have thousands or millions. This enables programming patterns impossible with OS threads.

**Safety**: Go provides tools (race detector, deadlock detector) and patterns (channel directions, contexts) to write safe concurrent code.

**The Conductor's Role**: You, the programmer, are the conductor. You decide when to start goroutines, how they communicate, and when they stop. With practice, you'll compose beautiful concurrent symphonies that efficiently utilize resources and scale effortlessly.

Remember: Start simple. Add concurrency only when needed. Measure the impact. Debug carefully. And always, always provide a way to stop your goroutines cleanly.

With goroutines, Go makes concurrent programming accessible and powerful. Master them, and you unlock the full potential of modern multi-core systems.