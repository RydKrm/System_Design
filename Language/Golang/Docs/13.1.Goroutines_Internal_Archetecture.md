# The Complete Guide to Goroutine Internal Architecture

## Table of Contents

1. [Introduction - The Magic of Goroutines](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#introduction)
2. [CPU and Memory Fundamentals](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#cpu-memory)
3. [Operating System Threads](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#os-threads)
4. [The Problem Goroutines Solve](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#problem)
5. [The Go Scheduler (GMP Model)](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#gmp-model)
6. [Goroutine Creation](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#goroutine-creation)
7. [Goroutine Execution](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#goroutine-execution)
8. [Goroutine Lifecycle](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#goroutine-lifecycle)
9. [Stack Management](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#stack-management)
10. [Blocking and System Calls](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#blocking)
11. [Work Stealing](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#work-stealing)
12. [Memory and Performance](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#memory-performance)

---

## Introduction - The Magic of Goroutines {#introduction}

Imagine you're running a restaurant. In a traditional threading model (like Java or C++), each customer (task) gets their own dedicated waiter (OS thread). If you have 10,000 customers, you need 10,000 waiters. This is expensive and chaotic - waiters bump into each other, you need a huge staff room, and coordinating everyone becomes a nightmare.

Now imagine Go's goroutine model. You have a small team of experienced waiters (OS threads), maybe 4-8 depending on your kitchen size (CPU cores). These waiters are incredibly efficient - they can switch between serving multiple customers seamlessly. When one customer needs time to decide (blocking operation), the waiter immediately moves to another customer. This is how goroutines work.

### What Makes Goroutines Special

Goroutines are lightweight, user-space threads managed by the Go runtime rather than the operating system. They're so cheap that you can create millions of them. A typical goroutine starts with only 2KB of stack space, compared to 1-2MB for an OS thread. This is a 500x difference!

```
Comparison:

OS Thread (Java, C++, Python):
┌─────────────────────────────────────┐
│ Stack: 1-2MB (fixed)                │
│ Context switch: ~1-2 microseconds   │
│ Creation cost: Expensive            │
│ Managed by: Operating System        │
│ Limit: ~10,000 threads per system   │
└─────────────────────────────────────┘

Goroutine (Go):
┌─────────────────────────────────────┐
│ Stack: 2KB (grows dynamically)      │
│ Context switch: ~0.2 microseconds   │
│ Creation cost: Very cheap           │
│ Managed by: Go Runtime              │
│ Limit: Millions per system          │
└─────────────────────────────────────┘
```

### The Journey Ahead

In this guide, we'll explore goroutines from the absolute lowest level - CPU registers and memory addresses - all the way up to how you write `go func()` in your code. We'll understand:

- How CPUs execute instructions and manage threads
- Why OS threads are expensive
- The brilliant Go scheduler (GMP model)
- How goroutines are created in memory
- The complete lifecycle from birth to death
- Stack growth and memory management
- Blocking operations and work stealing
- Performance characteristics

By the end, you'll understand exactly what happens when you type `go func()` and why Go is so efficient at concurrency.

---

## CPU and Memory Fundamentals {#cpu-memory}

To understand goroutines deeply, we must first understand the hardware they run on. Let's start from the very bottom - the CPU and memory.

### The CPU: The Brain of Execution

A modern CPU is an incredibly complex piece of silicon, but at its core, it does a simple job: fetch instructions, decode them, and execute them. Let's understand the key components relevant to goroutines.

#### CPU Cores

Modern CPUs have multiple cores, each capable of executing instructions independently. When you have 8 cores, you can literally execute 8 different instruction streams simultaneously.

```
CPU Architecture (4-core example):

┌────────────────────────────────────────────────────────┐
│                    CPU Die                             │
│                                                        │
│  ┌──────────────┐  ┌──────────────┐                    │
│  │   Core 0     │  │   Core 1     │                    │
│  │ ┌──────────┐ │  │ ┌──────────┐ │                    │
│  │ │ ALU+FPU  │ │  │ │ ALU+FPU  │ │                    │
│  │ │ Registers│ │  │ │ Registers│ │                    │
│  │ └──────────┘ │  │ └──────────┘ │                    │
│  │   L1 Cache   │  │   L1 Cache   │                    │
│  │    32KB      │  │    32KB      │                    │
│  └──────────────┘  └──────────────┘                    │
│         │                 │                            │
│         └────────┬────────┘                            │
│              L2 Cache                                  │
│               256KB                                    │
│                  │                                     │
│  ┌──────────────┐  ┌──────────────┐                    │
│  │   Core 2     │  │   Core 3     │                    │
│  │ ┌──────────┐ │  │ ┌──────────┐ │                    │
│  │ │ ALU+FPU  │ │  │ │ ALU+FPU  │ │                    │
│  │ │ Registers│ │  │ │ Registers│ │                    │
│  │ └──────────┘ │  │ └──────────┘ │                    │
│  │   L1 Cache   │  │   L1 Cache   │                    │
│  │    32KB      │  │    32KB      │                    │
│  └──────────────┘  └──────────────┘                    │
│         │                 │                            │
│         └────────┬────────┘                            │
│              L2 Cache                                  │
│               256KB                                    │
│                  │                                     │
│              L3 Cache (Shared)                         │
│                 8MB                                    │
└──────────────────┼─────────────────────────────────────┘
                   │
                   ↓
            Main Memory (RAM)
              16GB / 32GB
```

**Key insight**: The Go scheduler tries to keep goroutines running on the same CPU core to take advantage of cache locality. When a goroutine's data is in L1 cache, it runs incredibly fast (4 CPU cycles access time). If it has to go to RAM, it's 100x slower (400 cycles).

#### CPU Registers

Each CPU core has a small set of extremely fast storage locations called registers. These are the fastest memory in the computer - accessing them takes just 1 CPU cycle.

```
x86-64 CPU Registers (simplified):

General Purpose Registers (64-bit):
┌─────────────────────────────────────┐
│ RAX  - Accumulator                  │
│ RBX  - Base                         │
│ RCX  - Counter                      │
│ RDX  - Data                         │
│ RSI  - Source Index                 │
│ RDI  - Destination Index            │
│ RBP  - Base Pointer (stack frame)   │
│ RSP  - Stack Pointer (current)      │
│ R8-R15 - Additional general purpose │
└─────────────────────────────────────┘

Special Registers:
┌─────────────────────────────────────┐
│ RIP  - Instruction Pointer          │
│      (points to next instruction)   │
│                                     │
│ RFLAGS - Status flags               │
│        (zero, carry, overflow, etc) │
└─────────────────────────────────────┘

When goroutine switches occur:
- All these registers must be saved!
- Then loaded with new goroutine's values
- This is called "context switching"
```

When a goroutine is running, its state is held in these registers. When we switch to another goroutine, we must save all register values to memory and load the new goroutine's saved register values. This is called a context switch, and it's one of the key operations in the Go scheduler.

#### The Instruction Pointer (RIP)

The instruction pointer is perhaps the most important register for understanding goroutines. It tells the CPU which instruction to execute next.

```
Program Execution:

Memory Address  │  Instruction         │  Comments
────────────────┼──────────────────────┼─────────────────
0x1000          │  MOV RAX, 5          │  Put 5 in RAX
0x1004          │  ADD RAX, 3          │  Add 3 to RAX
0x1008          │  CALL 0x2000         │  Call function
0x100C          │  MOV RBX, RAX        │  Move result
...

Instruction Pointer (RIP):
┌─────────────────────────────────────┐
│ RIP: 0x1004                         │  ← Currently executing
└─────────────────────────────────────┘

After each instruction:
1. CPU fetches instruction at RIP
2. RIP advances to next instruction
3. CPU executes the instruction
4. Repeat

When goroutine switches:
- Save current RIP value
- Load new goroutine's RIP value
- CPU now executes different code!
```

### Memory Hierarchy and Access Times

Understanding memory is crucial for understanding why goroutines are designed the way they are. Not all memory is created equal.

```
Memory Hierarchy (from fastest to slowest):

Level          Size        Access Time    Cost/GB
──────────────────────────────────────────────────
Registers      ~1KB        1 cycle        N/A
               (~1ns)

L1 Cache       32KB        4 cycles       $1000+
               (~1ns)

L2 Cache       256KB       12 cycles      $100+
               (~3ns)

L3 Cache       8MB         40 cycles      $10+
               (~10ns)

Main RAM       16-32GB     100-300 cycles $10
               (~100ns)

SSD            512GB-1TB   150,000 cycles $0.20
               (~150μs)

HDD            2TB+        10,000,000 cy  $0.02
               (~10ms)
```

**Why this matters for goroutines**: The Go scheduler keeps goroutines' stacks small (starting at 2KB) so they fit in L1/L2 cache. This makes goroutine switching incredibly fast because all the data is already in the fastest memory.

Compare this to OS threads with 1-2MB stacks - these can't fit in cache, so every context switch involves slow RAM access.

### Memory Layout of a Running Program

When a Go program runs, the operating system allocates a virtual address space for it. Let's see how this memory is organized:

```
Virtual Address Space (64-bit Linux):

High Addresses (0x7FFFFFFFFFFF)
┌─────────────────────────────────────┐
│  Kernel Space (OS code and data)    │
│  Not accessible to user programs    │
├─────────────────────────────────────┤ 0x7FFF...
│  Stack (grows downward)             │
│  ↓ ↓ ↓ ↓                            │
│                                     │
│  [Thread 1 Stack: 8MB]              │
│  [Thread 2 Stack: 8MB]              │
│  [Thread 3 Stack: 8MB]              │
│                                     │
│  ← For OS threads, each needs       │
│     1-8MB of reserved space         │
│                                     │
├─────────────────────────────────────┤
│  Memory Mapped Files                │
│  (shared libraries, etc.)           │
├─────────────────────────────────────┤
│                                     │
│  ↑ ↑ ↑ ↑                            │
│  Heap (grows upward)                │
│  Dynamic allocations (malloc/new)   │
│                                     │
│  Goroutine stacks live here! ──────┐│
│  Each starts at ~2KB               ││
│                                    ││
│  G1: [2KB stack]                   ││
│  G2: [2KB stack]                   ││
│  G3: [2KB stack]                   ││
│  ... thousands more goroutines     ││
│                                    ↓│
├─────────────────────────────────────┤
│  BSS (uninitialized global data)    │
├─────────────────────────────────────┤
│  Data (initialized global data)     │
├─────────────────────────────────────┤
│  Text (executable code)             │
└─────────────────────────────────────┘ Low Addresses (0x400000)

Key differences:
- OS thread stacks: Fixed, large, fragmented
- Goroutine stacks: Dynamic, small, heap-allocated
```

Notice how OS threads each reserve megabytes of contiguous space, while goroutines only allocate what they need on the heap. This is one reason why you can create millions of goroutines but only thousands of threads.

---

## Operating System Threads {#os-threads}

Before we can appreciate goroutines, we must understand what they replace: operating system threads. OS threads are the traditional unit of concurrent execution.

### What is an OS Thread?

An OS thread (also called a kernel thread) is a sequence of execution managed by the operating system kernel. The OS is responsible for creating threads, scheduling them on CPU cores, and handling their lifecycle.

```
OS Thread Structure:

┌─────────────────────────────────────────────┐
│  Thread Control Block (in kernel memory)    │
├─────────────────────────────────────────────┤
│  Thread ID: 12345                           │
│  Process ID: 6789                           │
│  Priority: 10                               │
│  State: RUNNING / READY / BLOCKED           │
│  CPU affinity: Core 2                       │
│  Saved registers (when not running):        │
│    RAX: 0x1234                              │
│    RBX: 0x5678                              │
│    RIP: 0x400100                            │
│    RSP: 0x7FFE1000                          │
│    ... all other registers                  │
│  Signal mask                                │
│  Scheduling information                     │
│  Statistics (CPU time used, etc.)           │
├─────────────────────────────────────────────┤
│  Stack pointer → Stack memory               │
│                  (1-8MB allocated)          │
└─────────────────────────────────────────────┘
```

Each thread has significant overhead:

- **Thread Control Block**: 1-2 KB in kernel memory
- **Stack**: 1-8 MB of reserved address space
- **Kernel data structures**: Additional memory for scheduling
- **TLB entries**: Translation Lookaside Buffer entries
- **Context switch overhead**: Thousands of CPU cycles

### How OS Threads are Scheduled

The operating system kernel contains a thread scheduler that decides which threads run on which CPU cores. This is called preemptive scheduling because the OS can forcibly stop a running thread at any time.

```
OS Thread Scheduler:

┌────────────────────────────────────────────┐
│  CPU Core 0        CPU Core 1              │
│  ┌───────────┐    ┌───────────┐            │
│  │ Running:  │    │ Running:  │            │
│  │ Thread 5  │    │ Thread 12 │            │
│  └───────────┘    └───────────┘            │
└────────────────────────────────────────────┘
           ↑                  ↑
           │   Scheduled by   │
           │                  │
┌──────────┴──────────────────┴───────────────┐
│        OS Kernel Thread Scheduler           │
│                                             │
│  Ready Queue (runnable threads):            │
│  ┌────────────────────────────────────────┐ │
│  │ Thread 1, Thread 3, Thread 8, ...      │ │
│  └────────────────────────────────────────┘ │
│                                             │
│  Blocked Queue (waiting for I/O):           │
│  ┌────────────────────────────────────────┐ │
│  │ Thread 2, Thread 7, Thread 15, ...     │ │
│  └────────────────────────────────────────┘ │
│                                             │
│  Scheduling Algorithm: CFS (Linux)          │
│  - Priority-based                           │
│  - Time slices (~10-100ms)                  │
│  - Preemptive (can interrupt any thread)    │
└─────────────────────────────────────────────┘
```

**The Problem**: Every 10-100 milliseconds, a hardware timer interrupt fires. The CPU stops what it's doing, saves all registers, and jumps into kernel mode to run the scheduler. The scheduler picks the next thread to run, restores its registers, and jumps back to user mode. This happens thousands of times per second on a busy system.

### The Cost of Context Switching

A context switch between OS threads is expensive. Let's see exactly what happens:

```
OS Thread Context Switch (step by step):

Time T: Thread A running on Core 0
┌────────────────────────────────────┐
│ Core 0 Registers:                  │
│ RAX = 0x1234                       │
│ RBX = 0x5678                       │
│ RIP = 0x400500  ← Executing code   │
│ RSP = 0x7FFE1000 ← Stack           │
└────────────────────────────────────┘

Time T+1: Timer interrupt fires
┌────────────────────────────────────┐
│ Hardware saves RIP to stack        │
│ Jump to interrupt handler          │
│ CPU enters kernel mode             │
└────────────────────────────────────┘

Time T+2: Kernel saves context
┌────────────────────────────────────┐
│ Save ALL registers to TCB          │
│ Thread A TCB:                      │
│   RAX = 0x1234                     │
│   RBX = 0x5678                     │
│   RIP = 0x400500                   │
│   RSP = 0x7FFE1000                 │
│   ... 16+ more registers           │
└────────────────────────────────────┘

Time T+3: Scheduler runs
┌────────────────────────────────────┐
│ Look at ready queue                │
│ Pick Thread B (scheduling policy)  │
│ Update accounting                  │
│ Change page tables (TLB flush!)    │
└────────────────────────────────────┘

Time T+4: Restore Thread B context
┌────────────────────────────────────┐
│ Load Thread B's saved registers:   │
│ Core 0 Registers:                  │
│ RAX = 0xABCD  ← Thread B's value  │
│ RBX = 0xEF01                       │
│ RIP = 0x401200                     │
│ RSP = 0x7FFE5000                   │
└────────────────────────────────────┘

Time T+5: Return from interrupt
┌────────────────────────────────────┐
│ Return to user mode                │
│ Thread B now running               │
│ TLB misses for several cycles      │
│ Cache misses for Thread B's data   │
└────────────────────────────────────┘

Total cost: 1000-3000 CPU cycles
           (~1-2 microseconds)
```

**Why it's expensive**:

1. **Register saving/restoring**: 16-20 registers per context switch
2. **Mode switching**: User mode ↔ Kernel mode requires privilege change
3. **TLB flush**: Virtual to physical address mappings are invalidated
4. **Cache pollution**: New thread's data evicts old thread's cached data
5. **Scheduler overhead**: Kernel code execution time

### Why Many Threads Don't Scale

As you create more OS threads, performance actually gets worse, not better. This is called thread thrashing.

```
Thread Scalability Problem:

With 10 threads (good):
┌────────────────────────────────────┐
│ Threads: 10                        │
│ Context switches: ~1,000/sec       │
│ CPU time spent: 1% scheduling      │
│ Useful work: 99%                   │
└────────────────────────────────────┘

With 1,000 threads (bad):
┌────────────────────────────────────┐
│ Threads: 1,000                     │
│ Context switches: ~100,000/sec     │
│ CPU time spent: 50% scheduling     │
│ Useful work: 50%                   │
└────────────────────────────────────┘

With 10,000 threads (terrible):
┌────────────────────────────────────┐
│ Threads: 10,000                    │
│ Context switches: ~1,000,000/sec   │
│ CPU time spent: 90% scheduling     │
│ Useful work: 10%                   │
│ Memory: 10,000 * 8MB = 80GB!      │
└────────────────────────────────────┘
```

This is the fundamental problem that goroutines solve. Instead of mapping each concurrent task to an OS thread, Go implements its own user-space scheduler that multiplexes thousands or millions of goroutines onto a small number of OS threads.

## The Problem Goroutines Solve {#problem}

Let's understand the specific problems that motivated Go's creators to design goroutines instead of using OS threads directly.

### Problem 1: The C10K Problem

In the early 2000s, web servers faced a challenge: how to handle 10,000 concurrent connections (C10K = Concurrent 10,000). With one OS thread per connection, this was impossible due to memory and context switching overhead.

```
Traditional Thread-Per-Connection Model:

Web Server with 10,000 connections:
┌─────────────────────────────────────────────┐
│ Connection 1 → Thread 1 (8MB stack)        │
│ Connection 2 → Thread 2 (8MB stack)        │
│ Connection 3 → Thread 3 (8MB stack)        │
│ ...                                        │
│ Connection 10,000 → Thread 10,000          │
└─────────────────────────────────────────────┘

Total memory: 10,000 × 8MB = 80GB just for stacks!
Context switches: Millions per second
CPU time: 90% spent in scheduler
Result: System grinds to a halt
```

### Problem 2: Blocking Operations

Most threads spend most of their time waiting - for network I/O, disk I/O, database queries, or other threads. A thread waiting for I/O is wasting an entire OS thread's resources.

```
Thread Utilization in a Web Server:

Thread lifecycle for a typical request:
┌────────────────────────────────────────────┐
│ ▓ = Active CPU work                       │
│ ░ = Blocked waiting for I/O               │
└────────────────────────────────────────────┘

Request handling timeline:
▓░░░░░░░░░▓░░░░░░░░░░░░▓░░░░░░▓
│         │             │       │
│         │             │       └─ Send response
│         │             └───────── Process data
│         └─────────────────────── Database query (waiting)
└───────────────────────────────── Parse request

CPU utilization: 5%
Waiting time: 95%

With 1 thread per request:
- Thread blocks during I/O
- CPU core sits idle
- Thread occupies 8MB of memory
- Huge waste of resources!
```

### Problem 3: Thread Creation Cost

Creating and destroying OS threads is expensive. For short-lived tasks, the overhead of thread management exceeds the actual work.

```
Cost Analysis:

Short task (10 microseconds of work):
┌────────────────────────────────────────────┐
│ Thread creation: 20 microseconds           │
│ Actual work:     10 microseconds           │
│ Thread cleanup:  15 microseconds           │
│ Total:           45 microseconds           │
└────────────────────────────────────────────┘

Overhead: 350% (35 microseconds overhead for 10 microseconds work)

With thread pools:
- Pre-create threads to avoid creation cost
- But pool size must be tuned
- Too many = memory waste
- Too few = tasks queue up
- Still have context switch overhead
```

### The Goroutine Solution

Goroutines solve all these problems by implementing a user-space scheduler that multiplexes many goroutines onto a small number of OS threads.

```
Goroutine Model:

10,000 concurrent tasks:
┌────────────────────────────────────────────┐
│ 10,000 goroutines (2KB each = 20MB total)  │
│            ↓ ↓ ↓                           │
│      Go Scheduler (in user space)          │
│            ↓ ↓ ↓                           │
│    4-8 OS threads (one per CPU core)       │
│            ↓ ↓ ↓                           │
│         CPU Cores                          │
└────────────────────────────────────────────┘

Benefits:
- Tiny memory footprint (2KB vs 8MB)
- Fast creation (~200 nanoseconds vs 20 microseconds)
- Cooperative scheduling (no forced preemption)
- Efficient handling of blocking operations
- Work stealing for load balancing
```

---

## The Go Scheduler (GMP Model) {#gmp-model}

The Go scheduler is the heart of goroutine magic. It implements an M:N scheduler, meaning M goroutines are multiplexed onto N OS threads. The scheduler is built on three key entities: G (Goroutine), M (Machine/OS thread), and P (Processor/Context).

### The Three Components: G, M, P

Let's understand each component in detail.

#### G - Goroutine

The G structure represents a single goroutine. It contains all the state needed to execute a goroutine.

```
G (Goroutine) Structure:

┌─────────────────────────────────────────────┐
│ type g struct {                             │
│   stack       stack     // Stack bounds     │
│   stackguard0 uintptr   // Stack overflow   │
│   stackguard1 uintptr   // Stack overflow   │
│                                             │
│   m           *m        // Current M        │
│   sched       gobuf     // Saved context    │
│                                             │
│   atomicstatus uint32   // G state          │
│   goid         int64    // Goroutine ID     │
│                                             │
│   waitsince   int64     // Blocking start   │
│   waitreason  waitReason                    │
│                                             │
│   preempt     bool      // Should yield     │
│   preemptStop bool                          │
│                                             │
│   ... many more fields (~300 bytes)         │
│ }                                           │
└─────────────────────────────────────────────┘

gobuf (saved context):
┌─────────────────────────────────────────────┐
│ type gobuf struct {                         │
│   sp   uintptr  // Stack pointer            │
│   pc   uintptr  // Program counter          │
│   g    guintptr // Goroutine pointer        │
│   ctxt unsafe.Pointer                       │
│   ret  sys.Uintreg                          │
│   lr   uintptr  // Link register            │
│   bp   uintptr  // Base pointer             │
│ }                                           │
└─────────────────────────────────────────────┘

Memory layout of a goroutine:
┌─────────────────────────────────────────────┐
│ G structure:     ~300 bytes                 │
│ Stack:          2KB-1GB (grows dynamically) │
│ Total initial:  ~2.3KB                      │
└─────────────────────────────────────────────┘
```

**Key fields**:

- `stack`: Pointer to the goroutine's stack memory
- `sched.sp`: Saved stack pointer when not running
- `sched.pc`: Saved program counter (next instruction)
- `atomicstatus`: Current state (running, runnable, waiting, etc.)
- `m`: Which M (OS thread) is running this G
- `preempt`: Flag for scheduler to take back control

#### M - Machine (OS Thread)

The M structure represents an OS thread. It's called "M" for "Machine" because historically it represented a physical machine.

```
M (Machine/OS Thread) Structure:

┌─────────────────────────────────────────────┐
│ type m struct {                             │
│   g0          *g        // Scheduling stack │
│   curg        *g        // Current G        │
│   p           puintptr  // Attached P       │
│   nextp       puintptr  // Next P to run    │
│   oldp        puintptr  // Previous P       │
│                                             │
│   id          int64     // M id             │
│   spinning    bool      // Looking for work │
│   blocked     bool      // In syscall       │
│                                             │
│   park        note      // Thread parking   │
│   alllink     *m        // All M list       │
│                                             │
│   ... more fields (~1KB)                    │
│ }                                           │
└─────────────────────────────────────────────┘

Special goroutines on M:
┌─────────────────────────────────────────────┐
│ g0: Scheduling goroutine                    │
│     - Larger stack (8KB on 64-bit)          │
│     - Used for scheduler code execution     │
│     - Each M has one g0                     │
│                                             │
│ gsignal: Signal handling goroutine          │
│     - Handles OS signals                    │
│     - Stack size: 32KB                      │
└─────────────────────────────────────────────┘

M lifecycle:
1. Created by runtime when needed
2. Picks up a P (processor)
3. Runs goroutines from P's run queue
4. If blocks, hands off P to another M
5. Cached when idle (up to 10,000 Ms)
```

**Key fields**:

- `curg`: The goroutine currently running on this M
- `g0`: Special goroutine for executing scheduler code
- `p`: The P this M is attached to
- `spinning`: True if this M is looking for work to steal
- `blocked`: True if this M is blocked in a system call

#### P - Processor (Context)

The P structure represents the resources required to execute Go code. There are typically `GOMAXPROCS` Ps (default = number of CPU cores).

```
P (Processor) Structure:

┌─────────────────────────────────────────────┐
│ type p struct {                             │
│   id          int32     // P id             │
│   status      uint32    // P state          │
│   m           muintptr  // Attached M       │
│   mcache      *mcache   // Memory allocator │
│                                             │
│   // Local run queue                        │
│   runqhead    uint32                        │
│   runqtail    uint32                        │
│   runq        [256]guintptr                 │
│   runnext     guintptr  // Next G to run    │
│                                             │
│   // Free G cache                           │
│   gFree       struct {                      │
│     gList                                   │
│     n int32                                 │
│   }                                         │
│                                             │
│   ... more fields (~500 bytes)              │
│ }                                           │
└─────────────────────────────────────────────┘

P's Local Run Queue:
┌─────────────────────────────────────────────┐
│ runq (circular buffer, 256 capacity):       │
│                                             │
│ ┌────┬────┬────┬────┬────┬────┬─────────┐   │
│ │ G1 │ G2 │ G3 │ G4 │ G5 │... │ Empty   │   │
│ └────┴────┴────┴────┴────┴────┴─────────┘   │
│  ↑                         ↑                │
│  head                      tail             │
│                                             │
│ runnext: G99 (runs before queue)            │
└─────────────────────────────────────────────┘

P states:
- _Pidle:    Not being used
- _Prunning: Attached to an M, running Go code
- _Psyscall: In a system call
- _Pgcstop:  Stopped for garbage collection
```

**Key fields**:

- `runq`: Local run queue of goroutines (256 capacity)
- `runnext`: Next goroutine to run (hint for scheduler)
- `mcache`: Per-P memory cache for allocations
- `m`: The M this P is attached to

### The GMP Model in Action

Now let's see how G, M, and P work together:

```
Complete GMP Architecture:

Global Level:
┌─────────────────────────────────────────────────────┐
│                 Go Runtime                          │
│                                                     │
│ Global Run Queue (for overflow):                    │
│ ┌──────────────────────────────────────────────────┐│
│ │ G500, G501, G502, ... (rarely used)              ││
│ └──────────────────────────────────────────────────┘│
│                                                     │
│ All Ps (GOMAXPROCS = 4):                            │
│ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ │
│ │    P0    │ │    P1    │ │    P2    │ │    P3    │ │
│ │ ┌──────┐ │ │ ┌──────┐ │ │ ┌──────┐ │ │ ┌──────┐ │ │
│ │ │ runq │ │ │ │ runq │ │ │ │ runq │ │ │ │ runq │ │ │
│ │ │ G1   │ │ │ │ G5   │ │ │ │ G9   │ │ │ │ G13  │ │ │
│ │ │ G2   │ │ │ │ G6   │ │ │ │ G10  │ │ │ │ G14  │ │ │
│ │ │ G3   │ │ │ │ G7   │ │ │ │ G11  │ │ │ │ G15  │ │ │
│ │ └──────┘ │ │ └──────┘ │ │ └──────┘ │ │ └──────┘ │ │
│ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────┬─────┘ │
│      │            │            │            │       │
└──────┼────────────┼────────────┼────────────┼───────┘
       │            │            │            │
       ↓            ↓            ↓            ↓
┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐
│    M0    │ │    M1    │ │    M2    │ │    M3    │
│ (OS Thrd)│ │ (OS Thrd)│ │ (OS Thrd)│ │ (OS Thrd)│
│          │ │          │ │          │ │          │
│ Running: │ │ Running: │ │ Running: │ │ Running: │
│   G1     │ │   G5     │ │   G9     │ │   G13    │
└────┬─────┘ └────┬─────┘ └────┬─────┘ └────┬─────┘
     │            │            │            │
     ↓            ↓            ↓            ↓
┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐
│ CPU      │ │ CPU      │ │ CPU      │ │ CPU      │
│ Core 0   │ │ Core 1   │ │ Core 2   │ │ Core 3   │
└──────────┘ └──────────┘ └──────────┘ └──────────┘
```

**Key relationships**:

- P count = GOMAXPROCS (typically CPU count)
- M count ≥ P count (created as needed)
- G count = unlimited (millions possible)
- Each M must have a P to run Go code
- Each P has a local run queue of Gs
- Gs can be stolen from other Ps (work stealing)

### Scheduler Invariants

The Go scheduler maintains several key invariants:

```
Invariant 1: M must have P to run Go code
┌─────────────────────────────────────────────┐
│ M without P:                                │
│ - Can't run any goroutines                  │
│ - Can only execute system calls             │
│ - Waits for a P to become available         │
└─────────────────────────────────────────────┘

Invariant 2: P must be attached to M to be active
┌─────────────────────────────────────────────┐
│ P without M:                                │
│ - Is idle                                   │
│ - Goroutines in its queue wait              │
│ - Can be picked up by any M                 │
└─────────────────────────────────────────────┘

Invariant 3: G must be on a P to run
┌─────────────────────────────────────────────┐
│ G not on any P:                             │
│ - In global queue (overflow)                │
│ - Blocked (waiting for I/O)                 │
│ - Dead (finished execution)                 │
└─────────────────────────────────────────────┘
```

---

## Goroutine Creation {#goroutine-creation}

Let's trace exactly what happens when you write `go func()` in your code. We'll follow the journey from source code to running goroutine.

### Step 1: Compile Time

When the Go compiler sees the `go` keyword, it generates special code:

```go
// Your code:
go myFunction(arg1, arg2)

// Compiler generates (pseudo-code):
newproc(siz, fn, arg1, arg2)
```

The `newproc` function is a runtime function that creates a new goroutine. The compiler:

1. Calculates the size of arguments
2. Packages the function pointer and arguments
3. Inserts a call to runtime.newproc

### Step 2: Runtime Allocation

When `newproc` runs, it allocates memory for the new goroutine:

```
Goroutine Allocation Process:

Step 1: Try to get G from P's free list
┌─────────────────────────────────────────────┐
│ P's gFree list (cached dead goroutines):    │
│ ┌─────────────────────────────────────────┐ │
│ │ [G old1] [G old2] [G old3] ...          │ │
│ └─────────────────────────────────────────┘ │
│                                             │
│ If available: Reuse! (very fast)            │
└─────────────────────────────────────────────┘

Step 2: If no free G, allocate new one
┌─────────────────────────────────────────────┐
│ malg(stacksize):                            │
│   1. Allocate G struct (~300 bytes)         │
│   2. Allocate stack (2KB initially)         │
│   3. Set up stack guards                    │
│   4. Initialize G fields                    │
│                                             │
│ Memory layout:                              │
│ ┌───────────────────────────────────┐       │
│ │ G struct (300 bytes)              │       │
│ ├───────────────────────────────────┤       │
│ │ Stack (2048 bytes)                │       │
│ │ ┌─────────────────────────────┐   │       │
│ │ │ (grows toward lower addrs)  │   │       │
│ │ │                             │   │       │
│ │ │         [empty]             │   │       │
│ │ │                             │   │       │
│ │ └─────────────────────────────┘   │       │
│ └───────────────────────────────────┘       │
│ Total: ~2.3KB                               │
└─────────────────────────────────────────────┘
```

### Step 3: Initialize Goroutine State

The runtime initializes the new G structure:

```
G Initialization:

g := newG()

// Set up stack bounds
g.stack.lo = stackAlloc.lo
g.stack.hi = stackAlloc.hi
g.stackguard0 = g.stack.lo + StackGuard

// Set up saved context (where to start)
g.sched.sp = g.stack.hi
g.sched.pc = addressOf(goexit) + PCQuantum
g.sched.g = g

// Set goroutine ID
g.goid = atomic.Add64(&sched.goidgen, 1)

// Copy function arguments onto stack
memmove(g.stack.hi - argsSize, args, argsSize)
g.sched.sp -= argsSize

// Set function to run
g.startpc = addressOf(fn)

// Set state to runnable
g.atomicstatus = _Grunnable

Memory state after initialization:
┌─────────────────────────────────────────────┐
│ G struct:                                   │
│   goid: 123                                 │
│   atomicstatus: _Grunnable                  │
│   startpc: 0x401234 (function address)      │
│   sched.sp: 0x7FFE1FF8 (stack top - args)   │
│   sched.pc: addressOf(goexit)               │
│                                             │
│ Stack:                                      │
│   High address (0x7FFE2000)                 │
│   ┌─────────────────────────────────────┐   │
│   │ Function arguments (copied here)    │   │
│   ├─────────────────────────────────────┤   │
│   │                                     │   │
│   │         [empty]                     │   │
│   │                                     │   │
│   └─────────────────────────────────────┘   │
│   Low address (0x7FFE1000)                  │
└─────────────────────────────────────────────┘
```

### Step 4: Add to Run Queue

The new goroutine is added to a run queue:

```
Run Queue Insertion:

Decision tree:
┌─────────────────────────────────────────────┐
│ Where to put new goroutine?                 │
└──────────────┬──────────────────────────────┘
               │
       ┌───────┴───────┐
       │               │
   P's runq     runnext available?
   not full?           │
       │           ┌───┴───┐
       │          Yes      No
       │           │       │
       │           │       ↓
       │           │   Add to P's runq
       │           │
       │           ↓
       │   Put in runnext (runs next)
       │   Move old runnext to runq
       │
       ↓
   Add to P's local runq
   (circular buffer)
       │
       ↓
   P's runq full?
       │
   ┌───┴───┐
  Yes      No
   │       │
   │       ↓
   │   Success
   │
   ↓
Put in global runq

Algorithm (simplified):
┌─────────────────────────────────────────────┐
│ func runqput(p *p, gp *g, next bool) {      │
│   if next {                                 │
│     oldnext := p.runnext                    │
│     if !p.runnext.cas(oldnext, gp) {        │
│       goto retry                            │
│     }                                       │
│     if oldnext == 0 {                       │
│       return                                │
│     }                                       │
│     gp = oldnext                            │
│   }                                         │
│                                             │
│   if runqputslow(p, gp) {                   │
│     return                                  │
│   }                                         │
│                                             │
│   // Put in local queue                     │
│   ...                                       │
│ }                                           │
└─────────────────────────────────────────────┘
```

**Typical flow for `go func()`**:

1. New G created with _Grunnable state
2. Placed in P's runnext (hint to scheduler)
3. Old runnext (if any) moved to local run queue
4. If local queue full, batch moved to global queue
5. Scheduler will pick it up soon

### Step 5: Potential Immediate Execution

In some cases, the new goroutine may start immediately:

```
Immediate Execution Conditions:

Condition 1: Current goroutine yields
┌─────────────────────────────────────────────┐
│ go someFunc()     ← Creates new G           │
│ runtime.Gosched() ← Yields voluntarily      │
│                                             │
│ → Scheduler switches to new G immediately   │
└─────────────────────────────────────────────┘

Condition 2: Function call preemption point
┌─────────────────────────────────────────────┐
│ go someFunc()     ← Creates new G           │
│ functionCall()    ← Function call checks    │
│                     for preemption          │
│                                             │
│ → May switch to new G at call              │
└─────────────────────────────────────────────┘

Condition 3: Idle P available
┌─────────────────────────────────────────────┐
│ P1: Running current G                       │
│ P2: Idle (no work)                          │
│                                             │
│ go someFunc() → New G created               │
│              → P2 wakes up                  │
│              → P2 steals new G              │
│              → Runs immediately on P2       │
└─────────────────────────────────────────────┘
```

### Complete Creation Timeline

Let's see the complete timeline with CPU cycles:

```
Goroutine Creation Timeline:

T=0: go myFunc(x, y)
┌─────────────────────────────────────────────┐
│ Compiler-inserted call to runtime.newproc   │
│ Cost: ~5 CPU cycles                         │
└─────────────────────────────────────────────┘

T=1: Try to get G from free list
┌─────────────────────────────────────────────┐
│ Check P's gFree list                        │
│ Cost: ~10 CPU cycles                        │
└─────────────────────────────────────────────┘

T=2a: If free G found (common case)
┌─────────────────────────────────────────────┐
│ Reuse existing G structure                  │
│ Reset G fields                              │
│ Cost: ~50 CPU cycles                        │
└─────────────────────────────────────────────┘

T=2b: If no free G (rare case)
┌─────────────────────────────────────────────┐
│ Allocate new G struct                       │
│ Allocate 2KB stack                          │
│ Set up stack guards                         │
│ Cost: ~500 CPU cycles                       │
└─────────────────────────────────────────────┘

T=3: Initialize G
┌─────────────────────────────────────────────┐
│ Set goid, startpc, sched.sp, sched.pc       │
│ Copy function arguments to stack            │
│ Set atomicstatus = _Grunnable               │
│ Cost: ~30 CPU cycles                        │
└─────────────────────────────────────────────┘

T=4: Add to run queue
┌─────────────────────────────────────────────┐
│ Try runnext (preferred)                     │
│ Or add to local runq                        │
│ Cost: ~20 CPU cycles                        │
└─────────────────────────────────────────────┘

Total time (common case): ~115 CPU cycles
           ≈ 40 nanoseconds on 3GHz CPU

Compare to OS thread creation: ~20,000 nanoseconds
Goroutines are 500x faster!
```

## Goroutine Execution {#goroutine-execution}

Now that we understand how goroutines are created, let's see how they actually execute. This involves understanding the scheduler's main loop and how goroutines get CPU time.

### The Scheduler Main Loop

Each M (OS thread) runs a scheduling loop when it has a P. This loop continuously finds and executes goroutines.

```
M's Scheduling Loop (simplified):

func schedule() {
    _g_ := getg()  // Get current g (scheduler's g0)
    
top:
    // Check if we should garbage collect
    if sched.gcwaiting != 0 {
        gcstopm()
        goto top
    }
    
    var gp *g
    var inheritTime bool
    
    // Try to get G from various sources
    // (priority order)
    
    // 1. Check if we should run GC worker
    if gp == nil {
        gp, inheritTime = runqget(_g_.m.p.ptr())
    }
    
    // 2. Check global run queue (every 61 ticks)
    if gp == nil {
        if _g_.m.p.ptr().schedtick%61 == 0 {
            lock(&sched.lock)
            gp = globrunqget(_g_.m.p.ptr(), 1)
            unlock(&sched.lock)
        }
    }
    
    // 3. Try local run queue
    if gp == nil {
        gp, inheritTime = runqget(_g_.m.p.ptr())
    }
    
    // 4. Try to steal from other Ps
    if gp == nil {
        gp, inheritTime = findrunnable()  // Blocks until work found
    }
    
    // Execute the goroutine
    execute(gp, inheritTime)
}
```

Let's break down each step:

### Step 1: Finding Work

The scheduler looks for goroutines to run in a specific order:

```
Work Finding Priority:

Priority 1: Local Run Queue (P's runq)
┌─────────────────────────────────────────────┐
│ P's local queue (most common case):         │
│ ┌─────────────────────────────────────────┐ │
│ │ [G1] [G2] [G3] [G4] [G5] ...            │ │
│ └─────────────────────────────────────────┘ │
│                                             │
│ Check runnext first (hot path):             │
│ runnext: G99                                │
│                                             │
│ Cost: ~10 CPU cycles (no lock needed)       │
└─────────────────────────────────────────────┘

Priority 2: Global Run Queue (every 61 ticks)
┌─────────────────────────────────────────────┐
│ Global queue (overflow from local queues):  │
│ ┌─────────────────────────────────────────┐ │
│ │ [G100] [G101] [G102] ...                │ │
│ └─────────────────────────────────────────┘ │
│                                             │
│ Checked periodically to prevent starvation  │
│ Takes up to 1 G per P                       │
│                                             │
│ Cost: ~50 CPU cycles (requires lock)        │
└─────────────────────────────────────────────┘

Priority 3: Network Poller
┌─────────────────────────────────────────────┐
│ Check for Gs waiting on network I/O:        │
│ ┌─────────────────────────────────────────┐ │
│ │ G50 (waiting on socket read)            │ │
│ │ G51 (waiting on socket write)           │ │
│ └─────────────────────────────────────────┘ │
│                                             │
│ Uses epoll/kqueue for efficiency            │
│ Cost: ~100 CPU cycles                       │
└─────────────────────────────────────────────┘

Priority 4: Work Stealing
┌─────────────────────────────────────────────┐
│ Steal from other Ps' run queues:            │
│                                             │
│ P0 (idle):        P1 (busy):                │
│ runq: []          runq: [G1 G2 G3 G4 G5]    │
│    ↑                        ↓               │
│    └────── steals half ─────┘               │
│ runq: [G3 G4 G5]                            │
│                                             │
│ Randomized stealing order                   │
│ Cost: ~500 CPU cycles                       │
└─────────────────────────────────────────────┘
```

### Step 2: Executing a Goroutine

Once a goroutine is found, `execute()` prepares to run it:

```
execute() Function Flow:

func execute(gp *g, inheritTime bool) {
    _g_ := getg()  // Get current g (scheduler's g0)
    
    // 1. Bind G to M
    _g_.m.curg = gp
    gp.m = _g_.m
    
    // 2. Update state
    casgstatus(gp, _Grunnable, _Grunning)
    
    // 3. Record execution start time (for preemption)
    gp.waitsince = 0
    gp.preempt = false
    gp.stackguard0 = gp.stack.lo + _StackGuard
    
    // 4. Update statistics
    if !inheritTime {
        _g_.m.p.ptr().schedtick++
    }
    
    // 5. Switch to goroutine's stack and PC
    gogo(&gp.sched)  // ← Assembly code!
}

State transition:
┌─────────────────────────────────────────────┐
│ Before:                                     │
│ M is running: g0 (scheduler stack)          │
│ G state: _Grunnable                         │
│                                             │
│ After execute():                            │
│ M is running: G (user goroutine)            │
│ G state: _Grunning                          │
└─────────────────────────────────────────────┘
```

### Step 3: The gogo Assembly Magic

The `gogo` function is pure assembly code that performs the actual context switch:

```
gogo() Implementation (x86-64):

TEXT runtime·gogo(SB), NOSPLIT, $0-8
    MOVQ    8(SP), BX      // Load gobuf pointer into BX
    
    // Restore registers from gobuf
    MOVQ    gobuf_g(BX), DX
    MOVQ    0(DX), CX      // make sure g != nil
    get_tls(CX)
    MOVQ    DX, g(CX)
    
    // Load stack pointer
    MOVQ    gobuf_sp(BX), SP
    
    // Load other saved registers
    MOVQ    gobuf_ret(BX), AX
    MOVQ    gobuf_ctxt(BX), DX
    MOVQ    gobuf_bp(BX), BP
    
    // Clear gobuf
    MOVQ    $0, gobuf_sp(BX)
    MOVQ    $0, gobuf_ret(BX)
    MOVQ    $0, gobuf_ctxt(BX)
    MOVQ    $0, gobuf_bp(BX)
    
    // Jump to saved PC (program counter)
    MOVQ    gobuf_pc(BX), BX
    JMP     BX

What this does step-by-step:

T=0: Before gogo()
┌─────────────────────────────────────────────┐
│ CPU Registers (executing scheduler):        │
│ SP = 0x7FFEF000 (scheduler stack)          │
│ PC = 0x450000   (scheduler code)           │
│ BP = 0x7FFEF100                            │
└─────────────────────────────────────────────┘

T=1: Load saved goroutine state
┌─────────────────────────────────────────────┐
│ Load from gp.sched:                         │
│ BX = gp.sched.pc  (0x401234)              │
│ SP = gp.sched.sp  (0x7FFE1FF8)            │
│ BP = gp.sched.bp  (0x7FFE1000)            │
└─────────────────────────────────────────────┘

T=2: After gogo()
┌─────────────────────────────────────────────┐
│ CPU Registers (executing goroutine):        │
│ SP = 0x7FFE1FF8 (goroutine stack!)         │
│ PC = 0x401234   (goroutine code!)          │
│ BP = 0x7FFE1000                            │
│                                             │
│ CPU now executing goroutine's code!         │
└─────────────────────────────────────────────┘

The switch takes ~50 CPU cycles (~20 nanoseconds)
Compare to OS thread switch: ~1-2 microseconds
100x faster!
```

### Step 4: Goroutine Runs

Now the goroutine executes normally on the CPU. The CPU fetches and executes instructions from the goroutine's code:

```
Goroutine Execution:

func myGoroutine(x, y int) {
    result := x + y
    fmt.Println(result)
}

Compiled to assembly:
┌─────────────────────────────────────────────┐
│ 0x401234: MOV RAX, [SP+8]   // Load x      │
│ 0x401238: MOV RBX, [SP+16]  // Load y      │
│ 0x40123C: ADD RAX, RBX      // x + y       │
│ 0x401240: MOV [SP+24], RAX  // Store result│
│ 0x401244: CALL fmt.Println  // Print       │
│ 0x401248: RET                // Return      │
└─────────────────────────────────────────────┘

During execution, CPU registers hold:
┌─────────────────────────────────────────────┐
│ RAX: 15     (result)                        │
│ RBX: 10     (y value)                       │
│ RSP: 0x7FFE1FF8 (stack pointer)            │
│ RIP: 0x401244 (current instruction)         │
└─────────────────────────────────────────────┘

Goroutine runs until:
1. Function returns
2. Calls another function (may preempt)
3. Blocks on channel/I/O
4. Calls runtime.Gosched()
5. Runtime preempts it (every 10ms)
```

---

## Goroutine Lifecycle {#goroutine-lifecycle}

A goroutine transitions through several states during its lifetime. Understanding these states is key to understanding the scheduler.

### Goroutine States

```
Goroutine State Machine:

States (atomicstatus values):
┌─────────────────────────────────────────────┐
│ _Gidle       (0): Just allocated            │
│ _Grunnable   (1): Ready to run              │
│ _Grunning    (2): Executing                 │
│ _Gsyscall    (3): In system call            │
│ _Gwaiting    (4): Blocked                   │
│ _Gdead       (6): Finished/exited           │
│ _Gcopystack  (8): Stack being copied        │
│ _Gpreempted  (9): Preempted by runtime      │
└─────────────────────────────────────────────┘

State transitions:
                    ┌──────────────┐
                    │   _Gidle     │
                    │ (allocated)  │
                    └──────┬───────┘
                           │ Initialize
                           ↓
                    ┌──────────────┐
              ┌────>│ _Grunnable   │<────┐
              │     │ (in runq)    │     │
              │     └──────┬───────┘     │
              │            │ Schedule    │
              │            ↓             │
              │     ┌──────────────┐     │
              │     │  _Grunning   │     │
              │     │ (executing)  │     │
              │     └──────┬───────┘     │
              │            │             │
      Unblock │     ┌──────┴──────┐      │ Yield/
              │     │             │      │ Preempt
              │     ↓             ↓      │
        ┌──────────────┐   ┌──────────────┐
        │  _Gwaiting   │   │ _Gsyscall    │
        │  (blocked)   │   │ (in syscall) │
        └──────────────┘   └──────────────┘
              │
              │ Complete
              ↓
        ┌──────────────┐
        │   _Gdead     │
        │  (finished)  │
        └──────────────┘
```

### Complete Lifecycle Example

Let's trace a goroutine through its entire life:

```
Goroutine Lifecycle: HTTP Request Handler

T=0: go handleRequest(req)
┌─────────────────────────────────────────────┐
│ State: _Gidle → _Grunnable                  │
│ Action: G allocated, added to run queue     │
│ Location: P's runnext                        │
└─────────────────────────────────────────────┘

T=10ms: Scheduler picks G
┌─────────────────────────────────────────────┐
│ State: _Grunnable → _Grunning               │
│ Action: M executes G via gogo()             │
│ Location: Running on M0, Core 2             │
└─────────────────────────────────────────────┘

T=11ms: G makes database query (blocks)
┌─────────────────────────────────────────────┐
│ State: _Grunning → _Gwaiting                │
│ Action: G blocked on channel recv            │
│ Location: Waiting on channel queue           │
│ M0 continues with another G                  │
└─────────────────────────────────────────────┘

T=30ms: Database response arrives
┌─────────────────────────────────────────────┐
│ State: _Gwaiting → _Grunnable               │
│ Action: Channel send unblocks G             │
│ Location: Added back to P1's run queue      │
└─────────────────────────────────────────────┘

T=31ms: Scheduler picks G again
┌─────────────────────────────────────────────┐
│ State: _Grunnable → _Grunning               │
│ Action: M1 executes G                        │
│ Location: Running on M1, Core 1             │
└─────────────────────────────────────────────┘

T=32ms: G makes system call (write socket)
┌─────────────────────────────────────────────┐
│ State: _Grunning → _Gsyscall                │
│ Action: Enters syscall, P handed off         │
│ Location: Blocked in kernel                  │
│ P reassigned to M2                           │
└─────────────────────────────────────────────┘

T=33ms: System call completes
┌─────────────────────────────────────────────┐
│ State: _Gsyscall → _Grunnable               │
│ Action: Tries to reacquire P                 │
│ Location: P unavailable, goes to global q    │
└─────────────────────────────────────────────┘

T=34ms: Scheduled again
┌─────────────────────────────────────────────┐
│ State: _Grunnable → _Grunning               │
│ Action: P0 steals from global queue         │
│ Location: Running on M0, Core 0             │
└─────────────────────────────────────────────┘

T=35ms: Function returns
┌─────────────────────────────────────────────┐
│ State: _Grunning → _Gdead                   │
│ Action: Calls goexit, cleanup               │
│ Location: Added to P's gFree list           │
└─────────────────────────────────────────────┘

Total lifetime: 35ms
Active execution time: ~6ms
Blocked time: ~19ms (waiting)
Scheduling overhead: ~0.1ms
```

---

## Stack Management {#stack-management}

One of the most innovative aspects of goroutines is their dynamic stack growth. Let's understand how this works.

### Initial Stack Allocation

When a goroutine is created, it starts with a tiny stack:

```
Initial Stack (2KB):

┌─────────────────────────────────────────────┐
│ High Address: 0x7FFE1000 + 2048             │
│ ┌─────────────────────────────────────────┐ │
│ │                                         │ │
│ │                                         │ │
│ │         [Empty space]                   │ │
│ │                                         │ │
│ │  Stack grows downward ↓                 │ │
│ │                                         │ │
│ │                                         │ │
│ └─────────────────────────────────────────┘ │
│ Low Address: 0x7FFE1000                     │
│                                             │
│ Stack Guard: 0x7FFE1000 + 896               │
│ (triggers growth if crossed)                │
└─────────────────────────────────────────────┘

Why 2KB?
- Most goroutines never need more
- Fits in CPU L1 cache (32KB)
- Millions of goroutines = reasonable memory
- Can grow if needed
```

### Stack Growth Detection

The runtime detects when a stack needs to grow:

```
Stack Overflow Detection:

Every function prologue checks stack space:

func myFunction() {
    // Compiler inserts:
    if SP < g.stackguard0 {
        runtime.morestack()
    }
    
    // Actual function code...
}

Assembly prologue:
┌─────────────────────────────────────────────┐
│ MOVQ (TLS), CX         // Load g            │
│ CMPQ SP, stackguard0(CX) // Compare        │
│ JBE  morestack          // Jump if below   │
│                                             │
│ // Function code continues...              │
└─────────────────────────────────────────────┘

When to grow:
┌─────────────────────────────────────────────┐
│ Current SP: 0x7FFE1100                      │
│ Stack guard: 0x7FFE1380 (896 bytes up)     │
│                                             │
│ Function needs: 1KB of local variables      │
│ SP would become: 0x7FFE0D00                 │
│                                             │
│ 0x7FFE0D00 < 0x7FFE1380  ← Would overflow! │
│ → Call morestack()                          │
└─────────────────────────────────────────────┘
```

### Stack Growth Process

When `morestack()` is called, the runtime grows the stack:

```
Stack Growth (Doubling):

Step 1: Allocate new larger stack (2x size)
┌─────────────────────────────────────────────┐
│ Old stack: 2KB                              │
│ ┌─────────────────────────────────────────┐ │
│ │ [Used: 1.5KB]                           │ │
│ │ [Free: 0.5KB]                           │ │
│ └─────────────────────────────────────────┘ │
│                                             │
│         ↓ Allocate new                      │
│                                             │
│ New stack: 4KB                              │
│ ┌─────────────────────────────────────────┐ │
│ │ [Free: 4KB]                             │ │
│ │                                         │ │
│ │                                         │ │
│ └─────────────────────────────────────────┘ │
└─────────────────────────────────────────────┘

Step 2: Copy stack contents
┌─────────────────────────────────────────────┐
│ Old stack → New stack                       │
│                                             │
│ Copy all stack data:                        │
│ - Local variables                           │
│ - Function parameters                       │
│ - Return addresses                          │
│ - Saved registers                           │
│                                             │
│ Update pointers:                            │
│ - SP (stack pointer)                        │
│ - BP (base pointer)                         │
│ - All pointers into stack                   │
└─────────────────────────────────────────────┘

Step 3: Adjust pointers
┌─────────────────────────────────────────────┐
│ oldSP: 0x7FFE1100                           │
│ newSP: 0x7FFE2100 (in new stack)           │
│ delta: 0x1000                               │
│                                             │
│ Scan stack for pointers:                    │
│ oldPointer: 0x7FFE1200                      │
│   → Is this in old stack? YES               │
│   → Adjust: 0x7FFE1200 + 0x1000            │
│   → newPointer: 0x7FFE2200                  │
│                                             │
│ Stack map used for pointer identification   │
└─────────────────────────────────────────────┘

Step 4: Update G structure
┌─────────────────────────────────────────────┐
│ g.stack.lo = newStackLo                     │
│ g.stack.hi = newStackHi                     │
│ g.stackguard0 = newStackLo + _StackGuard   │
│                                             │
│ Free old stack                              │
└─────────────────────────────────────────────┘

Step 5: Resume execution
┌─────────────────────────────────────────────┐
│ Return from morestack()                     │
│ Function prologue checks again              │
│ SP < stackguard0? NO → Continue             │
│ Function executes normally                  │
└─────────────────────────────────────────────┘
```

### Stack Shrinking

Goroutines that no longer need large stacks can shrink:

```
Stack Shrinking:

Conditions:
- Stack is less than 1/4 used
- Stack size > 2KB
- Goroutine yields or blocks

Process:
┌─────────────────────────────────────────────┐
│ Current: 32KB stack, using 4KB              │
│ Usage: 12.5% (< 25%)                        │
│                                             │
│ During GC scan or goroutine yield:          │
│ 1. Allocate new 8KB stack (1/4 size)       │
│ 2. Copy active 4KB to new stack             │
│ 3. Adjust all pointers                      │
│ 4. Update G structure                       │
│ 5. Free old 32KB stack                      │
│                                             │
│ Result: 24KB saved                          │
└─────────────────────────────────────────────┘
```

### Maximum Stack Size

Goroutine stacks can grow very large, but there's a limit:

```
Stack Size Limits:

32-bit systems: 250MB maximum
64-bit systems: 1GB maximum

Growth progression:
2KB → 4KB → 8KB → 16KB → 32KB → 64KB → 128KB → ...
      ↑ Doubles each time until maximum

Typical sizes:
- Simple goroutines: 2-4KB
- HTTP handlers: 4-8KB
- Recursive functions: 16KB-1MB
- Pathological cases: 100MB+

If maximum reached:
- Runtime panics: "stack overflow"
- Program crashes
- Usually indicates infinite recursion
```

---

## Blocking and System Calls {#blocking}

When goroutines block, the scheduler needs to ensure that other goroutines can still run. Let's see how Go handles different types of blocking.

### Network I/O: The Network Poller

Go has a sophisticated network poller that makes blocking I/O non-blocking at the goroutine level:

```
Network Poller Architecture:

User code:
┌─────────────────────────────────────────────┐
│ conn.Read(buffer)  // Appears blocking      │
└─────────────────────────────────────────────┘
         │
         ↓ Runtime translates
┌─────────────────────────────────────────────┐
│ netpollBlockRead(fd)                        │
│ 1. Set fd to non-blocking mode              │
│ 2. Try read → would block (EAGAIN)          │
│ 3. Add fd to epoll/kqueue interest list     │
│ 4. Park goroutine (state: _Gwaiting)        │
│ 5. Call schedule() to run other G           │
└─────────────────────────────────────────────┘
         │
         ↓ Network poller thread
┌─────────────────────────────────────────────┐
│ Dedicated thread running epoll_wait():      │
│                                             │
│ while (true) {                              │
│   events = epoll_wait(epfd, ...)            │
│   for each ready fd {                       │
│     gp = find goroutine waiting on fd       │
│     make gp runnable                        │
│   }                                         │
│ }                                           │
└─────────────────────────────────────────────┘

Example timeline:
T=0: G1 calls conn.Read()
     - fd not ready
     - G1 parked (_Gwaiting)
     - M continues with G2

T=5ms: Data arrives on network
       - epoll_wait returns
       - G1 marked runnable
       - Added to run queue

T=6ms: Scheduler picks G1
       - Resumes conn.Read()
       - Data available now
       - Returns to user code

Result: Blocking call at user level,
        but M never blocked!
```

### System Calls: Handing Off P

When a goroutine makes a blocking system call, it can't use the network poller. Instead, the M blocks but hands off its P:

```
System Call Handling:

func write(fd int, buf []byte) {
    entersyscall()  // Runtime intercepts
    syscall.Write(fd, buf)
    exitsyscall()   // Runtime handles return
}

entersyscall() flow:
┌─────────────────────────────────────────────┐
│ 1. Save current state                       │
│    g.sched.sp = SP                          │
│    g.sched.pc = PC                          │
│    g.sched.bp = BP                          │
│                                             │
│ 2. Change G state                           │
│    g.atomicstatus = _Gsyscall               │
│                                             │
│ 3. Detach P from M                          │
│    m.oldp = m.p                             │
│    m.p = nil                                │
│                                             │
│ 4. Make P available for stealing            │
│    p.status = _Psyscall                     │
└─────────────────────────────────────────────┘

During syscall:
┌─────────────────────────────────────────────┐
│ M0: Blocked in kernel                       │
│ G1: In _Gsyscall state                      │
│ P0: Detached, available                     │
│                                             │
│ Other Ms can steal P0:                      │
│ M1: Idle, looking for work                  │
│ M1: Finds P0 in _Psyscall                   │
│ M1: Steals P0, runs other Gs                │
└─────────────────────────────────────────────┘

exitsyscall() flow:
┌─────────────────────────────────────────────┐
│ Syscall completes, M0 returns                │
│                                             │
│ Try to reacquire P:                         │
│ if m.oldp available:                        │
│   reacquire P                               │
│   change G to _Grunning                     │
│   continue execution                        │
│ else:                                       │
│   P stolen! Put G in global queue           │
│   M0 goes to sleep (cached)                 │
│   G will be scheduled later                 │
└─────────────────────────────────────────────┘
```

### Channel Operations

Channels are one of the most common blocking points:

```
Channel Blocking:

Send on full channel:
┌─────────────────────────────────────────────┐
│ ch := make(chan int, 2)  // Buffered        │
│ ch <- 1                  // OK              │
│ ch <- 2                  // OK (buffer full)│
│ ch <- 3                  // Blocks!         │
└─────────────────────────────────────────────┘
         │
         ↓
┌─────────────────────────────────────────────┐
│ 1. Channel is full, can't send              │
│ 2. Create sudog (wait queue entry):         │
│    sudog.g = current G                      │
│    sudog.elem = pointer to value to send    │
│ 3. Add sudog to channel's sendq             │
│ 4. Change G state to _Gwaiting              │
│ 5. Call gopark() → schedule()               │
└─────────────────────────────────────────────┘

Channel wait queue:
┌─────────────────────────────────────────────┐
│ hchan (channel structure):                  │
│                                             │
│ sendq (send wait queue):                    │
│ ┌─────────────────────────────────────────┐ │
│ │ sudog1 → sudog2 → sudog3                │ │
│ │   G1       G2       G3                  │ │
│ └─────────────────────────────────────────┘ │
│                                             │
│ recvq (receive wait queue):                 │
│ ┌─────────────────────────────────────────┐ │
│ │ sudog4 → sudog5                         │ │
│ │   G4       G5                           │ │
│ └─────────────────────────────────────────┘ │
└─────────────────────────────────────────────┘

When receiver runs:
┌─────────────────────────────────────────────┐
│ val := <-ch                                 │
│                                             │
│ 1. Check sendq for waiting senders          │
│ 2. Found sudog1 (G1 waiting)                │
│ 3. Copy value from sudog1.elem to val       │
│ 4. Remove sudog1 from sendq                 │
│ 5. Make G1 runnable (add to run queue)      │
│ 6. G1 will resume after ch <- 3             │
└─────────────────────────────────────────────┘
```

## Work Stealing {#work-stealing}

Work stealing is a crucial algorithm that keeps all CPU cores busy by redistributing work from busy Ps to idle Ps.

### Why Work Stealing is Needed

Without work stealing, you could have this situation:

```
Unbalanced Load (without work stealing):

P0 (idle):              P1 (overloaded):
runq: []                runq: [G1 G2 G3 G4 G5 G6 G7 G8]
      ↑                        ↑
      │                        │
   CPU idle!              CPU busy!
   
Result:
- CPU Core 0: 0% utilization
- CPU Core 1: 100% utilization
- Other goroutines wait unnecessarily
- Poor performance
```

### The Work Stealing Algorithm

Go implements a randomized work stealing algorithm:

```
Work Stealing Process:

Step 1: M becomes idle
┌─────────────────────────────────────────────┐
│ M0 finishes executing all Gs in P0          │
│ P0's runq: []                               │
│ P0's runnext: nil                           │
│                                             │
│ M0 calls findrunnable() to get work         │
└─────────────────────────────────────────────┘

Step 2: Check local sources (fast path)
┌─────────────────────────────────────────────┐
│ 1. Check local run queue → Empty            │
│ 2. Check global run queue → Empty           │
│ 3. Check network poller → No ready fds      │
│                                             │
│ Decision: Must steal work!                  │
└─────────────────────────────────────────────┘

Step 3: Try to steal (slow path)
┌─────────────────────────────────────────────┐
│ for i := 0; i < 4; i++ {  // 4 attempts     │
│   // Pick random P to steal from            │
│   victim := randomOrder[rand()%nprocs]      │
│   gp := runqsteal(p, victim, ...)           │
│   if gp != nil {                            │
│     return gp  // Success!                  │
│   }                                         │
│ }                                           │
└─────────────────────────────────────────────┘

Visualization:
                Steal attempt
P0 (idle) ─────────────────────────→ P1 (victim)
runq: []                              runq: [G1 G2 G3 G4]
                                             ↓
                    Steal half (2 Gs)       ↓
                    ←───────────────────────┘
P0 (now has work)                     P1 (still has work)
runq: [G3 G4]                         runq: [G1 G2]
```

### Stealing Strategy

The runtime steals half of the victim's run queue:

```
runqsteal() Algorithm:

func runqsteal(p, p2 *p) *g {
    n := len(p2.runq) / 2  // Steal half
    
    if n == 0 {
        return nil  // Nothing to steal
    }
    
    // Atomically steal n goroutines
    gs := make([]*g, n)
    for i := 0; i < n; i++ {
        gs[i] = p2.runq.pop()
    }
    
    // Add to our local queue
    for i := 1; i < n; i++ {
        p.runq.push(gs[i])
    }
    
    return gs[0]  // Run this one immediately
}

Example with numbers:
┌─────────────────────────────────────────────┐
│ Victim P (before steal):                    │
│ runq: [G1][G2][G3][G4][G5][G6][G7][G8]     │
│ count: 8                                    │
│                                             │
│ Stealer P (before steal):                   │
│ runq: []                                    │
│ count: 0                                    │
└─────────────────────────────────────────────┘
         ↓ Steal half (4 goroutines)
┌─────────────────────────────────────────────┐
│ Victim P (after steal):                     │
│ runq: [G1][G2][G3][G4]                     │
│ count: 4                                    │
│                                             │
│ Stealer P (after steal):                    │
│ runq: [G6][G7][G8]                         │
│ count: 3                                    │
│ running: G5 (execute immediately)           │
└─────────────────────────────────────────────┘

Benefits:
- Both Ps have work
- Victim still has work (not starved)
- Load balanced across CPUs
```

### Spin-Wait Optimization

Before giving up, an idle M will "spin" looking for work:

```
Spinning M Behavior:

func findrunnable() *g {
    // ... check local queues ...
    
    // No work found, start spinning
    if !spinning {
        m.spinning = true
        atomic.Xadd(&sched.nmspinning, 1)
    }
    
    for i := 0; i < 4; i++ {
        // Try to steal from random Ps
        for enum := 0; enum < nprocs; enum++ {
            p2 := allp[fastrand()%nprocs]
            if gp := runqsteal(p, p2); gp != nil {
                return gp
            }
        }
        
        // Check global queue
        if gp := globrunqget(p); gp != nil {
            return gp
        }
        
        // Check network poller
        if gp := netpoll(false); gp != nil {
            return gp
        }
    }
    
    // Still no work, stop spinning
    m.spinning = false
    atomic.Xadd(&sched.nmspinning, -1)
    
    // Park M (put to sleep)
    stopm()
}

Spinning vs Sleeping:
┌─────────────────────────────────────────────┐
│ Spinning M:                                 │
│ - Actively checks for work (busy loop)      │
│ - Uses CPU but responds instantly           │
│ - Limited number can spin (GOMAXPROCS)      │
│ - Duration: ~1-10 microseconds              │
│                                             │
│ Sleeping M:                                 │
│ - Gives up CPU completely                   │
│ - Must be woken up (slow)                   │
│ - Unlimited can sleep                        │
│ - Wakeup latency: ~1-10 milliseconds        │
└─────────────────────────────────────────────┘
```

### Global Queue Balancing

To prevent starvation, the scheduler periodically checks the global queue:

```
Global Queue Check (every 61 scheduler ticks):

func schedule() {
    // Every 61 ticks, check global queue
    if schedtick%61 == 0 {
        lock(&sched.lock)
        gp := globrunqget(p, 1)
        unlock(&sched.lock)
        
        if gp != nil {
            return gp
        }
    }
    
    // Continue with local queue...
}

Why 61?
- Prime number (good distribution)
- Not too frequent (avoid lock contention)
- Not too rare (prevent starvation)
- ~60 local runs per 1 global check

Global queue use cases:
┌─────────────────────────────────────────────┐
│ 1. Local queue overflow:                    │
│    - P's runq full (256 capacity)           │
│    - New Gs go to global queue              │
│                                             │
│ 2. Exiting syscall:                         │
│    - M can't reacquire P                    │
│    - G placed in global queue               │
│                                             │
│ 3. GC work:                                 │
│    - Background mark workers                 │
│    - Placed in global queue                 │
└─────────────────────────────────────────────┘
```

---

## Memory and Performance {#memory-performance}

Let's analyze the complete memory and performance characteristics of goroutines.

### Memory Breakdown

```
Memory Usage per Goroutine:

Minimum (idle goroutine):
┌─────────────────────────────────────────────┐
│ G structure:           ~300 bytes           │
│ Initial stack:        2,048 bytes           │
│ Stack guard pages:        0 bytes (virtual) │
│ Total:               ~2,348 bytes           │
└─────────────────────────────────────────────┘

Typical (active goroutine):
┌─────────────────────────────────────────────┐
│ G structure:           ~300 bytes           │
│ Stack (grown once):   4,096 bytes           │
│ Local variables:        ~500 bytes          │
│ Total:               ~4,896 bytes           │
└─────────────────────────────────────────────┘

Large (complex goroutine):
┌─────────────────────────────────────────────┐
│ G structure:           ~300 bytes           │
│ Stack (grown multiple): 32,768 bytes        │
│ Local variables:       ~5,000 bytes         │
│ Total:                ~38,068 bytes          │
└─────────────────────────────────────────────┘

Compare to OS Thread:
┌─────────────────────────────────────────────┐
│ Thread control block: ~2,000 bytes          │
│ Stack (fixed):      1-8 MB (1,048,576 bytes)│
│ Kernel structures:    ~5,000 bytes          │
│ Total:           ~1,055,576 bytes            │
└─────────────────────────────────────────────┘

Goroutine is 450x more memory efficient!
```

### Scalability

How many goroutines can you create?

```
Goroutine Scalability:

Theoretical limits:
┌─────────────────────────────────────────────┐
│ Machine: 16GB RAM                           │
│ OS overhead: 2GB                            │
│ Available: 14GB = 14,680,064 KB            │
│                                             │
│ Per goroutine: 2.3KB                        │
│ Max goroutines: 14,680,064 / 2.3           │
│              = 6,382,636 goroutines        │
│              ≈ 6 million!                   │
└─────────────────────────────────────────────┘

Practical limits:
┌─────────────────────────────────────────────┐
│ Other memory needs:                         │
│ - Application data: 4GB                     │
│ - Caches: 1GB                              │
│ - OS: 2GB                                  │
│ Available for goroutines: 9GB               │
│                                             │
│ Realistic maximum: ~2-3 million            │
│                                             │
│ Performance degrades with:                  │
│ - > 100,000: Increased GC pressure         │
│ - > 1,000,000: Scheduling overhead         │
└─────────────────────────────────────────────┘
```

### Performance Characteristics

```
Operation Costs:

Goroutine Creation:
┌─────────────────────────────────────────────┐
│ Fast path (reuse from cache): ~40 ns        │
│ Slow path (new allocation):  ~200 ns        │
│ OS thread creation:         ~20,000 ns      │
│                                             │
│ Goroutine is 100-500x faster!               │
└─────────────────────────────────────────────┘

Context Switch:
┌─────────────────────────────────────────────┐
│ Goroutine switch:           ~20 ns          │
│ OS thread switch:        ~1,000 ns          │
│                                             │
│ Goroutine is 50x faster!                    │
└─────────────────────────────────────────────┘

Stack Growth:
┌─────────────────────────────────────────────┐
│ 2KB → 4KB growth:           ~500 ns         │
│ Includes:                                   │
│ - Allocate new stack                        │
│ - Copy old stack                            │
│ - Adjust pointers                           │
│ - Update G structure                        │
│                                             │
│ Amortized cost: Usually once per goroutine  │
└─────────────────────────────────────────────┘

Channel Operations:
┌─────────────────────────────────────────────┐
│ Unbuffered send/recv:       ~100 ns         │
│ Buffered send (space available): ~30 ns     │
│ Buffered recv (data available): ~30 ns      │
│ Blocking operation overhead: ~50 ns         │
└─────────────────────────────────────────────┘
```

### CPU Cache Effects

Goroutines benefit significantly from CPU cache locality:

```
Cache Effects:

Goroutine Stack in Cache:
┌─────────────────────────────────────────────┐
│ L1 Cache: 32KB per core                    │
│ Goroutine stack: 2-4KB (fits!)             │
│                                             │
│ When goroutine runs:                        │
│ 1. Stack data loaded into L1               │
│ 2. All local variable access: ~1ns         │
│ 3. No RAM access needed                    │
│                                             │
│ Benefits:                                   │
│ - 100x faster than RAM access              │
│ - Reduced memory bus traffic               │
│ - Better energy efficiency                 │
└─────────────────────────────────────────────┘

OS Thread Stack (can't fit in cache):
┌─────────────────────────────────────────────┐
│ L1 Cache: 32KB per core                    │
│ Thread stack: 1-8MB (way too big!)         │
│                                             │
│ When thread runs:                           │
│ 1. Only small portion in cache             │
│ 2. Frequent cache misses                   │
│ 3. Must access RAM (100x slower)           │
│                                             │
│ Problems:                                   │
│ - Cache pollution                           │
│ - Memory bus saturation                    │
│ - Poor performance                         │
└─────────────────────────────────────────────┘
```

---

## Complete Example: Goroutine Lifecycle

Let's trace a complete realistic example from start to finish:

```go
package main

import (
    "fmt"
    "net/http"
    "time"
)

func main() {
    // Main goroutine (G1) starts here
    http.HandleFunc("/", handler)
    http.ListenAndServe(":8080", nil)
}

func handler(w http.ResponseWriter, r *http.Request) {
    // G2: HTTP handler goroutine
    
    // Spawn worker goroutine
    result := make(chan string)
    go worker(result)  // Creates G3
    
    // Wait for result
    data := <-result
    
    fmt.Fprintf(w, "Result: %s", data)
}

func worker(result chan string) {
    // G3: Worker goroutine
    
    // Simulate work
    time.Sleep(100 * time.Millisecond)
    
    result <- "done"
}
```

Complete execution trace:

```
T=0: Program starts
┌─────────────────────────────────────────────┐
│ Runtime initialization:                     │
│ 1. Create M0 (first OS thread)              │
│ 2. Create P0 (first processor)              │
│ 3. Create G1 (main goroutine)               │
│ 4. G1.state = _Grunning                     │
│ 5. Execute main()                           │
└─────────────────────────────────────────────┘

Memory:
M0: Running G1 (main)
P0: Attached to M0
G1: _Grunning, executing main()

T=10ms: HTTP request arrives
┌─────────────────────────────────────────────┐
│ HTTP server accepts connection:             │
│ 1. Accept() returns new socket              │
│ 2. newproc() creates G2 for handler         │
│ 3. G2.state = _Grunnable                    │
│ 4. G2 added to P0's runnext                 │
└─────────────────────────────────────────────┘

Memory:
M0: Running G1 (HTTP accept loop)
P0: runq=[G2]
G1: _Grunning
G2: _Grunnable (waiting in queue)

T=11ms: G1 yields, G2 scheduled
┌─────────────────────────────────────────────┐
│ 1. G1 calls accept() again (blocks)         │
│ 2. G1 parks via gopark()                    │
│ 3. G1.state = _Gwaiting                     │
│ 4. schedule() picks G2 from runnext         │
│ 5. G2.state = _Grunning                     │
│ 6. gogo(&G2.sched) - context switch!        │
│ 7. CPU now executing handler()              │
└─────────────────────────────────────────────┘

Memory:
M0: Running G2 (handler)
P0: runq=[]
G1: _Gwaiting (on socket)
G2: _Grunning
Context switch: ~20ns

T=12ms: go worker(result)
┌─────────────────────────────────────────────┐
│ 1. Compiler calls newproc()                 │
│ 2. Try P0's gFree list → Found G3!         │
│ 3. Reset G3 fields, copy arguments          │
│ 4. G3.startpc = addressOf(worker)          │
│ 5. G3.state = _Grunnable                    │
│ 6. Add to P0's runnext                      │
└─────────────────────────────────────────────┘

Memory:
M0: Running G2
P0: runq=[], runnext=G3
G1: _Gwaiting
G2: _Grunning
G3: _Grunnable (ready to run)
Cost: ~40ns (reused from cache)

T=13ms: <-result (blocks)
┌─────────────────────────────────────────────┐
│ 1. G2 tries to receive from empty channel   │
│ 2. Create sudog for G2                      │
│ 3. Add sudog to channel's recvq             │
│ 4. G2.state = _Gwaiting                     │
│ 5. gopark() called                          │
│ 6. schedule() picks G3                      │
│ 7. G3.state = _Grunning                     │
│ 8. gogo(&G3.sched)                          │
└─────────────────────────────────────────────┘

Memory:
M0: Running G3 (worker)
P0: runq=[]
G1: _Gwaiting (on socket)
G2: _Gwaiting (on channel)
G3: _Grunning
Channel recvq: [G2]

T=14ms: time.Sleep(100ms)
┌─────────────────────────────────────────────┐
│ 1. Create timer for 100ms                   │
│ 2. G3.state = _Gwaiting                     │
│ 3. gopark() called                          │
│ 4. schedule() called                        │
│ 5. No work in P0, check global queue        │
│ 6. No work anywhere, M0 spins               │
│ 7. After spinning, M0 sleeps                │
└─────────────────────────────────────────────┘

Memory:
M0: Sleeping (no work)
P0: Idle
G1: _Gwaiting (on socket)
G2: _Gwaiting (on channel)
G3: _Gwaiting (on timer)
All goroutines blocked!

T=114ms: Timer fires
┌─────────────────────────────────────────────┐
│ 1. Timer goroutine wakes up                 │
│ 2. Makes G3 runnable                        │
│ 3. Adds G3 to P0's run queue                │
│ 4. Wakes up M0 (or creates new M)          │
│ 5. M0 acquires P0                           │
│ 6. schedule() picks G3                      │
│ 7. G3.state = _Grunning                     │
└─────────────────────────────────────────────┘

Memory:
M0: Running G3
P0: Attached to M0
G1: _Gwaiting (on socket)
G2: _Gwaiting (on channel)
G3: _Grunning

T=115ms: result <- "done"
┌─────────────────────────────────────────────┐
│ 1. G3 sends to channel                      │
│ 2. Channel's recvq has waiting G2           │
│ 3. Direct send to G2's stack                │
│ 4. Remove G2 from recvq                     │
│ 5. G2.state = _Grunnable                    │
│ 6. Add G2 to P0's run queue                 │
│ 7. G3 continues to return                   │
└─────────────────────────────────────────────┘

Memory:
M0: Running G3
P0: runq=[G2]
G1: _Gwaiting (on socket)
G2: _Grunnable (woken up!)
G3: _Grunning

T=116ms: worker() returns
┌─────────────────────────────────────────────┐
│ 1. G3 executes goexit()                     │
│ 2. G3.state = _Gdead                        │
│ 3. Add G3 to P0's gFree list (reuse!)      │
│ 4. schedule() picks G2                      │
│ 5. G2.state = _Grunning                     │
│ 6. G2 resumes after <-result                │
│ 7. G2 received value "done"                 │
└─────────────────────────────────────────────┘

Memory:
M0: Running G2
P0: gFree=[G3]
G1: _Gwaiting (on socket)
G2: _Grunning
G3: _Gdead (cached for reuse)

T=117ms: fmt.Fprintf(w, ...)
┌─────────────────────────────────────────────┐
│ 1. G2 writes HTTP response                  │
│ 2. May block briefly on socket write        │
│ 3. Returns from handler()                   │
│ 4. G2.state = _Gdead                        │
│ 5. Add G2 to P0's gFree                     │
│ 6. M0 continues serving other requests      │
└─────────────────────────────────────────────┘

Memory:
M0: Looking for work
P0: gFree=[G2, G3]
G1: _Gwaiting (on socket)
G2: _Gdead (cached)
G3: _Gdead (cached)

Total time: 117ms
G2 active time: ~6ms
G3 active time: ~2ms
Blocked time: ~109ms (efficient!)
Context switches: ~6 (each ~20ns)
Total switch overhead: ~120ns

Efficiency:
- 2 goroutines handled request
- Only 2.3KB * 2 = 4.6KB memory used
- Reused from cache (fast creation)
- M never blocked (served other requests during sleep)
- CPU idle most of time (waiting is free!)
```

---

## Summary

Goroutines are lightweight, user-space threads that make concurrent programming in Go efficient and scalable. Let's recap the key concepts:

**CPU and Memory Fundamentals**:

- Modern CPUs have multiple cores with cache hierarchies
- Memory access speed varies: registers (1ns) → L1 cache (1ns) → RAM (100ns)
- Goroutines are designed to fit in CPU cache for maximum speed

**OS Threads vs Goroutines**:

- OS threads: 1-8MB stack, expensive creation/switching, kernel-managed
- Goroutines: 2KB stack, cheap creation/switching, user-space managed
- Goroutines are 100-500x more efficient

**The GMP Model**:

- G (Goroutine): Execution unit, ~2KB, millions possible
- M (Machine): OS thread, one per CPU core typically
- P (Processor): Execution context, GOMAXPROCS count
- M needs P to execute goroutines

**Goroutine Creation**:

- Fast path: ~40ns (reuse from cache)
- Slow path: ~200ns (new allocation)
- Added to P's local run queue
- Scheduled quickly by Go runtime

**Goroutine Execution**:

- Scheduled cooperatively (mostly)
- Preempted every 10ms for fairness
- Context switch: ~20ns (~50 CPU cycles)
- Runs on M+P combination

**Stack Management**:

- Starts at 2KB
- Grows dynamically (doubles each time)
- Can shrink when usage drops
- Maximum: 1GB on 64-bit systems

**Blocking Operations**:

- Network I/O: Uses network poller (epoll/kqueue)
- System calls: P handed off to another M
- Channels: Goroutine parked, woken when unblocked
- Never wastes OS thread on blocked goroutine

**Work Stealing**:

- Idle Ps steal work from busy Ps
- Steals half of victim's run queue
- Randomized to avoid contention
- Keeps all CPU cores busy

**Performance**:

- 6+ million goroutines on 16GB RAM
- Context switch: 50x faster than OS threads
- Creation: 500x faster than OS threads
- Memory: 450x more efficient than OS threads

Goroutines make concurrent programming practical and efficient in Go!