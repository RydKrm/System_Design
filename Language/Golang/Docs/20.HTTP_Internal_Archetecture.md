# The Internal Architecture of HTTP Server and Client in Go

## Table of Contents

1. [Introduction - The Complete Journey](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#introduction)
2. [Hardware to Software: The Foundation](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#hardware-foundation)
3. [Operating System Level](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#os-level)
4. [Network Stack: TCP/IP](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#network-stack)
5. [Go Runtime and Goroutines](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#go-runtime)
6. [HTTP Client Internal Architecture](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#http-client-architecture)
7. [HTTP Server Internal Architecture](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#http-server-architecture)
8. [Memory Management](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#memory-management)
9. [Connection Lifecycle](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#connection-lifecycle)
10. [Performance Optimization](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#performance-optimization)

---

## Introduction - The Complete Journey {#introduction}

When you write `http.Get("https://example.com")` or `http.ListenAndServe(":8080", nil)`, what actually happens? This journey takes us from the CPU and memory at the hardware level, through the operating system, network stack, Go runtime, and finally to your application code. Understanding this complete picture helps you write better, faster, and more reliable network applications.

### Why This Matters

Imagine you're debugging a production issue where your HTTP server is using 100% CPU or running out of memory. Without understanding the internals, you're shooting in the dark. But when you understand how connections are managed, how goroutines work, how memory is allocated, and how the network stack operates, you can diagnose and fix issues quickly.

Similarly, if you're building a high-performance API that needs to handle 100,000 requests per second, understanding these internals helps you make the right architectural decisions about connection pooling, timeout configuration, and resource management.

### The Layered Architecture

Network communication in Go operates in layers, like an onion. Each layer has specific responsibilities:

**Hardware Layer**: Physical CPU cores execute instructions, RAM stores data, network cards send/receive electrical signals over wires or wireless.

**Operating System Layer**: The kernel manages hardware resources, schedules processes, handles interrupts from network cards, manages memory pages, and provides system calls.

**Network Stack Layer**: TCP/IP protocols handle packet routing, connection management, reliability, and flow control.

**Go Runtime Layer**: The Go scheduler manages goroutines, the garbage collector manages memory, and the netpoller handles I/O efficiently.

**Application Layer**: Your Go code using `net/http` package, which orchestrates all the lower layers.

Let's dive deep into each layer.

---

## Hardware to Software: The Foundation {#hardware-foundation}

### CPU Architecture and Execution

At the most fundamental level, your HTTP server and client run on a CPU. Modern CPUs are incredibly complex, but let's understand the key concepts relevant to networking:

**CPU Cores**: Your computer has multiple CPU cores (typically 4, 8, or more). Each core can execute instructions independently. When a request arrives at your HTTP server, the OS schedules your code to run on one of these cores.

```
Physical CPU Layout:
┌─────────────────────────────────────────┐
│  CPU Die                                 │
│  ┌──────────┐  ┌──────────┐             │
│  │  Core 0  │  │  Core 1  │             │
│  │  ┌────┐  │  │  ┌────┐  │             │
│  │  │ALU │  │  │  │ALU │  │             │
│  │  │FPU │  │  │  │FPU │  │             │
│  │  └────┘  │  │  └────┘  │             │
│  │  L1 Cache│  │  L1 Cache│             │
│  │   32KB   │  │   32KB   │             │
│  └──────────┘  └──────────┘             │
│         │              │                 │
│         └──────┬───────┘                 │
│            L2 Cache                      │
│             256KB                        │
│                │                         │
│            L3 Cache                      │
│              8MB                         │
└─────────────────────────────────────────┘
           │
           ↓
    Main Memory (RAM)
      16GB / 32GB
```

**CPU Caches**: CPUs have multiple levels of cache (L1, L2, L3) that store frequently accessed data. When your HTTP server processes a request, data flows through these caches:

- **L1 Cache** (~32KB, ~4 cycles): Fastest, stores most frequently used data
- **L2 Cache** (~256KB, ~12 cycles): Per-core cache
- **L3 Cache** (~8MB, ~40 cycles): Shared across cores
- **RAM** (~16GB, ~100 cycles): Main system memory
- **SSD/HDD** (~500GB, ~100,000 cycles): Persistent storage

When you read HTTP request data, it travels from the network card through these layers into CPU registers where it's processed.

**Instruction Pipeline**: CPUs execute instructions in a pipeline. A simple HTTP response write might translate to dozens or hundreds of CPU instructions:

```assembly
; Simplified assembly for writing HTTP response
MOV  RAX, [buffer_address]    ; Load buffer address into register
MOV  RBX, [data_length]        ; Load data length
CALL write_syscall             ; System call to write data
CMP  RAX, 0                    ; Check return value
JL   error_handler             ; Jump if error
```

### Memory Architecture

Memory is organized hierarchically, and understanding this is crucial for optimizing HTTP servers:

**Virtual Memory**: Each process (including your Go program) sees its own virtual address space. The OS maps this to physical RAM:

```
Virtual Memory Layout of Go HTTP Server:
┌────────────────────────────────────┐ High Address
│  Kernel Space (OS)                 │
├────────────────────────────────────┤
│  Stack (goroutine stacks)          │
│    - Handler goroutine 1 stack     │
│    - Handler goroutine 2 stack     │
│    - ... (thousands of stacks)     │
├────────────────────────────────────┤
│  Heap (dynamic allocations)        │
│    - HTTP request buffers          │
│    - Response buffers              │
│    - Connection structs            │
│    - User data structures          │
├────────────────────────────────────┤
│  Data Segment (global vars)        │
├────────────────────────────────────┤
│  Text Segment (executable code)    │
└────────────────────────────────────┘ Low Address
```

**Memory Pages**: The OS divides memory into pages (typically 4KB). When your HTTP server allocates memory, it requests pages from the OS. The Go runtime manages these pages efficiently.

**NUMA (Non-Uniform Memory Access)**: On multi-socket servers, memory access speed depends on which CPU socket accesses which RAM bank. This affects HTTP server performance on large machines.

### Network Interface Card (NIC)

The NIC is the hardware that actually sends and receives network packets:

**Packet Reception Flow**:

```
1. Electrical signals arrive at NIC
2. NIC converts signals to bits
3. NIC performs CRC check (error detection)
4. NIC raises hardware interrupt
5. CPU stops current work, runs interrupt handler
6. OS kernel copies packet from NIC buffer to memory
7. Packet queued for processing
8. CPU resumes previous work
```

**DMA (Direct Memory Access)**: Modern NICs use DMA to copy data directly to memory without CPU involvement, freeing the CPU for other work.

**Ring Buffers**: The NIC and kernel communicate through ring buffers:

```
NIC Ring Buffer (in kernel memory):
┌───┬───┬───┬───┬───┬───┬───┬───┐
│ P │ P │ P │ E │ E │ E │ E │ E │
└───┴───┴───┴───┴───┴───┴───┴───┘
  ↑           ↑
  Head        Tail
  (NIC)       (Kernel)

P = Packet ready for kernel to process
E = Empty slot for incoming packets
```

When packets arrive faster than the kernel can process them, the buffer fills up, causing packet loss.

---

## Operating System Level {#os-level}

The operating system is the bridge between your Go program and the hardware. Let's understand how it manages network I/O.

### System Calls

Your Go HTTP server doesn't directly access hardware. It makes system calls to ask the OS to do things:

**Key System Calls for Networking**:

```c
// Socket creation
int socket(int domain, int type, int protocol);
// domain: AF_INET (IPv4), AF_INET6 (IPv6)
// type: SOCK_STREAM (TCP), SOCK_DGRAM (UDP)

// Bind socket to address
int bind(int sockfd, struct sockaddr *addr, socklen_t addrlen);

// Listen for connections
int listen(int sockfd, int backlog);

// Accept incoming connection
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);

// Read data
ssize_t read(int fd, void *buf, size_t count);

// Write data
ssize_t write(int fd, const void *buf, size_t count);

// Close connection
int close(int fd);

// I/O multiplexing
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

When you call `http.ListenAndServe(":8080", nil)`, Go eventually calls these system calls:

```go
// Simplified version of what happens internally
fd := socket(AF_INET, SOCK_STREAM, 0)  // Create socket
bind(fd, "0.0.0.0:8080")                // Bind to port 8080
listen(fd, 128)                         // Listen with backlog of 128
```

**System Call Overhead**: Each system call has overhead:

1. CPU switches from user mode to kernel mode
2. Context saved (registers, program counter)
3. Kernel executes the system call
4. Context restored
5. CPU switches back to user mode

This context switch takes ~100-1000 CPU cycles. This is why batching I/O operations and using efficient I/O models (like epoll) is crucial.

### File Descriptors

In Unix-like systems, everything is a file, including network connections. Each connection is represented by a file descriptor (fd):

```
File Descriptor Table for HTTP Server Process:
┌────┬─────────────────────────────┐
│ FD │ Description                  │
├────┼─────────────────────────────┤
│ 0  │ stdin (standard input)       │
│ 1  │ stdout (standard output)     │
│ 2  │ stderr (standard error)      │
│ 3  │ Listening socket (port 8080) │
│ 4  │ Client connection 1          │
│ 5  │ Client connection 2          │
│ 6  │ Database connection          │
│ 7  │ Client connection 3          │
│... │ ...                          │
└────┴─────────────────────────────┘
```

The OS limits the number of file descriptors per process (typically 1024-65536). This limits how many concurrent connections your HTTP server can handle.

### I/O Models

The OS provides different models for handling I/O. Understanding these is crucial for HTTP server performance:

**1. Blocking I/O (Traditional Model)**:

```
Thread 1:              Thread 2:              Thread 3:
┌─────────┐           ┌─────────┐           ┌─────────┐
│ accept()│           │ accept()│           │ accept()│
│  BLOCK  │           │  BLOCK  │           │  BLOCK  │
└────┬────┘           └────┬────┘           └────┬────┘
     │                     │                     │
     │ Client 1 connects   │                     │
     ↓                     │                     │
┌─────────┐               │                     │
│ read()  │               │ Client 2 connects   │
│  BLOCK  │               ↓                     │
└────┬────┘           ┌─────────┐               │
     │                │ read()  │               │
     ↓                │  BLOCK  │               │
┌─────────┐           └────┬────┘               │
│process()│                │                     │
└────┬────┘                ↓                     │
     │                ┌─────────┐               │
     ↓                │process()│               │
┌─────────┐           └────┬────┘               │
│ write() │                │                     │
│  BLOCK  │                ↓                     │
└─────────┘           ┌─────────┐               │
                      │ write() │               │
                      │  BLOCK  │               │
                      └─────────┘               │

Problem: Each connection needs its own thread!
With 10,000 connections = 10,000 threads = disaster!
```

**2. Non-blocking I/O**:

```c
// Set socket to non-blocking mode
int flags = fcntl(sockfd, F_GETFL, 0);
fcntl(sockfd, F_SETFL, flags | O_NONBLOCK);

// Now read() returns immediately
ssize_t n = read(sockfd, buffer, size);
if (n == -1 && errno == EAGAIN) {
    // No data available yet, try again later
}
```

**3. I/O Multiplexing (epoll/kqueue)**:

This is what Go uses internally. It's the most efficient model:

```
Single Thread with epoll:
┌─────────────────────────────────────┐
│  Epoll Event Loop                   │
│                                     │
│  while (true) {                     │
│    events = epoll_wait(...)         │
│    for each event {                 │
│      if (event is readable)         │
│        read data                    │
│      if (event is writable)         │
│        write data                   │
│    }                                │
│  }                                  │
└─────────────────────────────────────┘
           │
           ↓
    ┌──────────────┐
    │  Epoll FD    │
    └──────┬───────┘
           │
    ┌──────┴───────┬──────────┬──────────┐
    │              │          │          │
┌───▼───┐     ┌───▼───┐  ┌──▼───┐  ┌───▼───┐
│Conn 1 │     │Conn 2 │  │Conn 3│  │Conn N │
│ Ready │     │ Ready │  │Wait  │  │ Ready │
└───────┘     └───────┘  └──────┘  └───────┘

One thread monitors thousands of connections!
Only processes connections that have data ready!
```

**How epoll works**:

1. Create epoll instance: `epfd = epoll_create()`
2. Add socket to epoll: `epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &event)`
3. Wait for events: `n = epoll_wait(epfd, events, MAX_EVENTS, timeout)`
4. Process ready connections
5. Go back to step 3

This is extremely efficient because:

- One thread can monitor thousands of connections
- Kernel notifies you only when connections have data
- No busy-waiting or constant polling

### Kernel Network Buffer

The kernel maintains buffers for each socket:

```
Socket Buffers:
┌─────────────────────────────────────┐
│  Socket FD: 4                       │
├─────────────────────────────────────┤
│  Receive Buffer (SO_RCVBUF)         │
│  ┌───────────────────────────────┐  │
│  │ [Data][Data][Data][Empty]     │  │
│  └───────────────────────────────┘  │
│  Size: 87380 bytes (default)        │
│  Used: 45234 bytes                  │
│                                     │
│  Send Buffer (SO_SNDBUF)            │
│  ┌───────────────────────────────┐  │
│  │ [Data][Data][Empty][Empty]    │  │
│  └───────────────────────────────┘  │
│  Size: 16384 bytes (default)        │
│  Used: 8192 bytes                   │
└─────────────────────────────────────┘
```

When you read from a socket, Go copies data from the kernel's receive buffer to your application's buffer. When you write, Go copies to the kernel's send buffer, and the kernel sends it over the network.

Buffer sizes affect performance:

- Too small: Frequent system calls
- Too large: Wasted memory
- Go uses reasonable defaults but you can tune them

---

## Network Stack: TCP/IP {#network-stack}

Before data reaches your Go HTTP server, it passes through the network stack. Understanding this helps you optimize and troubleshoot.

### The TCP/IP Layer Model

```
HTTP Request Journey Through Network Stack:

Application Layer (Your Go Code):
┌─────────────────────────────────────┐
│  http.Get("example.com/api")        │
│  Constructs HTTP request            │
└───────────────┬─────────────────────┘
                ↓
Transport Layer (TCP):
┌─────────────────────────────────────┐
│  Wraps HTTP in TCP segments         │
│  Adds: Source Port, Dest Port       │
│  Handles: Reliability, Ordering     │
│  Sequence: 1001, ACK: 2000          │
└───────────────┬─────────────────────┘
                ↓
Network Layer (IP):
┌─────────────────────────────────────┐
│  Wraps TCP in IP packets            │
│  Adds: Source IP, Dest IP           │
│  Handles: Routing                   │
│  192.168.1.100 → 93.184.216.34      │
└───────────────┬─────────────────────┘
                ↓
Link Layer (Ethernet):
┌─────────────────────────────────────┐
│  Wraps IP in Ethernet frames        │
│  Adds: Source MAC, Dest MAC         │
│  Handles: Physical delivery         │
└───────────────┬─────────────────────┘
                ↓
Physical Layer:
┌─────────────────────────────────────┐
│  Electrical signals on wire         │
│  Or radio waves for WiFi            │
└─────────────────────────────────────┘
```

### TCP Connection Establishment (Three-Way Handshake)

Before any HTTP data is sent, TCP establishes a connection:

```
Client (HTTP Client)          Server (HTTP Server)
     │                              │
     │         SYN (seq=1000)       │
     ├─────────────────────────────>│
     │                              │ Server allocates
     │                              │ resources, creates
     │                              │ connection state
     │                              │
     │    SYN-ACK (seq=2000,       │
     │            ack=1001)         │
     │<─────────────────────────────┤
     │                              │
Client allocates                    │
resources                           │
     │                              │
     │     ACK (seq=1001,           │
     │          ack=2001)           │
     ├─────────────────────────────>│
     │                              │
     │    Connection Established    │
     │                              │
     │    HTTP GET / HTTP/1.1       │
     ├─────────────────────────────>│
     │                              │
```

**Time cost**: The three-way handshake takes one full round-trip time (RTT). If the server is 50ms away:

- SYN: 50ms
- SYN-ACK: 50ms
- ACK + Data: 50ms
- Total: 150ms before any data is sent!

This is why connection pooling is crucial for HTTP clients.

### TCP State Machine

Each TCP connection goes through states:

```
TCP Connection States:

CLOSED ──→ SYN_SENT ──→ ESTABLISHED ──→ FIN_WAIT_1
                                    ↓
                                  FIN_WAIT_2
                                    ↓
                                TIME_WAIT
                                    ↓
                                 CLOSED

For HTTP Server listening socket:
CLOSED ──→ LISTEN ──→ (stays in LISTEN)

For accepted connection:
LISTEN ──→ SYN_RCVD ──→ ESTABLISHED ──→ CLOSE_WAIT ──→ LAST_ACK ──→ CLOSED
```

Understanding these states helps debug connection issues. For example, `TIME_WAIT` state keeps ports busy for 2 minutes after closing, which can exhaust ports on a busy server.

### TCP Flow Control and Congestion Control

TCP ensures reliable delivery through complex mechanisms:

**Sliding Window**:

```
Sender's Send Window:
┌────┬────┬────┬────┬────┬────┬────┬────┐
│ 1  │ 2  │ 3  │ 4  │ 5  │ 6  │ 7  │ 8  │
└────┴────┴────┴────┴────┴────┴────┴────┘
 Sent &  Sent,    Unsent but     Not yet
  ACKed   Not    allowed to      allowed
         ACKed     send

Receiver advertises window size: 4 packets
Sender can only send 4 unacknowledged packets
```

**Congestion Control** (Simplified):

```
Start: Slow Start
cwnd = 1 MSS

     │    cwnd (packets)
 16  │                    ╱
     │                 ╱
 14  │              ╱
     │           ╱
 12  │        ╱
     │     ╱
 10  │  ╱  
     │╱      Packet Loss!
  8  │         ↓
     │         └─→ cwnd = cwnd / 2
  6  │              ╱
     │           ╱
  4  │        ╱
     │     ╱
  2  │  ╱
     │╱
  0  └────────────────────────────────> Time
     Slow    Congestion    Fast
     Start   Avoidance     Recovery
```

This affects HTTP performance:

- New connections start slow (slow start)
- Lost packets reduce throughput
- Long-lived connections perform better

### HTTP Over TCP

HTTP is built on top of TCP. Here's what happens when you make an HTTP request:

```
HTTP Request on TCP Stream:

Byte Stream:
┌────────────────────────────────────────────────────┐
│ GET /api/users HTTP/1.1\r\n                        │
│ Host: example.com\r\n                              │
│ User-Agent: Go-http-client/1.1\r\n                │
│ Accept-Encoding: gzip\r\n                         │
│ \r\n                                               │
└────────────────────────────────────────────────────┘
         │
         ↓ TCP segments this into packets
         │
TCP Packet 1:                TCP Packet 2:
┌─────────────────────┐     ┌──────────────────┐
│ GET /api/users HT   │     │ example.com\r\n  │
│ Seq: 1000           │     │ Seq: 1017        │
└─────────────────────┘     └──────────────────┘

Server receives packets, reassembles into stream,
parses HTTP request.
```

**Key Point**: TCP provides a byte stream, not messages. HTTP has to define message boundaries using `\r\n\r\n` (blank line) between headers and body, and `Content-Length` or chunked encoding for body.

## Go Runtime and Goroutines {#go-runtime}

Now we move from the OS level to the Go runtime. This is where Go's magic happens - efficient concurrency without the overhead of OS threads.

### The Go Scheduler

Go programs don't run directly on OS threads. Instead, the Go scheduler multiplexes goroutines onto OS threads:

```
Go Scheduler Architecture:

┌─────────────────────────────────────────────────────────┐
│                    Go Runtime                           │
│                                                         │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐            │
│  │    M1    │  │    M2    │  │    M3    │   M = Machine│
│  │ (OS Thrd)│  │ (OS Thrd)│  │ (OS Thrd)│   (OS Thread)│
│  └─────┬────┘  └─────┬────┘  └─────┬────┘            │
│        │             │             │                   │
│  ┌─────▼────┐  ┌─────▼────┐  ┌─────▼────┐            │
│  │    P1    │  │    P2    │  │    P3    │   P = Processor│
│  │(Scheduler│  │(Scheduler│  │(Scheduler│   (Logical)  │
│  │ Context) │  │ Context) │  │ Context) │            │
│  └─────┬────┘  └─────┬────┘  └─────┬────┘            │
│        │             │             │                   │
│   ┌────┴────┐   ┌────┴────┐   ┌────┴────┐            │
│   │RunQueue │   │RunQueue │   │RunQueue │            │
│   ├─────────┤   ├─────────┤   ├─────────┤            │
│   │ G1 G2 G3│   │ G4 G5 G6│   │ G7 G8 G9│   G = Goroutine│
│   │ G10 G11 │   │ G12 G13 │   │ G14 G15 │            │
│   └─────────┘   └─────────┘   └─────────┘            │
│                                                         │
│  Global Run Queue:                                     │
│  ┌─────────────────────────────────────────┐          │
│  │ G100, G101, G102, ... (unscheduled)     │          │
│  └─────────────────────────────────────────┘          │
└─────────────────────────────────────────────────────────┘
```

**Components**:

**M (Machine)**: An OS thread. Go creates one M per CPU core by default (GOMAXPROCS).

**P (Processor)**: A logical processor that holds the context for executing goroutines. Represents resources needed to execute Go code.

**G (Goroutine)**: A lightweight thread managed by Go. Each HTTP request handler runs in its own goroutine.

**Run Queues**: Each P has a local run queue of goroutines ready to execute. There's also a global run queue.

### Goroutine Creation and Execution

When an HTTP server receives a connection, it creates a goroutine:

```go
// Inside HTTP server
go server.handleConnection(conn)
```

What happens internally:

```
1. Stack Allocation:
   ┌─────────────────────────────────────┐
   │  Goroutine Stack (initial 2KB)      │
   │  ┌───────────────────────────────┐  │
   │  │ Function local variables      │  │
   │  │ Return addresses              │  │
   │  │ Function parameters           │  │
   │  └───────────────────────────────┘  │
   │  Can grow up to 1GB if needed       │
   └─────────────────────────────────────┘

2. G Structure Creation:
   type g struct {
       stack       stack   // Stack bounds
       stackguard0 uintptr // Stack overflow detection
       m           *m      // Current M executing this G
       sched       gobuf   // Saved context when switching
       atomicstatus uint32 // Current state
       // ... many more fields
   }

3. Add to Run Queue:
   P's local run queue <- New G

4. Eventually Scheduled:
   M picks G from P's run queue
   M executes G's function
```

### Goroutine Context Switching

Unlike OS thread context switches (expensive), goroutine switches are cheap:

```
OS Thread Context Switch (~1000 cycles):
1. Save all CPU registers (16-20 registers)
2. Save FPU state (~512 bytes)
3. Flush TLB (Translation Lookaside Buffer)
4. Switch page table
5. Restore new thread's state
6. Prime pipeline

Goroutine Context Switch (~50 cycles):
1. Save: PC (program counter), SP (stack pointer)
2. Switch stack pointer
3. Restore: PC, SP
4. Continue execution

50x faster!
```

This is why Go can efficiently handle 10,000+ concurrent HTTP connections with goroutines, while OS threads would struggle with even 1,000.

### The Network Poller (netpoller)

This is the secret sauce that makes Go's network I/O efficient. It integrates with epoll (Linux), kqueue (BSD/macOS), or IOCP (Windows):

```
Network Poller Architecture:

┌─────────────────────────────────────────────────────┐
│  Goroutine 1 (HTTP Handler)                         │
│  conn.Read(buffer)  ────────┐                       │
└─────────────────────────────┼───────────────────────┘
                               │
                               ↓ Blocks? Park goroutine
┌─────────────────────────────────────────────────────┐
│  Network Poller (Background Thread)                  │
│                                                      │
│  epoll_wait([fd1, fd2, fd3, ...])                   │
│       │                                              │
│       └──→ fd2 is ready!                            │
│                                                      │
│  netpollready(fd2)  ────────┐                       │
└─────────────────────────────┼───────────────────────┘
                               │
                               ↓ Unpark goroutine
┌─────────────────────────────────────────────────────┐
│  Goroutine 1 (HTTP Handler)                         │
│  conn.Read(buffer)  ──→ Returns with data           │
└─────────────────────────────────────────────────────┘
```

**How it works**:

1. Goroutine calls `conn.Read()`
2. If no data available, goroutine is "parked" (removed from run queue)
3. Network poller monitors the file descriptor with epoll
4. When data arrives, network poller "unparks" the goroutine
5. Goroutine resumes execution with data

**Key Benefit**: Blocking I/O in Go code becomes non-blocking under the hood. You write simple blocking code, but it's actually asynchronous and efficient.

### Memory Model and Goroutine Communication

Goroutines communicate through channels, which are carefully synchronized:

```
Channel Internal Structure:

type hchan struct {
    qcount   uint           // Total data in the queue
    dataqsiz uint           // Size of circular queue
    buf      unsafe.Pointer // Pointer to array of elements
    elemsize uint16         // Size of each element
    closed   uint32         // Channel closed?
    
    sendx    uint   // Send index
    recvx    uint   // Receive index
    
    recvq    waitq  // List of blocked receivers
    sendq    waitq  // List of blocked senders
    
    lock mutex      // Protects all fields
}

Buffered Channel (cap=3):
┌─────────────────────────────────────┐
│  buf: [elem1][elem2][empty]         │
│       ↑            ↑                │
│     recvx        sendx              │
│                                     │
│  recvq: [G1, G2]  (blocked readers) │
│  sendq: [G3]      (blocked writers) │
└─────────────────────────────────────┘
```

When an HTTP server handler sends a response through a channel:

```go
responseChan <- response
```

Internally:

1. Acquire channel's lock
2. If buffer has space, copy data to buffer
3. If receiver is waiting, directly copy to receiver's stack
4. Release lock
5. Wake up receiver goroutine if needed

---

## HTTP Client Internal Architecture {#http-client-architecture}

Now let's dive deep into what happens when you call `http.Get()` or use `http.Client`.

### The http.Client Structure

```go
// Simplified version of actual http.Client
type Client struct {
    Transport RoundTripper  // Does the actual work
    CheckRedirect func(req *Request, via []*Request) error
    Jar CookieJar          // Cookie management
    Timeout time.Duration   // Overall timeout
}
```

The `Transport` is the heart of the client. Let's examine it:

```go
type Transport struct {
    // Connection pooling
    idleConn     map[connectMethodKey][]*persistConn
    idleConnWait map[connectMethodKey]wantConnQueue
    idleLRU      connLRU
    
    // Limits
    MaxIdleConns          int
    MaxIdleConnsPerHost   int
    MaxConnsPerHost       int
    IdleConnTimeout       time.Duration
    
    // Timeouts
    ResponseHeaderTimeout time.Duration
    ExpectContinueTimeout time.Duration
    
    // TLS config
    TLSClientConfig *tls.Config
    
    // Dial function
    DialContext func(ctx context.Context, network, addr string) (net.Conn, error)
}
```

### Making an HTTP Request - Complete Flow

Let's trace what happens when you call:

```go
resp, err := http.Get("https://api.example.com/users")
```

**Step 1: Parse URL and Create Request**

```
URL Parsing:
https://api.example.com:443/users
  ↓        ↓               ↓     ↓
Scheme   Host            Port  Path

Creates http.Request:
type Request struct {
    Method string              // "GET"
    URL    *url.URL           // Parsed URL
    Proto  string             // "HTTP/1.1"
    Header Header             // Headers map
    Body   io.ReadCloser      // Request body (nil for GET)
    // ... many more fields
}
```

**Step 2: Look for Existing Connection in Pool**

```
Connection Pool Lookup:

Key = (scheme, host, port) = ("https", "api.example.com", "443")

idleConn map:
┌────────────────────────────────────────────────┐
│ Key: (https, api.example.com, 443)            │
│ Value: []*persistConn{                        │
│   conn1 (idle since 5s ago),                  │
│   conn2 (idle since 10s ago),                 │
│   conn3 (idle since 2s ago)                   │
│ }                                              │
└────────────────────────────────────────────────┘

Decision: Reuse conn3 (most recently used)
```

**Step 3: If No Connection, Establish New One**

```
Connection Establishment:

1. DNS Lookup:
   api.example.com ──DNS resolver──> 93.184.216.34
   
   Internal cache checked first:
   ┌─────────────────────────────────┐
   │ DNS Cache (in resolver)         │
   │ api.example.com -> 93.184.216.34│
   │ TTL: 300 seconds                │
   └─────────────────────────────────┘

2. TCP Connection:
   ┌──────────────────────────────────────┐
   │ net.Dial("tcp", "93.184.216.34:443")│
   └──────────────────────────────────────┘
   
   This calls system calls:
   socket() -> connect() -> (3-way handshake)

3. TLS Handshake (for HTTPS):
   Client Hello  ──────────────> Server
   <────────────── Server Hello, Certificate
   Key Exchange  ──────────────>
   <──────────────────────────── Finished
   Finished      ──────────────>
   
   Encrypted tunnel established!
```

**TLS Handshake Details** (This is expensive!):

```
TLS 1.3 Handshake Timeline:

0ms:    Client sends ClientHello
        - Supported cipher suites
        - TLS versions
        - Key share (for faster handshake)

50ms:   Server sends ServerHello
        - Chosen cipher suite
        - Certificate (2-4KB)
        - Key share
        - Finished (encrypted)

50ms:   Client verifies certificate
        - Check signature
        - Check expiration
        - Check domain name
        - Check certificate chain

50ms:   Client sends Finished (encrypted)

Total: 1.5 RTT for TLS 1.3 (vs 2 RTT for TLS 1.2)

Cost: CPU time for crypto operations
      + Network round trips
      ≈ 100ms for distant server
```

**Step 4: Send HTTP Request**

```
Request Serialization:

HTTP Request in Memory:
┌────────────────────────────────────────┐
│ Request struct fields                  │
│ - Method: "GET"                        │
│ - URL: "/users"                        │
│ - Headers: {Host, User-Agent, ...}    │
└────────────────────────────────────────┘
         │
         ↓ Serialize to bytes
         │
Wire Format (bytes):
┌────────────────────────────────────────┐
│ GET /users HTTP/1.1\r\n               │
│ Host: api.example.com\r\n             │
│ User-Agent: Go-http-client/1.1\r\n    │
│ Accept-Encoding: gzip\r\n             │
│ \r\n                                   │
└────────────────────────────────────────┘
         │
         ↓ Write to TLS connection
         │
TLS Encrypted:
┌────────────────────────────────────────┐
│ [Encrypted HTTP request]               │
│ (impossible to read without key)       │
└────────────────────────────────────────┘
         │
         ↓ Write to TCP connection
         │
TCP Segments:
┌────────────────────────────────────────┐
│ Segment 1: [encrypted data]            │
│ Segment 2: [encrypted data]            │
└────────────────────────────────────────┘
         │
         ↓ Send over network
```

Code that does this:

```go
// Inside Transport
func (pc *persistConn) writeRequest(req *Request) error {
    // Write request line
    fmt.Fprintf(pc.bw, "%s %s HTTP/1.1\r\n", req.Method, req.URL.RequestURI())
    
    // Write headers
    req.Header.Write(pc.bw)
    
    // Write blank line
    pc.bw.WriteString("\r\n")
    
    // Write body if present
    if req.Body != nil {
        io.Copy(pc.bw, req.Body)
    }
    
    // Flush buffered writer
    return pc.bw.Flush()
}
```

**Step 5: Read Response**

```
Response Reading Pipeline:

1. Read from TCP connection:
   ┌─────────────────────────────────────┐
   │ conn.Read(buffer)                   │
   │ - Blocks until data available       │
   │ - Goroutine parked by netpoller     │
   └─────────────────────────────────────┘

2. TLS decryption:
   ┌─────────────────────────────────────┐
   │ [Encrypted bytes]                   │
   │         ↓ TLS decrypt               │
   │ [Plain HTTP response]               │
   └─────────────────────────────────────┘

3. Parse status line:
   "HTTP/1.1 200 OK\r\n"
    ↓
   Proto: "HTTP/1.1"
   StatusCode: 200
   Status: "200 OK"

4. Parse headers:
   "Content-Type: application/json\r\n"
   "Content-Length: 1234\r\n"
   "\r\n"
    ↓
   Header map:
   {
     "Content-Type": ["application/json"],
     "Content-Length": ["1234"]
   }

5. Body handling:
   - If Content-Length: read exactly that many bytes
   - If Transfer-Encoding: chunked: read chunks
   - If neither: read until connection closes
```

**Step 6: Return Response to Caller**

```go
type Response struct {
    Status     string        // "200 OK"
    StatusCode int          // 200
    Proto      string       // "HTTP/1.1"
    Header     Header       // Headers
    Body       io.ReadCloser // Response body (important!)
    
    Request    *Request     // Original request
    TLS        *tls.ConnectionState
}
```

**Critical**: The `Body` is an `io.ReadCloser`. It's not read yet! It's backed by the TCP connection. When you read from it, you're reading from the network.

```go
resp, _ := http.Get(url)
// At this point, headers are parsed but body is NOT read!

defer resp.Body.Close() // MUST close to return connection to pool

body, _ := io.ReadAll(resp.Body)
// NOW the body is read from network and stored in memory
```

### Connection Pooling Internals

Connection pooling is crucial for performance. Let's see how it works:

```
Connection Lifecycle:

1. Request arrives:
   ┌──────────────────────────┐
   │ Need connection for      │
   │ api.example.com:443      │
   └──────────────────────────┘
            │
            ↓
   ┌──────────────────────────┐
   │ Check idle pool          │
   └──────────────────────────┘
            │
       ┌────┴────┐
       │Found?   │
       └────┬────┘
     Yes    │    No
       │    │    │
       ↓    │    ↓
   ┌────────┐  ┌─────────────────┐
   │ Reuse  │  │ Create new conn │
   │ conn   │  │ (Dial + TLS)    │
   └────────┘  └─────────────────┘
       │              │
       └──────┬───────┘
              ↓
   ┌──────────────────────────┐
   │ Use connection           │
   │ - Write request          │
   │ - Read response          │
   └──────────────────────────┘
              │
              ↓
   ┌──────────────────────────┐
   │ Response body closed?    │
   └──────────────────────────┘
       │              │
     Yes              No
       │              │
       ↓              ↓
   ┌────────┐    ┌─────────┐
   │Return  │    │ Close   │
   │to pool │    │ conn    │
   └────────┘    └─────────┘
```

**persistConn Structure**:

```go
type persistConn struct {
    // Connection
    conn     net.Conn
    tlsState *tls.ConnectionState
    
    // Buffered I/O
    br *bufio.Reader  // Buffered reader for efficiency
    bw *bufio.Writer  // Buffered writer
    
    // State
    reused       bool
    closed       bool
    broken       bool
    
    // Channels for coordination
    reqch        chan requestAndChan
    writech      chan writeRequest
    closech      chan struct{}
    
    // Timestamps
    idleAt       time.Time
    idleTimer    *time.Timer
}
```

**Connection Pool Management**:

```
Idle Connection Pool:

┌─────────────────────────────────────────────────────────┐
│  idleConn map                                           │
│                                                         │
│  Key: (https, api.example.com, 443)                    │
│  ┌───────────────────────────────────────────────────┐ │
│  │ conn1: idle 5s, last used: 10:30:00                │ │
│  │ conn2: idle 10s, last used: 10:29:55               │ │
│  │ conn3: idle 2s, last used: 10:30:03                │ │
│  └───────────────────────────────────────────────────┘ │
│                                                         │
│  Key: (https, api.other.com, 443)                     │
│  ┌───────────────────────────────────────────────────┐ │
│  │ conn4: idle 20s, last used: 10:29:45               │ │
│  └───────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────┘

Cleanup goroutine runs periodically:
- Removes connections idle > IdleConnTimeout (default 90s)
- Enforces MaxIdleConns limit
- Closes broken connections
```

**Why Connection Pooling Matters**:

```
Without pooling (each request):
DNS lookup:    50ms
TCP connect:   50ms
TLS handshake: 100ms
Request:       50ms
Total:         250ms per request

With pooling (reusing connection):
Request:       50ms
Total:         50ms per request

5x faster!
```

### Request Cancellation with Context

When you cancel a request with context, here's what happens:

```go
ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
defer cancel()

req, _ := http.NewRequestWithContext(ctx, "GET", url, nil)
resp, err := client.Do(req)
```

Internally:

```
Request Lifecycle with Context:

Goroutine 1 (Your code):
┌────────────────────────┐
│ Start request          │
│ with 5s timeout        │
└───────────┬────────────┘
            │
            ↓
┌────────────────────────┐
│ Transport.RoundTrip()  │
│ - Dials connection     │
│ - Sends request        │
│ - Waits for response   │
└───────────┬────────────┘
            │
            │  Goroutine 2 (Context timer):
            │  ┌──────────────────┐
            │  │ time.Sleep(5s)   │
            │  │ cancel context   │
            │  └────────┬─────────┘
            │           │
            ↓           ↓
    ┌───────────────────────────┐
    │ Context cancelled!        │
    │ - Close connection        │
    │ - Return error            │
    └───────────────────────────┘
```

Code that implements this:

```go
// Inside Transport
func (pc *persistConn) roundTrip(req *Request) (*Response, error) {
    // Watch for context cancellation
    select {
    case <-req.Context().Done():
        // Context cancelled, abort request
        pc.close(req.Context().Err())
        return nil, req.Context().Err()
    default:
    }
    
    // ... rest of request processing
}
```

## HTTP Server Internal Architecture {#http-server-architecture}

Now let's explore what happens inside an HTTP server. This is more complex than the client because a server handles many concurrent connections.

### Server Initialization

When you call:

```go
http.ListenAndServe(":8080", nil)
```

Here's the complete initialization flow:

```
Server Initialization Steps:

1. Create Server struct:
   ┌────────────────────────────────────┐
   │ type Server struct {               │
   │   Addr         string   // ":8080" │
   │   Handler      Handler  // nil     │
   │   ReadTimeout  Duration            │
   │   WriteTimeout Duration            │
   │   IdleTimeout  Duration            │
   │   // ... more fields               │
   │ }                                  │
   └────────────────────────────────────┘

2. Create TCP listener:
   System calls:
   fd = socket(AF_INET, SOCK_STREAM, 0)
   bind(fd, "0.0.0.0:8080")
   listen(fd, backlog=128)
   
   ┌────────────────────────────────────┐
   │ TCP Listener Socket                │
   │ - File descriptor: 3               │
   │ - Bound to: 0.0.0.0:8080          │
   │ - State: LISTENING                 │
   │ - Backlog: 128 (SYN queue)        │
   └────────────────────────────────────┘

3. Accept loop starts:
   for {
       conn, err := listener.Accept()
       go server.handleConnection(conn)
   }
```

**SYN Queue and Accept Queue**:

```
Connection Queuing in Kernel:

SYN Queue (Incomplete connections):
┌──────────────────────────────────┐
│ Client1 (SYN received)           │
│ Client2 (SYN received)           │
│ Client3 (SYN received)           │
└──────────────────────────────────┘
         ↓ SYN-ACK sent
         ↓ ACK received
Accept Queue (Complete connections):
┌──────────────────────────────────┐
│ Client4 (ready to accept)        │
│ Client5 (ready to accept)        │
│ Client6 (ready to accept)        │
└──────────────────────────────────┘
         ↓ accept() system call
         ↓
Application gets connection
```

When queues are full, new connections are rejected (SYN flood protection).

### Connection Acceptance

When a client connects:

```
Connection Acceptance Flow:

1. Kernel receives SYN packet:
   ┌────────────────────────────────┐
   │ NIC receives packet            │
   │ Kernel TCP stack processes it  │
   │ Adds to SYN queue              │
   │ Sends SYN-ACK                  │
   └────────────────────────────────┘

2. Kernel receives ACK:
   ┌────────────────────────────────┐
   │ Connection established         │
   │ Move to accept queue           │
   │ Wake up listener goroutine     │
   └────────────────────────────────┘

3. Application calls Accept():
   ┌────────────────────────────────┐
   │ accept() system call           │
   │ Returns new socket fd          │
   └────────────────────────────────┘
            ↓
   ┌────────────────────────────────┐
   │ Goroutine spawned:             │
   │ go server.handleConnection()   │
   └────────────────────────────────┘
```

Each new connection gets its own goroutine. This is the key to Go's scalability:

```
Traditional Thread-per-Connection:

┌──────────┐ ┌──────────┐ ┌──────────┐
│ Thread 1 │ │ Thread 2 │ │ Thread N │
│  8MB     │ │  8MB     │ │  8MB     │
│  stack   │ │  stack   │ │  stack   │
└──────────┘ └──────────┘ └──────────┘

1,000 connections = 8GB memory!
Context switch overhead = High


Go Goroutine-per-Connection:

┌─────────┐ ┌─────────┐ ┌─────────┐
│  G1     │ │  G2     │ │  GN     │
│  2KB    │ │  2KB    │ │  2KB    │
│  stack  │ │  stack  │ │  stack  │
└─────────┘ └─────────┘ └─────────┘

1,000 connections = 2MB memory!
Context switch overhead = Low
```

### Connection Handling

Let's trace a complete HTTP request through the server:

```
HTTP Request Processing Flow:

1. Read Request Line:
   ┌────────────────────────────────────┐
   │ conn.Read() blocks                 │
   │ Goroutine parked by netpoller      │
   └────────────────────────────────────┘
            ↓ Data arrives
   ┌────────────────────────────────────┐
   │ Netpoller wakes goroutine          │
   │ Read: "GET /api/users HTTP/1.1\r\n"│
   └────────────────────────────────────┘

2. Parse Request:
   ┌────────────────────────────────────┐
   │ bufio.Reader reads until \r\n\r\n │
   │                                    │
   │ Parse:                             │
   │ - Method: GET                      │
   │ - Path: /api/users                 │
   │ - Protocol: HTTP/1.1               │
   │ - Headers: Host, User-Agent, ...  │
   └────────────────────────────────────┘

3. Create Request object:
   ┌────────────────────────────────────┐
   │ req := &http.Request{              │
   │   Method: "GET",                   │
   │   URL: parseURL("/api/users"),     │
   │   Header: parseHeaders(),          │
   │   Body: conn,                      │
   │   RemoteAddr: "192.168.1.100",     │
   │ }                                  │
   └────────────────────────────────────┘

4. Route to handler:
   ┌────────────────────────────────────┐
   │ handler := mux.match("/api/users") │
   │ // Find registered handler         │
   └────────────────────────────────────┘

5. Call handler:
   ┌────────────────────────────────────┐
   │ handler.ServeHTTP(w, req)          │
   │ // Your application code runs here │
   └────────────────────────────────────┘

6. Write response:
   ┌────────────────────────────────────┐
   │ w.WriteHeader(200)                 │
   │ w.Write([]byte(`{"users":[...]}`)) │
   └────────────────────────────────────┘

7. Flush and close:
   ┌────────────────────────────────────┐
   │ Flush buffered writer              │
   │ Check for Connection: keep-alive   │
   │ If keep-alive: reuse connection    │
   │ If close: close connection         │
   └────────────────────────────────────┘
```

### The conn Structure

Each connection is represented by:

```go
type conn struct {
    // Network connection
    rwc net.Conn
    
    // Buffered I/O
    bufr *bufio.Reader
    bufw *bufio.Writer
    
    // Current request being served
    curReq atomic.Value
    
    // State
    mu           sync.Mutex
    hijacked     bool
    closeWriteAndWait bool
    
    // HTTP state
    lastMethod string
    
    // Connection state
    curState struct{ atomic uint64 }
}
```

### ResponseWriter Implementation

The `http.ResponseWriter` interface is implemented by:

```go
type response struct {
    conn          *conn
    req           *Request
    
    // Response building
    status        int
    statusWritten bool
    header        Header
    
    // Writing
    w             *bufio.Writer
    cw            chunkWriter
    
    // Content length
    contentLength int64
    written       int64
    
    // Buffers
    handlerHeader Header
}
```

When you write to `ResponseWriter`:

```go
func (w *response) Write(data []byte) (int, error) {
    // If status not written yet, write 200 OK
    if !w.statusWritten {
        w.WriteHeader(200)
    }
    
    // Update content length
    w.written += int64(len(data))
    
    // Write to buffered writer
    return w.w.Write(data)
}
```

**Buffered Writing**:

```
Response Writing Pipeline:

Your Handler:
┌─────────────────────────────────┐
│ w.Write([]byte("Hello"))        │
└──────────────┬──────────────────┘
               ↓
Buffered Writer (4KB buffer):
┌─────────────────────────────────┐
│ [Hello.....................]     │
│  5 bytes used, 4091 free        │
└──────────────┬──────────────────┘
               ↓ (on flush)
TCP Connection:
┌─────────────────────────────────┐
│ Send data over network          │
└─────────────────────────────────┘

Why buffering?
- Reduces system calls
- Better network efficiency
- Combines small writes
```

### Keep-Alive Connection Handling

HTTP/1.1 supports persistent connections (keep-alive):

```
Keep-Alive Connection Lifecycle:

Request 1:
┌──────────────────────────────────┐
│ Client sends: GET /users         │
│ Header: Connection: keep-alive   │
└──────────────────────────────────┘
         ↓
┌──────────────────────────────────┐
│ Server processes request         │
│ Sends response                   │
│ Connection stays open            │
└──────────────────────────────────┘
         ↓
Request 2 (same connection):
┌──────────────────────────────────┐
│ Client sends: GET /posts         │
│ No new TCP handshake needed!     │
└──────────────────────────────────┘
         ↓
┌──────────────────────────────────┐
│ Server processes request         │
│ Sends response                   │
│ Connection stays open            │
└──────────────────────────────────┘

Benefits:
- No TCP handshake overhead
- No TLS handshake overhead
- Lower latency
- Better performance
```

Server implementation:

```go
func (c *conn) serve(ctx context.Context) {
    for {
        // Read request
        req, err := c.readRequest(ctx)
        if err != nil {
            return // Close connection
        }
        
        // Serve request
        serverHandler{c.server}.ServeHTTP(w, req)
        
        // Check if connection should close
        if w.shouldCloseAfterReply() {
            return // Close connection
        }
        
        // Reset for next request on same connection
        c.setState(StateIdle)
    }
}
```

### Request Routing

The server uses a `ServeMux` to route requests:

```
ServeMux Structure:

type ServeMux struct {
    mu    sync.RWMutex
    m     map[string]muxEntry
    es    []muxEntry // sorted from longest to shortest
    hosts bool       // whether any patterns have hostnames
}

type muxEntry struct {
    h       Handler
    pattern string
}

Example routing table:
┌─────────────────────────────────────┐
│ Pattern          Handler            │
├─────────────────────────────────────┤
│ /api/users       UserHandler        │
│ /api/posts       PostHandler        │
│ /api/            APIHandler         │
│ /                DefaultHandler     │
└─────────────────────────────────────┘

Matching algorithm:
1. Check for exact match
2. Check for longest prefix match
3. Host-specific patterns take precedence
```

Request routing:

```go
func (mux *ServeMux) match(path string) (h Handler, pattern string) {
    // Check for exact match first
    v, ok := mux.m[path]
    if ok {
        return v.h, v.pattern
    }
    
    // Check for longest prefix match
    for _, e := range mux.es {
        if strings.HasPrefix(path, e.pattern) {
            return e.h, e.pattern
        }
    }
    
    return NotFoundHandler(), ""
}
```

### Connection States

Connections transition through states:

```
Connection State Machine:

StateNew ──────> StateActive ──────> StateIdle ──────> StateActive
  │                   │                   │
  │                   │                   ↓
  │                   │              StateHijacked
  │                   │
  │                   ↓
  └─────────────> StateClosed

StateNew:       Connection just accepted
StateActive:    Processing a request
StateIdle:      Between requests (keep-alive)
StateHijacked:  Taken over by handler (e.g., WebSocket)
StateClosed:    Connection closed
```

Transitions:

```go
const (
    StateNew = iota
    StateActive
    StateIdle
    StateHijacked
    StateClosed
)

func (c *conn) setState(state uint64) {
    atomic.StoreUint64(&c.curState.atomic, state)
    
    // Notify connection state hooks
    if hook := c.server.ConnState; hook != nil {
        hook(c.rwc, ConnState(state))
    }
}
```

---

## Memory Management {#memory-management}

Memory management is critical for HTTP servers handling thousands of requests. Let's understand how Go manages memory for HTTP operations.

### Memory Allocation for Requests

When a request arrives:

```
Memory Allocations:

1. Connection struct:
   ┌────────────────────────────────┐
   │ type conn struct { ... }       │
   │ Size: ~200 bytes               │
   └────────────────────────────────┘

2. Request struct:
   ┌────────────────────────────────┐
   │ type Request struct { ... }    │
   │ Size: ~500 bytes               │
   │ + URL struct: ~100 bytes       │
   │ + Header map: variable         │
   └────────────────────────────────┘

3. Response struct:
   ┌────────────────────────────────┐
   │ type response struct { ... }   │
   │ Size: ~300 bytes               │
   │ + Header map: variable         │
   └────────────────────────────────┘

4. Buffers:
   ┌────────────────────────────────┐
   │ Read buffer: 4KB               │
   │ Write buffer: 4KB              │
   └────────────────────────────────┘

Total per request: ~10KB (minimum)
```

### Buffer Pooling

Go uses sync.Pool to reuse buffers:

```go
var bufferPool = sync.Pool{
    New: func() interface{} {
        // Create 4KB buffer
        b := make([]byte, 4096)
        return &b
    },
}

// Get buffer from pool
func getBuffer() *[]byte {
    return bufferPool.Get().(*[]byte)
}

// Return buffer to pool
func putBuffer(buf *[]byte) {
    bufferPool.Put(buf)
}
```

**How sync.Pool works**:

```
sync.Pool Architecture:

Per-P (Processor) pools:
┌────────────┐  ┌────────────┐  ┌────────────┐
│   Pool 1   │  │   Pool 2   │  │   Pool 3   │
│ [buf][buf] │  │ [buf][buf] │  │ [buf][buf] │
└────────────┘  └────────────┘  └────────────┘
      │                │                │
      └────────────────┴────────────────┘
                      │
              Global victim cache
           (survives one GC cycle)

Operations:
- Get(): Try local pool → Try steal from others → New()
- Put(): Add to local pool
- GC: Clears all pools (except victim cache)

Benefits:
- Reduces allocations
- Reduces GC pressure
- Lock-free for common case
```

### Garbage Collection Impact

The Go garbage collector affects HTTP server performance:

```
GC Cycle Impact:

Normal Operation:
┌────────────────────────────────────┐
│ Request processing                 │
│ 1000 req/sec, 1ms latency         │
└────────────────────────────────────┘

During GC (Stop-The-World):
┌────────────────────────────────────┐
│ All goroutines paused              │
│ Mark phase: 1-10ms                 │
│ Request latency spike: +10ms       │
└────────────────────────────────────┘

After GC:
┌────────────────────────────────────┐
│ Resume normal operation            │
│ Memory freed                       │
└────────────────────────────────────┘
```

**GC Tuning**:

```bash
# Set GC target percentage
GOGC=100 # default (collect when heap grows 100%)
GOGC=200 # less frequent, more memory
GOGC=50  # more frequent, less memory
```

### Memory Profiling

Profile memory usage:

```go
import _ "net/http/pprof"

func main() {
    http.ListenAndServe(":8080", nil)
}
```

Then access:

- `/debug/pprof/heap` - Current heap allocations
- `/debug/pprof/allocs` - All allocations since start
- `/debug/pprof/goroutine` - Current goroutines

Heap profile visualization:

```
Heap Profile:
┌────────────────────────────────────────────┐
│ Function          Allocations   Memory     │
├────────────────────────────────────────────┤
│ handleRequest     1,000,000     100MB      │
│ parseHeaders      500,000       50MB       │
│ writeResponse     1,000,000     80MB       │
│ bufio.NewReader   100,000       400MB      │
└────────────────────────────────────────────┘

Analysis:
- bufio.NewReader is the largest consumer
- Opportunity: Pool bufio.Readers
```

---

## Connection Lifecycle {#connection-lifecycle}

Let's trace the complete lifecycle of an HTTP connection.

### Client Perspective

```
Complete HTTP Client Request Lifecycle:

T=0ms: Create Request
┌────────────────────────────────────┐
│ req := &Request{                   │
│   Method: "GET",                   │
│   URL: url,                        │
│ }                                  │
└────────────────────────────────────┘
Memory: +500 bytes

T=1ms: DNS Lookup
┌────────────────────────────────────┐
│ Resolve api.example.com            │
│ - Check cache                      │
│ - Query DNS server if needed       │
│ Result: 93.184.216.34              │
└────────────────────────────────────┘
Time: 0-100ms (cached vs uncached)

T=50ms: TCP Connect
┌────────────────────────────────────┐
│ socket() -> connect()              │
│ Three-way handshake                │
│ SYN -> SYN-ACK -> ACK              │
└────────────────────────────────────┘
Time: 1 RTT (e.g., 50ms)
Memory: +4KB (buffers)

T=100ms: TLS Handshake (if HTTPS)
┌────────────────────────────────────┐
│ ClientHello -> ServerHello         │
│ Certificate verification           │
│ Key exchange                       │
│ Finished                           │
└────────────────────────────────────┘
Time: 1-2 RTT (50-100ms)
CPU: Crypto operations

T=200ms: Send Request
┌────────────────────────────────────┐
│ Serialize request                  │
│ Write to socket                    │
│ GET /api HTTP/1.1\r\n...          │
└────────────────────────────────────┘
Time: 0.5 RTT (25ms)

T=225ms: Receive Response
┌────────────────────────────────────┐
│ Read from socket                   │
│ Parse status line                  │
│ Parse headers                      │
│ Body available for reading         │
└────────────────────────────────────┘
Time: 0.5 RTT (25ms)
Memory: +8KB (response buffers)

T=250ms: Process Response
┌────────────────────────────────────┐
│ io.ReadAll(resp.Body)              │
│ Parse JSON                         │
│ Close body                         │
└────────────────────────────────────┘
Time: Depends on body size

T=260ms: Connection Returned to Pool
┌────────────────────────────────────┐
│ Connection idle                    │
│ Available for reuse                │
│ Idle timeout: 90s                  │
└────────────────────────────────────┘

Total time: 260ms (first request)
Next request on same connection: ~75ms
(no DNS, TCP, TLS overhead)
```

### Server Perspective

```
Complete HTTP Server Request Lifecycle:

T=0ms: Accept Connection
┌────────────────────────────────────┐
│ listener.Accept()                  │
│ New socket fd: 42                  │
│ Client: 192.168.1.100:54321       │
└────────────────────────────────────┘
Memory: +200 bytes (conn struct)

T=1ms: Spawn Goroutine
┌────────────────────────────────────┐
│ go conn.serve()                    │
│ Goroutine stack: 2KB               │
└────────────────────────────────────┘
Memory: +2KB (goroutine stack)

T=2ms: Read Request
┌────────────────────────────────────┐
│ bufio.Reader.ReadLine()            │
│ Parse: "GET /api/users HTTP/1.1"  │
│ Read until \r\n\r\n                │
└────────────────────────────────────┘
Time: Network dependent
Memory: +4KB (read buffer)

T=50ms: Request Received
┌────────────────────────────────────┐
│ Create Request struct              │
│ Parse headers                      │
│ Initialize ResponseWriter          │
└────────────────────────────────────┘
Memory: +500 bytes (Request)
Memory: +300 bytes (response)

T=51ms: Route Request
┌────────────────────────────────────┐
│ mux.match("/api/users")            │
│ Find handler: UserHandler          │
└────────────────────────────────────┘
Time: <1ms (map lookup)

T=52ms: Call Handler
┌────────────────────────────────────┐
│ handler.ServeHTTP(w, req)          │
│ - Query database                   │
│ - Process data                     │
│ - Format response                  │
└────────────────────────────────────┘
Time: Depends on handler (e.g., 100ms)
Memory: Depends on handler

T=152ms: Write Response
┌────────────────────────────────────┐
│ w.WriteHeader(200)                 │
│ w.Write(jsonBytes)                 │
│ Flush buffers                      │
└────────────────────────────────────┘
Time: Network dependent (e.g., 25ms)

T=177ms: Cleanup
┌────────────────────────────────────┐
│ If keep-alive: wait for next req   │
│ If close: close connection         │
│ Release buffers to pool            │
└────────────────────────────────────┘
Memory: Freed (~7KB)

Total time: 177ms
Goroutine: Either destroyed or waits for next request
```

---

## Performance Optimization {#performance-optimization}

Understanding internals helps you optimize. Here are key optimization techniques:

### 1. Connection Pooling Configuration

```go
transport := &http.Transport{
    MaxIdleConns:        100,  // Total across all hosts
    MaxIdleConnsPerHost: 10,   // Per host
    MaxConnsPerHost:     100,  // Limit concurrent to host
    IdleConnTimeout:     90 * time.Second,
}

client := &http.Client{
    Transport: transport,
    Timeout:   30 * time.Second,
}
```

**Impact**:

- Reusing connections saves 100-200ms per request
- Too many idle connections wastes memory
- Too few limits concurrency

### 2. Buffer Size Tuning

```go
// Larger buffer for high-throughput
reader := bufio.NewReaderSize(conn, 64*1024) // 64KB

// Smaller for memory-constrained
reader := bufio.NewReaderSize(conn, 4*1024)  // 4KB
```

### 3. Timeout Configuration

```go
server := &http.Server{
    Addr: ":8080",
    
    // Time to read request headers
    ReadHeaderTimeout: 5 * time.Second,
    
    // Time to read entire request
    ReadTimeout: 10 * time.Second,
    
    // Time to write response
    WriteTimeout: 10 * time.Second,
    
    // Time between requests (keep-alive)
    IdleTimeout: 60 * time.Second,
}
```

### 4. Reduce Allocations

```go
// Bad: Allocates on every request
func handler(w http.ResponseWriter, r *http.Request) {
    data := make([]byte, 1024)
    // use data
}

// Good: Use sync.Pool
var dataPool = sync.Pool{
    New: func() interface{} {
        return make([]byte, 1024)
    },
}

func handler(w http.ResponseWriter, r *http.Request) {
    data := dataPool.Get().([]byte)
    defer dataPool.Put(data)
    // use data
}
```

### 5. Batch Processing

```go
// Bad: One DB query per request
func getUsers(w http.ResponseWriter, r *http.Request) {
    for _, id := range ids {
        user := db.GetUser(id) // N queries
        users = append(users, user)
    }
}

// Good: Batch query
func getUsers(w http.ResponseWriter, r *http.Request) {
    users := db.GetUsers(ids) // 1 query
}
```

### Performance Metrics

Understanding what to measure:

```
Key HTTP Server Metrics:

1. Request Rate:
   requests/second

2. Latency Distribution:
   p50: 10ms  (median)
   p95: 50ms  (95th percentile)
   p99: 100ms (99th percentile)

3. Error Rate:
   errors/requests (%)

4. Concurrent Connections:
   active goroutines

5. Memory Usage:
   heap size, GC frequency

6. CPU Usage:
   % CPU utilized

7. Network I/O:
   bytes sent/received per second
```

---

## Summary

We've journeyed from CPU and memory through the operating system, network stack, Go runtime, and finally to application code. Here's the complete picture:

**Hardware Level**: CPU executes instructions, memory stores data, NIC sends/receives packets

**OS Level**: Kernel manages resources, provides system calls (socket, accept, read, write), uses epoll/kqueue for efficient I/O

**Network Stack**: TCP provides reliable connections, HTTP is built on top

**Go Runtime**: Scheduler multiplexes goroutines on threads, netpoller makes blocking I/O efficient, GC manages memory

**HTTP Client**: Connection pooling, DNS caching, TLS session resumption optimize performance

**HTTP Server**: Goroutine-per-connection, efficient routing, keep-alive support

**Memory**: Buffer pooling, careful allocation, GC tuning

Understanding these internals helps you:

- Debug production issues
- Optimize performance
- Configure timeouts and limits appropriately
- Write efficient concurrent code
- Handle thousands of connections efficiently

The key insight: Go's networking is efficient because it combines simple, synchronous-looking code with asynchronous execution under the hood, all managed automatically by the runtime.