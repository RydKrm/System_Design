# The Complete Guide to Go Concurrency Patterns and Synchronization

## Table of Contents

1. [Introduction - Coordinating Concurrent Work](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#introduction)
2. [Worker Pools](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#worker-pools)
3. [WaitGroups](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#waitgroups)
4. [Rate Limiting](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#rate-limiting)
5. [Atomic Counters](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#atomic-counters)
6. [Mutexes](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#mutexes)
7. [Stateful Goroutines](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#stateful-goroutines)
8. [Pattern Comparison and Selection](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#pattern-comparison)
9. [Real-World Applications](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#real-world-applications)

---

## Introduction - Coordinating Concurrent Work {#introduction}

Imagine you're managing a large restaurant during the lunch rush. You have dozens of orders coming in simultaneously, limited kitchen staff, shared resources (ovens, prep stations, ingredients), and the need to track when orders are complete. How do you coordinate all this concurrent activity efficiently and safely?

This is exactly the challenge you face when writing concurrent Go programs. You have goroutines (your kitchen staff) that need to:

**Execute work concurrently**: Multiple tasks happening at once, but you can't create unlimited goroutines - you need to limit concurrency to match available resources (CPU cores, memory, external service capacity).

**Wait for completion**: Your main program needs to know when all concurrent work is done before proceeding - like a restaurant manager waiting for all lunch orders to be fulfilled before closing the kitchen.

**Control access rates**: You can't overwhelm external services - like a kitchen that can only handle so many orders per minute from delivery apps.

**Share state safely**: Multiple goroutines might need to update shared counters, access shared data structures, or modify shared state - this requires careful coordination to avoid corruption.

Go provides several patterns and primitives to handle these scenarios:

**Worker Pools**: A fixed number of goroutines processing an unlimited stream of work. Like having exactly N chefs in your kitchen, no matter how many orders arrive.

**WaitGroups**: Waiting for multiple goroutines to complete before proceeding. Like a manager tracking when all orders for a table are ready.

**Rate Limiting**: Controlling how frequently operations execute. Like throttling incoming orders to match kitchen capacity.

**Atomic Counters**: Safe increment/decrement of counters across goroutines. Like multiple servers safely updating an order counter.

**Mutexes**: Protecting critical sections of code from concurrent access. Like ensuring only one chef uses the oven control panel at a time.

**Stateful Goroutines**: Encapsulating state in a goroutine that processes requests sequentially. Like a dedicated chef managing the salad station.

Each pattern solves specific coordination problems. Understanding when and how to use each one is essential for writing correct, efficient concurrent programs. Let's explore each in depth.

---

## Worker Pools {#worker-pools}

The worker pool pattern is one of the most important and commonly used concurrency patterns in Go. It solves a fundamental problem: how do you process many tasks concurrently without creating an unbounded number of goroutines?

### The Problem: Unbounded Concurrency

The naive approach to concurrent task processing is to spawn a goroutine for each task:

```go
for task := range tasks {
    go processTask(task)  // Creates one goroutine per task
}
```

This seems simple, but it has serious problems:

**Resource Exhaustion**: If you have 1 million tasks, you create 1 million goroutines. Each goroutine consumes memory (at least 2KB for the stack, plus whatever it allocates). 1 million goroutines = at least 2GB of memory, plus the memory they allocate. On systems with limited memory, this crashes your program.

**Scheduler Overhead**: The Go scheduler needs to manage all these goroutines. With millions of goroutines, even with Go's efficient scheduler, there's significant overhead switching between them.

**External Resource Limits**: If your tasks access external resources (database connections, API calls, file handles), you might overwhelm those resources. Opening 1 million database connections simultaneously will crash your database server.

**CPU Thrashing**: If tasks are CPU-bound, creating more goroutines than CPU cores just causes context switching overhead without actual parallelism. With 8 CPU cores and 1 million CPU-bound goroutines, you waste time switching instead of computing.

### The Solution: Worker Pool

A worker pool maintains a fixed number of worker goroutines. Tasks are submitted to a queue (channel), and workers pull tasks from the queue and process them. The number of workers is bounded, typically set based on available resources.

**Conceptual Model**: Think of an assembly line factory. You have a conveyor belt (task queue) with items that need processing. You have exactly N workers (fixed number) standing along the belt. Each worker takes an item, processes it, and then takes the next item. No matter how many items arrive on the belt, you only ever have N workers processing them.

### Basic Worker Pool Implementation

Let's build a worker pool from the ground up:

```go
type Task struct {
    ID   int
    Data string
}

type WorkerPool struct {
    numWorkers int
    tasks      chan Task
    results    chan Result
}

func NewWorkerPool(numWorkers int) *WorkerPool {
    return &WorkerPool{
        numWorkers: numWorkers,
        tasks:      make(chan Task, 100),    // Buffered for efficiency
        results:    make(chan Result, 100),
    }
}

func (wp *WorkerPool) Start() {
    for i := 0; i < wp.numWorkers; i++ {
        go wp.worker(i)
    }
}

func (wp *WorkerPool) worker(id int) {
    for task := range wp.tasks {
        // Process task
        result := processTask(task)
        wp.results <- result
    }
}

func (wp *WorkerPool) Submit(task Task) {
    wp.tasks <- task
}

func (wp *WorkerPool) Results() <-chan Result {
    return wp.results
}
```

**How It Works**:

**Initialization**: You create a worker pool specifying how many workers (e.g., `NewWorkerPool(10)` for 10 workers).

**Starting Workers**: When you call `Start()`, it spawns exactly `numWorkers` goroutines. Each worker runs the `worker()` function.

**Worker Loop**: Each worker runs an infinite loop receiving from the `tasks` channel. When a task arrives, the worker processes it and sends the result to the `results` channel. When the `tasks` channel is closed and empty, the worker's loop terminates.

**Task Submission**: You submit tasks via `Submit()`, which sends them to the `tasks` channel. If the channel's buffer is full, `Submit()` blocks until space is available (providing backpressure).

**Result Collection**: Results are sent to the `results` channel, which you can receive from.

### Why Worker Pools Work

**Bounded Resource Usage**: With N workers, you never have more than N goroutines actively processing tasks. Memory usage is predictable: N × (stack + allocations per task).

**Efficient CPU Utilization**: For CPU-bound tasks, set N = number of CPU cores. You get maximum parallelism without thrashing. For I/O-bound tasks, set N higher (workers spend most time blocked on I/O, so having more doesn't hurt).

**External Resource Protection**: If each task opens a database connection, and you have 10 workers, you never open more than 10 database connections simultaneously. The database won't be overwhelmed.

**Automatic Load Balancing**: Workers automatically balance load. A fast worker that finishes quickly immediately takes the next task. A slow worker that's stuck on a difficult task holds only one slot. No manual load balancing needed.

### Sizing the Worker Pool

Choosing the right number of workers is critical:

**For CPU-Bound Tasks**: Set workers = number of CPU cores (or runtime.NumCPU()). More workers than cores just causes context switching overhead. If you have 8 cores and 8 workers, all 8 can run in parallel. With 16 workers on 8 cores, half are idle at any moment.

**For I/O-Bound Tasks**: Set workers much higher than CPU cores. If tasks spend 90% of time waiting for I/O (network, disk, database), you can have hundreds or thousands of workers. While one worker waits for I/O, others use the CPU.

**For External Service Limits**: Set workers based on external rate limits. If an API allows 100 concurrent requests, use 100 workers. More workers would just result in rejected requests.

**For Memory Constraints**: Consider per-worker memory usage. If each worker needs 10MB, and you have 1GB available, you can afford 100 workers maximum.

**General Rule**: Start conservative (e.g., NumCPU for CPU-bound, 10× NumCPU for I/O-bound). Measure performance. Increase if you're not saturating resources. Decrease if you're overwhelmed.

### Advanced Worker Pool with Graceful Shutdown

A production-ready worker pool needs graceful shutdown:

```go
type WorkerPool struct {
    numWorkers int
    tasks      chan Task
    results    chan Result
    wg         sync.WaitGroup
}

func (wp *WorkerPool) Start() {
    for i := 0; i < wp.numWorkers; i++ {
        wp.wg.Add(1)
        go func(workerID int) {
            defer wp.wg.Done()
            wp.worker(workerID)
        }(i)
    }
}

func (wp *WorkerPool) worker(id int) {
    for task := range wp.tasks {
        result := processTask(task)
        wp.results <- result
    }
}

func (wp *WorkerPool) Shutdown() {
    close(wp.tasks)  // Signal workers to stop
    wp.wg.Wait()     // Wait for all workers to finish
    close(wp.results)  // Close results channel
}
```

**Shutdown Process**:

1. Close the `tasks` channel - this signals all workers that no more tasks are coming
2. Workers finish processing their current task
3. Workers see the closed channel and exit their loop
4. `wg.Wait()` blocks until all workers have exited
5. Close the `results` channel to signal to result consumers that no more results are coming

This ensures no tasks are lost and all in-progress work completes.

### Real-World Example: Image Processing Service

Imagine a service that processes uploaded images (resize, apply filters, generate thumbnails). Each image processing is CPU-intensive and takes 100-500ms.

**Without Worker Pool**:

```go
func handleUpload(images []Image) {
    for _, img := range images {
        go processImage(img)  // One goroutine per image
    }
}
```

If 10,000 images are uploaded, 10,000 goroutines are created. Each image processing loads the image into memory (several MB). 10,000 images = tens of GB of memory. System thrashes or crashes.

**With Worker Pool**:

```go
pool := NewWorkerPool(runtime.NumCPU())  // 8 workers on 8-core machine
pool.Start()

for _, img := range images {
    pool.Submit(img)
}

pool.Shutdown()
```

Only 8 images are processed simultaneously. Peak memory usage: 8 × (image size + processing overhead) - probably under 100MB. All 10,000 images are still processed, just in batches of 8 instead of all at once. Total time might be higher, but the system remains stable.

### Worker Pool Variations

**Fixed vs Dynamic Workers**: The basic pattern uses a fixed number of workers. An advanced variation adjusts the number of workers based on load (adds workers when queue is backing up, removes workers when idle). This is more complex but can adapt to varying workloads.

**Priority Queue**: Instead of a simple FIFO queue, use a priority queue. High-priority tasks get processed before low-priority ones. Workers always take the highest-priority available task.

**Worker-Specific State**: Sometimes workers need setup/teardown (database connection, file handle). Create workers with their own state:

```go
func (wp *WorkerPool) worker(id int) {
    conn := openDatabaseConnection()  // Each worker has its own connection
    defer conn.Close()
    
    for task := range wp.tasks {
        result := processWithConnection(conn, task)
        wp.results <- result
    }
}
```

Now each worker maintains a database connection for its entire lifetime, amortizing connection overhead.

---

## WaitGroups {#waitgroups}

WaitGroups are Go's primitive for waiting for multiple goroutines to complete. They're like a countdown latch - you add to the count each time you start a goroutine, and each goroutine decrements the count when it finishes. When the count reaches zero, all goroutines have completed.

### The Problem: Waiting for Multiple Goroutines

You spawn multiple goroutines to do work in parallel. Your main goroutine needs to wait for all of them to finish before proceeding.

**Naive Approach (Doesn't Work)**:

```go
done := false

go func() {
    // Do work
    done = true
}()

for !done {
    time.Sleep(10 * time.Millisecond)  // Busy-wait
}
```

This has multiple problems: race condition on `done`, inefficient busy-waiting, and doesn't scale to multiple goroutines.

**Channel Approach (Works but Verbose)**:

```go
done := make(chan bool, 3)

go func() { /* work */ done <- true }()
go func() { /* work */ done <- true }()
go func() { /* work */ done <- true }()

<-done
<-done
<-done
```

This works, but you need to manually count receives. With 100 goroutines, you need 100 receives. Error-prone.

### The Solution: sync.WaitGroup

A WaitGroup is designed specifically for this pattern:

```go
var wg sync.WaitGroup

wg.Add(3)  // We're starting 3 goroutines

go func() {
    defer wg.Done()  // Decrement counter when done
    // Do work
}()

go func() {
    defer wg.Done()
    // Do work
}()

go func() {
    defer wg.Done()
    // Do work
}()

wg.Wait()  // Block until counter reaches zero
```

**How It Works**:

**Add(n)**: Increments the internal counter by n. Call this before starting goroutines. `wg.Add(3)` means "I'm about to start 3 goroutines."

**Done()**: Decrements the counter by 1. Each goroutine calls this when it finishes. It's equivalent to `wg.Add(-1)`.

**Wait()**: Blocks until the counter reaches zero. When all goroutines have called `Done()`, the counter is zero, and `Wait()` returns.

### Internal Mechanics

A WaitGroup maintains an internal counter and uses a semaphore for blocking. When you call `Wait()`, the current goroutine blocks on the semaphore. When the counter reaches zero (via `Done()` calls), all waiting goroutines are woken up.

The counter is protected by atomic operations, making WaitGroup safe for concurrent use from multiple goroutines without external locking.

### Best Practices

**Always Use Defer for Done()**:

```go
go func() {
    defer wg.Done()  // ALWAYS use defer
    
    // Do work - if this panics, Done() still gets called
}()
```

Using `defer` ensures `Done()` is called even if the goroutine panics. Without defer, a panic would mean `Done()` never executes, and `Wait()` blocks forever (deadlock).

**Add Before Launching Goroutines**:

```go
// Good
wg.Add(1)
go func() {
    defer wg.Done()
    // work
}()

// Bad - race condition!
go func() {
    defer wg.Done()
    // work
}()
wg.Add(1)  // Might execute after goroutine starts and calls Done()
```

Always call `Add()` before `go func()`. Otherwise, there's a race: the goroutine might call `Done()` before you've called `Add()`, causing the counter to go negative (panic).

**Add in Batches for Performance**:

```go
// Better
wg.Add(100)  // Single Add for all goroutines
for i := 0; i < 100; i++ {
    go func() {
        defer wg.Done()
        // work
    }()
}

// Works but less efficient
for i := 0; i < 100; i++ {
    wg.Add(1)  // 100 separate Adds
    go func() {
        defer wg.Done()
        // work
    }()
}
```

`Add()` has some overhead (atomic operation). Adding in bulk is more efficient than many small adds.

**Don't Reuse WaitGroups Incorrectly**:

A WaitGroup can be reused after `Wait()` returns (counter is back to zero), but be careful:

```go
// Good - wait before next batch
wg.Add(10)
// Launch 10 goroutines
wg.Wait()

wg.Add(10)  // Can reuse now
// Launch next 10 goroutines
wg.Wait()

// Bad - don't add while Wait() is blocked elsewhere
wg.Add(10)
go func() {
    // These goroutines finish quickly
    defer wg.Done()
}()

go func() {
    wg.Wait()  // Blocked here
}()

wg.Add(5)  // BAD! Adding while Wait() is potentially blocked
```

Only add to a WaitGroup when no goroutine is calling `Wait()` on it (or when all previous adds have been matched by `Done()` calls).

### Common Patterns

**Pattern 1: Parallel Task Execution**

Execute multiple independent tasks in parallel, wait for all to complete:

```go
func processBatch(items []Item) {
    var wg sync.WaitGroup
    
    for _, item := range items {
        wg.Add(1)
        go func(i Item) {
            defer wg.Done()
            process(i)
        }(item)
    }
    
    wg.Wait()
    fmt.Println("All items processed")
}
```

All items are processed in parallel (up to the number of goroutines the scheduler can run). The function returns only when all processing is complete.

**Pattern 2: Fan-Out, Collect Results**

Launch multiple goroutines to compute results in parallel, collect all results:

```go
func computeResults(inputs []Input) []Result {
    results := make([]Result, len(inputs))
    var wg sync.WaitGroup
    
    for i, input := range inputs {
        wg.Add(1)
        go func(idx int, in Input) {
            defer wg.Done()
            results[idx] = compute(in)
        }(i, input)
    }
    
    wg.Wait()
    return results
}
```

Each goroutine computes a result and writes it to a specific index in the results slice. Since each writes to a different index, there's no race condition. After `Wait()`, all results are ready.

**Pattern 3: Pipeline Stages**

Multiple stages of a pipeline, wait for each stage to complete:

```go
func pipeline(data []int) {
    var wg sync.WaitGroup
    
    // Stage 1
    stage1Out := make(chan int, len(data))
    wg.Add(1)
    go func() {
        defer wg.Done()
        defer close(stage1Out)
        for _, v := range data {
            stage1Out <- stage1Process(v)
        }
    }()
    
    // Stage 2
    stage2Out := make(chan int, len(data))
    wg.Add(1)
    go func() {
        defer wg.Done()
        defer close(stage2Out)
        for v := range stage1Out {
            stage2Out <- stage2Process(v)
        }
    }()
    
    // Collect results
    wg.Add(1)
    go func() {
        defer wg.Done()
        for v := range stage2Out {
            handleResult(v)
        }
    }()
    
    wg.Wait()  // Wait for entire pipeline
}
```

Each stage runs in its own goroutine. Stages communicate via channels. The main goroutine waits for all stages to complete.

### WaitGroup vs. Channels

When should you use WaitGroup vs. channels for synchronization?

**Use WaitGroup When**:

- You just need to wait for completion (no data to transfer)
- Multiple goroutines doing independent work
- Simple "fire and wait" pattern

**Use Channels When**:

- You need to transfer data between goroutines
- You need more complex coordination (select, timeouts)
- You need cancellation or context propagation

**Use Both Together**: Often, you use channels for data transfer and WaitGroup for completion tracking:

```go
results := make(chan Result, 100)
var wg sync.WaitGroup

for i := 0; i < 10; i++ {
    wg.Add(1)
    go func() {
        defer wg.Done()
        result := doWork()
        results <- result
    }()
}

go func() {
    wg.Wait()
    close(results)  // Close results after all workers done
}()

for result := range results {
    handle(result)
}
```

Workers send results to a channel. A separate goroutine waits for all workers (WaitGroup) and then closes the results channel, signaling to the consumer that no more results are coming.

## Rate Limiting {#rate-limiting}

Rate limiting controls how frequently operations execute. It's essential for protecting services from being overwhelmed and for respecting external service quotas.

### The Problem: Uncontrolled Request Rates

Imagine your application calls an external API. Without rate limiting:

**Scenario 1: Overwhelming External Service**: You make 10,000 API calls in 1 second. The external service has a limit of 100 requests/second. It rejects 9,900 requests with "429 Too Many Requests" errors. Your application fails because it exceeded the rate limit.

**Scenario 2: Overwhelming Your Own Service**: You process incoming requests as fast as they arrive. During a traffic spike, 50,000 requests arrive per second. Your service can handle 1,000 requests/second. Memory fills with queued requests, latency skyrockets, and eventually the service crashes from out-of-memory errors.

**Scenario 3: Inefficient Resource Usage**: You make database queries as fast as possible. The database becomes a bottleneck, with connection pools exhausted and queries queuing. Meanwhile, your application goroutines are all blocked waiting for the database. CPU sits idle while everything waits.

Rate limiting solves these by controlling the rate at which operations execute.

### Core Concept: Token Bucket Algorithm

The most common rate limiting algorithm is the token bucket:

**Conceptual Model**: Imagine a bucket that holds tokens. Tokens are added to the bucket at a fixed rate (e.g., 10 tokens per second). The bucket has a maximum capacity (e.g., 100 tokens). To perform an operation, you must take a token from the bucket. If the bucket is empty, you wait until a token is added.

**Properties**:

- **Average Rate**: Tokens are added at a fixed rate, enforcing an average request rate
- **Burst Capacity**: The bucket's capacity allows bursts (if the bucket is full, you can take many tokens quickly)
- **Fairness**: First-come, first-served - whoever asks for a token first gets it first

### Implementation 1: Channel-Based Rate Limiter

The simplest rate limiter uses a buffered channel that receives tokens at a fixed rate:

```go
func rateLimiter(requestsPerSecond int) <-chan time.Time {
    interval := time.Second / time.Duration(requestsPerSecond)
    limiter := make(chan time.Time, requestsPerSecond)
    
    go func() {
        ticker := time.NewTicker(interval)
        defer ticker.Stop()
        
        for t := range ticker.C {
            select {
            case limiter <- t:
            default:
                // Bucket full, drop this token
            }
        }
    }()
    
    return limiter
}

// Usage
limiter := rateLimiter(100)  // 100 requests per second

for i := 0; i < 1000; i++ {
    <-limiter  // Wait for token
    
    go func() {
        makeAPICall()  // Now allowed to proceed
    }()
}
```

**How It Works**:

**Token Generation**: A ticker fires every `interval` (10ms for 100 req/sec). Each tick represents a token.

**Token Storage**: Tokens are sent to a buffered channel (the bucket). The buffer capacity is the burst limit.

**Token Consumption**: To make a request, receive from the channel (`<-limiter`). If a token is available, you proceed immediately. If the channel is empty, you block until the next token arrives.

**Burst Handling**: If no requests arrive for a while, the channel fills with tokens (up to the buffer capacity). The next batch of requests can all proceed immediately (burst), then requests throttle to the rate.

**Example Timeline**:

```
Rate: 10 requests/second (1 token every 100ms)
Bucket capacity: 10 tokens

T=0ms:    Start, bucket empty
T=0-1000ms: No requests, bucket fills
          [token] every 100ms
T=1000ms: Bucket full (10 tokens)

T=1000ms: 15 requests arrive simultaneously
          - Requests 1-10: Take tokens, proceed immediately (burst)
          - Requests 11-15: Block (bucket empty)
          
T=1100ms: 1 token added
          - Request 11 unblocks, proceeds
          
T=1200ms: 1 token added
          - Request 12 unblocks, proceeds
          
...and so on (requests 13-15 proceed at 100ms intervals)
```

### Implementation 2: golang.org/x/time/rate

The standard community package `golang.org/x/time/rate` provides a robust rate limiter:

```go
import "golang.org/x/time/rate"

// Create limiter: 100 requests per second, burst of 10
limiter := rate.NewLimiter(rate.Limit(100), 10)

// Wait for permission (blocks if necessary)
err := limiter.Wait(context.Background())
if err != nil {
    // Context cancelled
    return
}
makeAPICall()

// Or check without blocking
if limiter.Allow() {
    makeAPICall()
} else {
    // Rate limit exceeded, handle gracefully
}
```

**Features**:

**Wait(ctx)**: Blocks until a token is available or context is cancelled. Respects context deadlines and cancellation.

**Allow()**: Non-blocking. Returns true if a token is available, false otherwise. Use when you want to try an operation but skip it if rate limited.

**Reserve()**: Reserves a token for future use, returning when it will be available. Advanced usage for scheduling.

**Dynamic Adjustment**: You can change the rate limit at runtime with `SetLimit()`.

### Rate Limiting Strategies

**Strategy 1: Fixed Window**

Count requests in fixed time windows (e.g., per second, per minute). Allow up to N requests per window.

```go
type FixedWindowLimiter struct {
    limit       int
    window      time.Duration
    count       int
    windowStart time.Time
    mu          sync.Mutex
}

func (f *FixedWindowLimiter) Allow() bool {
    f.mu.Lock()
    defer f.mu.Unlock()
    
    now := time.Now()
    if now.Sub(f.windowStart) >= f.window {
        // New window
        f.count = 0
        f.windowStart = now
    }
    
    if f.count < f.limit {
        f.count++
        return true
    }
    
    return false
}
```

**Problem**: Burst at window boundaries. If limit is 100/second:

- At T=0.9s: Make 100 requests (allowed)
- At T=1.0s: Window resets
- At T=1.0s: Make 100 more requests (allowed)
- Total: 200 requests in 0.1 seconds!

**Strategy 2: Sliding Window**

Instead of fixed windows, track requests in a sliding time window. For "100 requests per second," count requests in the last 1 second (sliding).

```go
type SlidingWindowLimiter struct {
    limit    int
    window   time.Duration
    requests []time.Time
    mu       sync.Mutex
}

func (s *SlidingWindowLimiter) Allow() bool {
    s.mu.Lock()
    defer s.mu.Unlock()
    
    now := time.Now()
    cutoff := now.Add(-s.window)
    
    // Remove old requests outside window
    for len(s.requests) > 0 && s.requests[0].Before(cutoff) {
        s.requests = s.requests[1:]
    }
    
    if len(s.requests) < s.limit {
        s.requests = append(s.requests, now)
        return true
    }
    
    return false
}
```

**Advantage**: More accurate, no burst at boundaries.

**Disadvantage**: More memory (stores all request times in window).

**Strategy 3: Token Bucket** (Described Earlier)

Balance between simplicity and accuracy. Allows controlled bursts, enforces average rate.

**Strategy 4: Leaky Bucket**

Like token bucket, but processes requests at a fixed rate. Excess requests are queued or dropped. Ensures output rate is perfectly smooth (no bursts).

```go
type LeakyBucket struct {
    rate     time.Duration  // Time between requests
    queue    chan struct{}  // Request queue
}

func NewLeakyBucket(requestsPerSecond int, queueSize int) *LeakyBucket {
    lb := &LeakyBucket{
        rate:  time.Second / time.Duration(requestsPerSecond),
        queue: make(chan struct{}, queueSize),
    }
    
    go lb.process()
    return lb
}

func (lb *LeakyBucket) process() {
    ticker := time.NewTicker(lb.rate)
    defer ticker.Stop()
    
    for range ticker.C {
        select {
        case <-lb.queue:
            // Process one request
        default:
            // Queue empty, do nothing
        }
    }
}

func (lb *LeakyBucket) Allow() bool {
    select {
    case lb.queue <- struct{}{}:
        return true  // Queued successfully
    default:
        return false  // Queue full
    }
}
```

### Real-World Example: API Client with Rate Limiting

You're building a client for a third-party API that allows 1000 requests per hour (about 0.28 requests/second, or 1 request every 3.6 seconds).

**Without Rate Limiting**:

```go
for _, user := range users {
    data := apiClient.GetUserData(user)
    process(data)
}
```

If you have 10,000 users, you make 10,000 API calls as fast as possible. After 1000 calls, the API returns 429 errors, and the remaining 9,000 calls fail.

**With Rate Limiting**:

```go
limiter := rate.NewLimiter(rate.Every(3600*time.Millisecond), 1)  // 1 per 3.6s

for _, user := range users {
    limiter.Wait(context.Background())  // Wait for rate limit
    data := apiClient.GetUserData(user)
    process(data)
}
```

Now each API call waits for rate limit permission. The loop takes 10,000 × 3.6s = 10 hours to complete, but all 10,000 calls succeed. The API's rate limit is respected.

**With Burst Allowance**:

```go
// Allow bursts of 10 (for retries or initial batch)
limiter := rate.NewLimiter(rate.Every(3600*time.Millisecond), 10)
```

You can make 10 calls immediately (burst), then throttle to 1 per 3.6s. This is useful if the API allows short bursts but enforces an average rate.

### Rate Limiting for Internal Services

Rate limiting isn't just for external APIs. It protects your own services too.

**Example: Database Query Rate Limiting**

Your application makes database queries. Under load, too many concurrent queries overwhelm the database. Rate limit queries to a safe level:

```go
var queryLimiter = rate.NewLimiter(rate.Limit(100), 10)  // 100 queries/sec

func queryDatabase(query string) (Result, error) {
    if err := queryLimiter.Wait(context.Background()); err != nil {
        return nil, err
    }
    
    return db.Query(query)
}
```

This ensures your database never receives more than 100 queries per second, preventing overload.

### Per-User Rate Limiting

Often you want to rate limit per user, not globally:

```go
type PerUserLimiter struct {
    limiters map[string]*rate.Limiter
    mu       sync.Mutex
    rate     rate.Limit
    burst    int
}

func NewPerUserLimiter(r rate.Limit, burst int) *PerUserLimiter {
    return &PerUserLimiter{
        limiters: make(map[string]*rate.Limiter),
        rate:     r,
        burst:    burst,
    }
}

func (p *PerUserLimiter) getLimiter(userID string) *rate.Limiter {
    p.mu.Lock()
    defer p.mu.Unlock()
    
    limiter, exists := p.limiters[userID]
    if !exists {
        limiter = rate.NewLimiter(p.rate, p.burst)
        p.limiters[userID] = limiter
    }
    
    return limiter
}

func (p *PerUserLimiter) Allow(userID string) bool {
    limiter := p.getLimiter(userID)
    return limiter.Allow()
}
```

**Usage**:

```go
perUserLimiter := NewPerUserLimiter(rate.Limit(10), 5)  // 10 req/sec per user

func handleRequest(userID string) {
    if !perUserLimiter.Allow(userID) {
        http.Error(w, "Rate limit exceeded", 429)
        return
    }
    
    // Process request
}
```

Each user has their own rate limit. One user can't exhaust the quota for others.

---

## Atomic Counters {#atomic-counters}

Atomic counters solve a simple but critical problem: multiple goroutines need to increment or decrement a shared counter safely.

### The Problem: Race Conditions on Counters

Without synchronization, incrementing a shared counter is not safe:

```go
var counter int

func increment() {
    counter++  // NOT SAFE with multiple goroutines!
}

// Multiple goroutines
for i := 0; i < 1000; i++ {
    go increment()
}
```

**Why It's Unsafe**: The operation `counter++` is not atomic. It's actually three operations:

1. Read the current value of `counter` from memory
2. Add 1 to the value
3. Write the new value back to memory

If two goroutines execute these steps concurrently:

```
Timeline:
Goroutine A                Goroutine B
───────────────────────────────────────────
Read counter (0)           
Add 1 → 1                  Read counter (0)
                           Add 1 → 1
Write 1                    Write 1

Result: counter = 1, but we incremented twice!
Expected: counter = 2
```

Both goroutines read 0, both compute 1, both write 1. One increment is lost. This is a **race condition**.

With 1000 goroutines incrementing, you might expect counter = 1000, but you get something less (maybe 600, maybe 850, non-deterministic). Worse, on different CPUs or different runs, you get different results.

### Solutions Comparison

**Solution 1: Mutex** (Locking)

```go
var (
    counter int
    mu      sync.Mutex
)

func increment() {
    mu.Lock()
    counter++
    mu.Unlock()
}
```

This works. The mutex ensures only one goroutine accesses `counter` at a time. But it's overkill for a simple counter. Mutexes are heavy - they involve OS-level synchronization, and lock contention under high concurrency can be a bottleneck.

**Solution 2: Atomic Operations** (Lock-Free)

```go
var counter int64

func increment() {
    atomic.AddInt64(&counter, 1)
}
```

This is faster and simpler. Atomic operations use CPU-level instructions (like compare-and-swap) that guarantee atomicity without locks. Multiple goroutines can increment atomically without blocking each other (on modern CPUs with multiple cores, multiple atomic operations can happen simultaneously on different cache lines).

### The sync/atomic Package

Go's `sync/atomic` package provides lock-free atomic operations:

**For Integer Types**:

```go
import "sync/atomic"

var counter int64

// Add
atomic.AddInt64(&counter, 1)   // Increment
atomic.AddInt64(&counter, -1)  // Decrement
atomic.AddInt64(&counter, 5)   // Add 5

// Load (read)
value := atomic.LoadInt64(&counter)

// Store (write)
atomic.StoreInt64(&counter, 100)

// Compare-and-Swap (CAS)
swapped := atomic.CompareAndSwapInt64(&counter, 100, 200)
// If counter == 100, set it to 200 and return true
// If counter != 100, do nothing and return false
```

**Supported Types**: `int32`, `int64`, `uint32`, `uint64`, `uintptr`, `unsafe.Pointer`

**Not Supported**: `int`, `uint` (size varies by platform, use `int64`/`uint64`)

### Why Atomic Operations Are Fast

Atomic operations are implemented using CPU instructions like `LOCK` prefix on x86 or `LDREX`/`STREX` on ARM. These instructions guarantee that the read-modify-write sequence happens atomically - no other CPU can access that memory location during the operation.

**Key Advantages**:

- **No OS involvement**: Unlike mutexes (which might involve OS scheduler), atomic operations are pure CPU instructions
- **No blocking**: Goroutines don't block. They execute the atomic instruction and continue.
- **Low contention**: On modern CPUs, atomic operations on different memory locations can happen in parallel

**Performance**: Atomic operations are typically 10-100x faster than mutex operations.

### When to Use Atomic Operations

**Use Atomics For**:

- Simple counters (request counts, error counts, metrics)
- Flags (boolean state shared across goroutines)
- Reference counting (tracking how many goroutines reference an object)
- Lock-free data structures (advanced)

**Use Mutexes For**:

- Protecting multiple related variables (must be updated together)
- Complex operations (multiple reads/writes in a critical section)
- When you need to do more than just read/write (e.g., conditional logic)

**Example of When Mutex is Needed**:

```go
// Bad: Multiple atomic operations
atomic.AddInt64(&balance, amount)
atomic.AddInt64(&transactionCount, 1)
// NOT ATOMIC TOGETHER - another goroutine might see balance updated but not count

// Good: Mutex for compound operation
mu.Lock()
balance += amount
transactionCount++
mu.Unlock()
// Both updated atomically together
```

### Real-World Example: Request Counter

Track request metrics in a web server:

```go
type Metrics struct {
    totalRequests   int64
    successRequests int64
    errorRequests   int64
}

func (m *Metrics) IncrementTotal() {
    atomic.AddInt64(&m.totalRequests, 1)
}

func (m *Metrics) IncrementSuccess() {
    atomic.AddInt64(&m.successRequests, 1)
}

func (m *Metrics) IncrementError() {
    atomic.AddInt64(&m.errorRequests, 1)
}

func (m *Metrics) Snapshot() (total, success, error int64) {
    total = atomic.LoadInt64(&m.totalRequests)
    success = atomic.LoadInt64(&m.successRequests)
    error = atomic.LoadInt64(&m.errorRequests)
    return
}

// Usage in HTTP handler
var metrics Metrics

func handleRequest(w http.ResponseWriter, r *http.Request) {
    metrics.IncrementTotal()
    
    if err := processRequest(r); err != nil {
        metrics.IncrementError()
        http.Error(w, "Internal Server Error", 500)
        return
    }
    
    metrics.IncrementSuccess()
    w.Write([]byte("Success"))
}

// Periodic metrics reporting
go func() {
    ticker := time.NewTicker(10 * time.Second)
    for range ticker.C {
        total, success, error := metrics.Snapshot()
        log.Printf("Metrics - Total: %d, Success: %d, Error: %d", 
                   total, success, error)
    }
}()
```

Thousands of goroutines (HTTP handlers) safely increment these counters concurrently using atomic operations. No locks needed, minimal overhead.

### Atomic Pointers and Values

For more complex types, use `atomic.Value`:

```go
type Config struct {
    MaxConnections int
    Timeout        time.Duration
}

var config atomic.Value

// Initialize
config.Store(&Config{MaxConnections: 100, Timeout: 30 * time.Second})

// Read (from any goroutine)
cfg := config.Load().(*Config)
fmt.Println("Max connections:", cfg.MaxConnections)

// Update (from another goroutine)
config.Store(&Config{MaxConnections: 200, Timeout: 60 * time.Second})
```

**Use Case**: Configuration that can be updated at runtime. Reading the config is lock-free (atomic load). Updating the config is atomic (atomic store). Multiple readers don't block each other.

**Important**: The type stored must be consistent. Storing an `*Config` and then loading as `*OtherType` panics.

### Compare-and-Swap (CAS) for Lock-Free Algorithms

CAS is a building block for lock-free data structures:

```go
func incrementWithCAS(counter *int64) {
    for {
        old := atomic.LoadInt64(counter)
        new := old + 1
        
        if atomic.CompareAndSwapInt64(counter, old, new) {
            return  // Success
        }
        // CAS failed (another goroutine changed counter), retry
    }
}
```

**How It Works**: Read the current value. Compute the new value. Try to swap: "If the counter is still `old`, set it to `new`." If CAS succeeds, you're done. If CAS fails (another goroutine changed the counter), retry.

This is a lock-free increment. Under low contention, it succeeds on the first try. Under high contention, it might retry a few times, but goroutines never block - they just keep trying.

**When to Use CAS**: Building lock-free data structures like queues, stacks, or skip lists. For simple increments, `atomic.AddInt64` is simpler and just as fast (it's often implemented using CAS internally).

## Mutexes {#mutexes}

Mutexes (mutual exclusion locks) are the traditional way to protect shared state from concurrent access. While Go encourages communication over shared memory, mutexes are still essential for many scenarios.

### The Problem: Protecting Complex State

Sometimes you have complex state that multiple goroutines need to access or modify. Atomic operations aren't enough because you need to protect multiple related variables or perform complex operations.

**Example: Bank Account**

```go
type Account struct {
    balance int64
    transactions []Transaction
}
```

You need to update both `balance` and `transactions` together. Atomic operations can't help here - you need a way to say "no other goroutine can access this Account while I'm updating it."

### Mutex Basics

A mutex has two operations:

**Lock()**: Acquire the mutex. If another goroutine holds the lock, block until it's released.

**Unlock()**: Release the mutex. Wake up one goroutine waiting to acquire it.

```go
import "sync"

var (
    balance int
    mu      sync.Mutex
)

func deposit(amount int) {
    mu.Lock()        // Acquire lock
    balance += amount // Critical section
    mu.Unlock()      // Release lock
}

func withdraw(amount int) bool {
    mu.Lock()
    defer mu.Unlock()  // Always use defer!
    
    if balance >= amount {
        balance -= amount
        return true
    }
    return false
}
```

**Critical Section**: Code between `Lock()` and `Unlock()` is the critical section. Only one goroutine can execute this code at a time. While one goroutine holds the lock, others block at `Lock()` waiting their turn.

**Always Use Defer**: Always call `Unlock()` with `defer`. This ensures the lock is released even if your function panics. Forgetting to unlock causes deadlock - all other goroutines wait forever.

### How Mutexes Work Internally

When you call `Lock()`:

**If Unlocked**: Atomically mark the mutex as locked and proceed immediately. Fast path, no blocking.

**If Locked**: Add the goroutine to the mutex's wait queue and park it (block). The goroutine is taken off the CPU and sleeps. When the mutex is unlocked, the runtime wakes one waiting goroutine from the queue.

Go's mutex uses a combination of spinlock (for very short critical sections, a goroutine spins briefly hoping the lock becomes available) and sleep (for longer waits, the goroutine is descheduled to avoid wasting CPU).

**Fairness**: Go's mutex is semi-fair. Goroutines waiting longer have higher priority. But a goroutine that just arrives might still acquire the lock if it happens to check at the perfect moment (spin lock succeeds). This prevents starvation while maintaining performance.

### Common Mutex Patterns

**Pattern 1: Protecting Struct State**

```go
type SafeCounter struct {
    mu    sync.Mutex
    count int
}

func (sc *SafeCounter) Inc() {
    sc.mu.Lock()
    defer sc.mu.Unlock()
    sc.count++
}

func (sc *SafeCounter) Get() int {
    sc.mu.Lock()
    defer sc.mu.Unlock()
    return sc.count
}
```

Embed the mutex in the struct. Every method that accesses the protected state locks/unlocks the mutex. This encapsulates the synchronization - callers don't need to know about the mutex.

**Pattern 2: Multiple Related Variables**

```go
type Cache struct {
    mu    sync.Mutex
    items map[string]Item
    hits  int64
    misses int64
}

func (c *Cache) Get(key string) (Item, bool) {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    item, ok := c.items[key]
    if ok {
        c.hits++
    } else {
        c.misses++
    }
    
    return item, ok
}
```

The mutex protects `items`, `hits`, and `misses` together. They're always consistent - no goroutine sees intermediate state.

**Pattern 3: Conditional Logic**

```go
type Queue struct {
    mu    sync.Mutex
    items []Item
}

func (q *Queue) PopIfAvailable() (Item, bool) {
    q.mu.Lock()
    defer q.mu.Unlock()
    
    if len(q.items) == 0 {
        return Item{}, false
    }
    
    item := q.items[0]
    q.items = q.items[1:]
    return item, true
}
```

The check (`len(q.items) == 0`) and the modification (`q.items = q.items[1:]`) happen atomically within the critical section. No other goroutine can sneak in between the check and modification.

### Mutex Performance Considerations

Mutexes have costs:

**Lock Contention**: If many goroutines try to acquire the same mutex, they queue up. Only one can proceed at a time. High contention means goroutines spend time waiting, reducing parallelism.

**Cache Coherency Overhead**: On multi-CPU systems, when one CPU modifies memory protected by a mutex, other CPUs' caches are invalidated. This causes cache misses when they next access that memory.

**Context Switching**: Blocked goroutines are descheduled. When the lock becomes available, the OS scheduler must wake up a waiting goroutine. Context switches are expensive (microseconds, but they add up).

**Optimization Strategies**:

**Reduce Critical Section Size**: Hold the lock for the shortest time possible.

```go
// Bad: Long critical section
func (c *Cache) Process(key string) {
    c.mu.Lock()
    defer c.mu.Unlock()
    
    item := c.items[key]
    result := expensiveComputation(item)  // Holding lock during computation!
    c.items[key] = result
}

// Good: Short critical section
func (c *Cache) Process(key string) {
    c.mu.Lock()
    item := c.items[key]
    c.mu.Unlock()
    
    result := expensiveComputation(item)  // Lock released during computation
    
    c.mu.Lock()
    c.items[key] = result
    c.mu.Unlock()
}
```

**Partition Locks**: Instead of one mutex protecting everything, use multiple mutexes for different partitions.

```go
type ShardedCache struct {
    shards [16]shard
}

type shard struct {
    mu    sync.Mutex
    items map[string]Item
}

func (sc *ShardedCache) Get(key string) (Item, bool) {
    shard := &sc.shards[hash(key) % 16]  // Pick shard based on key hash
    
    shard.mu.Lock()
    defer shard.mu.Unlock()
    
    item, ok := shard.items[key]
    return item, ok
}
```

With 16 shards, up to 16 goroutines can access the cache simultaneously (each on a different shard). Contention is reduced 16x.

**Use RWMutex for Read-Heavy Workloads**: If most operations are reads, use `sync.RWMutex`.

### RWMutex: Read-Write Mutex

`sync.RWMutex` allows multiple readers or one writer:

```go
var (
    data   map[string]string
    rwmu   sync.RWMutex
)

// Multiple readers can hold RLock simultaneously
func read(key string) string {
    rwmu.RLock()
    defer rwmu.RUnlock()
    
    return data[key]
}

// Writer has exclusive access
func write(key, value string) {
    rwmu.Lock()
    defer rwmu.Unlock()
    
    data[key] = value
}
```

**How It Works**:

**RLock()**: Acquires a read lock. Multiple goroutines can hold read locks simultaneously. If a write lock is held, readers block.

**RUnlock()**: Releases a read lock.

**Lock()**: Acquires a write lock. Blocks until all readers and writers release their locks. Gives exclusive access.

**Unlock()**: Releases a write lock.

**When to Use**: When reads vastly outnumber writes (e.g., 90%+ reads). The overhead of tracking multiple readers is worth it. For write-heavy workloads, RWMutex can be slower than regular Mutex.

### Common Mutex Mistakes

**Mistake 1: Forgetting to Unlock**

```go
func bad() {
    mu.Lock()
    // Do work
    if errorCondition {
        return  // Forgot mu.Unlock()! DEADLOCK
    }
    mu.Unlock()
}

// Fix: Always use defer
func good() {
    mu.Lock()
    defer mu.Unlock()
    
    if errorCondition {
        return  // Unlock still happens
    }
}
```

**Mistake 2: Locking Same Mutex Twice (Deadlock)**

```go
func outer() {
    mu.Lock()
    defer mu.Unlock()
    
    inner()  // DEADLOCK! inner() tries to acquire same mutex
}

func inner() {
    mu.Lock()
    defer mu.Unlock()
    
    // Work
}

// Fix: Don't call functions that lock the same mutex
```

Go's mutex is not reentrant - the same goroutine can't acquire a mutex it already holds. This causes immediate deadlock.

**Mistake 3: Copying Mutex**

```go
type Counter struct {
    mu    sync.Mutex
    count int
}

func bad() {
    c1 := Counter{}
    c2 := c1  // Copies the mutex! BAD
    
    c1.Inc()  // Locks c1.mu
    c2.Inc()  // Locks c2.mu (different mutex!) NO PROTECTION
}
```

Never copy a struct containing a mutex. The copy gets its own mutex, so you lose synchronization. Pass by pointer instead.

**Mistake 4: Holding Lock Across Channel Operation**

```go
func bad(ch chan int) {
    mu.Lock()
    defer mu.Unlock()
    
    ch <- 42  // Sending on channel while holding mutex - RISKY
}
```

If the channel blocks (full buffer, no receiver), you're holding the mutex while blocked. Other goroutines can't acquire the mutex, even for unrelated work. This can cause deadlock.

**Fix**: Release the lock before channel operations, or use a buffered channel to avoid blocking.

### Mutex vs. Channels

A common question: when to use mutex vs. channels?

**Use Mutex When**:

- Protecting simple shared state (counters, caches, data structures)
- Multiple goroutines need to read/modify the same data
- Operations are short and don't involve waiting (no I/O, no channel operations)
- You want to leverage RWMutex for read-heavy workloads

**Use Channels When**:

- Transferring data between goroutines
- Coordinating goroutines (waiting for completion, signaling)
- Building pipelines or event-driven architectures
- You want to avoid shared memory altogether

**Combining Both**:

Sometimes you use both. Mutex protects state within a goroutine, channels communicate between goroutines:

```go
type SafeQueue struct {
    mu    sync.Mutex
    items []Item
    ready chan struct{}  // Signal when items available
}

func (sq *SafeQueue) Push(item Item) {
    sq.mu.Lock()
    sq.items = append(sq.items, item)
    sq.mu.Unlock()
    
    select {
    case sq.ready <- struct{}{}:
    default:
    }
}

func (sq *SafeQueue) Pop() Item {
    for {
        sq.mu.Lock()
        if len(sq.items) > 0 {
            item := sq.items[0]
            sq.items = sq.items[1:]
            sq.mu.Unlock()
            return item
        }
        sq.mu.Unlock()
        
        <-sq.ready  // Wait for signal
    }
}
```

Mutex protects the `items` slice. Channel signals when items are available.

---

## Stateful Goroutines {#stateful-goroutines}

The stateful goroutine pattern encapsulates state within a goroutine. Instead of protecting state with mutexes, the state is owned by a single goroutine, and other goroutines communicate with it via channels. This is the "share memory by communicating" philosophy in action.

### The Concept

Instead of:

```
Shared State (protected by mutex)
    ↑           ↑         ↑
    |           |         |
Goroutine 1  Goroutine 2  Goroutine 3
```

You do:

```
Goroutine 1 ──request──→  State Owner  ←──request── Goroutine 3
Goroutine 2 ──request──→  Goroutine    ←──request── Goroutine 4
                         (owns state)
```

The state owner is the only goroutine that accesses the state. Other goroutines send requests (via channels) to read or modify the state. The owner processes requests sequentially, ensuring all state access is serialized naturally.

### Implementation Example: Counter

**With Mutex**:

```go
type Counter struct {
    mu    sync.Mutex
    count int
}

func (c *Counter) Inc() {
    c.mu.Lock()
    c.count++
    c.mu.Unlock()
}

func (c *Counter) Get() int {
    c.mu.Lock()
    defer c.mu.Unlock()
    return c.count
}
```

**With Stateful Goroutine**:

```go
type Counter struct {
    ops chan operation
}

type operation struct {
    typ   string  // "inc" or "get"
    reply chan int
}

func NewCounter() *Counter {
    c := &Counter{
        ops: make(chan operation),
    }
    go c.run()
    return c
}

func (c *Counter) run() {
    count := 0
    
    for op := range c.ops {
        switch op.typ {
        case "inc":
            count++
        case "get":
            op.reply <- count
        }
    }
}

func (c *Counter) Inc() {
    c.ops <- operation{typ: "inc"}
}

func (c *Counter) Get() int {
    reply := make(chan int)
    c.ops <- operation{typ: "get", reply: reply}
    return <-reply
}
```

**How It Works**:

**State Ownership**: The `count` variable is a local variable in `run()`. Only the `run()` goroutine can access it. No other goroutine sees it.

**Request Processing**: Other goroutines send operation requests to the `ops` channel. The `run()` goroutine receives these requests and processes them sequentially.

**Serialization**: Because `run()` processes one request at a time, all state access is serialized. No race conditions possible.

**Synchronous Reads**: For the `Get()` operation, we need to return the count. We send a reply channel with the request. The `run()` goroutine sends the count on the reply channel. `Get()` blocks waiting for the reply.

### Advantages of Stateful Goroutines

**No Mutexes**: No need to think about locks, deadlocks, or lock contention. The state is simply not accessible from multiple goroutines.

**Complex State Machines**: For complex state with many transitions, the sequential processing makes reasoning easier. You can write the state machine logic straightforwardly without worrying about concurrent access.

**Auditing**: All state changes happen in one place (`run()`). Easy to log, trace, or debug state transitions.

**Atomic Compound Operations**: Operations involving multiple state changes are naturally atomic:

```go
func (c *Counter) run() {
    count := 0
    total := 0
    
    for op := range c.ops {
        switch op.typ {
        case "inc":
            count++
            total += op.value  // Both updated atomically
        }
    }
}
```

No need for mutex around both updates - they happen sequentially in the same goroutine.

### Disadvantages

**Overhead**: Channel communication has overhead. For very simple operations (incrementing a counter), atomic operations or mutexes are faster.

**Latency**: Every operation requires sending a request and waiting for the goroutine to process it. For high-performance code with thousands of operations per microsecond, this latency matters.

**Goroutine Overhead**: You're dedicating a goroutine to managing state. For thousands of objects, that's thousands of goroutines. Goroutines are cheap, but not free.

### When to Use Stateful Goroutines

**Use When**:

- State machine is complex (many states, complex transitions)
- You want to completely eliminate shared memory and mutexes
- Auditability and traceability are important
- Operations are not ultra-high-frequency

**Use Mutex When**:

- Simple state (counter, flag, cache)
- Ultra-high-performance requirements
- Read-heavy workloads (use RWMutex)

### Real-World Example: Connection Pool

A connection pool manages a pool of database connections. Connections are taken from the pool, used, and returned. The pool ensures connections aren't reused while in use and manages creation of new connections.

**Stateful Goroutine Approach**:

```go
type ConnectionPool struct {
    requests chan request
}

type request struct {
    typ   string  // "get" or "put"
    conn  *Connection
    reply chan *Connection
}

func NewConnectionPool(maxSize int) *ConnectionPool {
    cp := &ConnectionPool{
        requests: make(chan request),
    }
    go cp.run(maxSize)
    return cp
}

func (cp *ConnectionPool) run(maxSize int) {
    connections := make([]*Connection, 0, maxSize)
    inUse := make(map[*Connection]bool)
    
    for req := range cp.requests {
        switch req.typ {
        case "get":
            var conn *Connection
            
            // Check for available connection
            for _, c := range connections {
                if !inUse[c] {
                    conn = c
                    inUse[c] = true
                    break
                }
            }
            
            // Create new if needed and under limit
            if conn == nil && len(connections) < maxSize {
                conn = createConnection()
                connections = append(connections, conn)
                inUse[conn] = true
            }
            
            req.reply <- conn
            
        case "put":
            inUse[req.conn] = false
        }
    }
}

func (cp *ConnectionPool) Get() *Connection {
    reply := make(chan *Connection)
    cp.requests <- request{typ: "get", reply: reply}
    return <-reply
}

func (cp *ConnectionPool) Put(conn *Connection) {
    cp.requests <- request{typ: "put", conn: conn}
}
```

**Benefits**:

- All pool state (`connections`, `inUse` map) is owned by one goroutine
- No race conditions possible
- Complex logic (checking availability, creating new connections, tracking in-use) is straightforward
- Easy to add features (metrics, health checks, connection expiration)

---

## Pattern Comparison and Selection {#pattern-comparison}

Let's compare all the patterns we've covered to help you choose the right one.

### Quick Reference Table

|Pattern|Best For|Overhead|Complexity|Scalability|
|---|---|---|---|---|
|**Worker Pool**|Processing many tasks with bounded concurrency|Low|Medium|Excellent|
|**WaitGroup**|Waiting for goroutines to complete|Very Low|Low|Excellent|
|**Rate Limiting**|Controlling operation frequency|Low|Low-Medium|Good|
|**Atomic Counters**|Simple counters/flags|Very Low|Very Low|Excellent|
|**Mutexes**|Protecting complex shared state|Low-Medium|Medium|Good|
|**Stateful Goroutines**|Complex state machines, avoiding shared memory|Medium|Medium-High|Good|

### Decision Tree

**Question 1: Do you need to process many tasks concurrently?**

- Yes → Use **Worker Pool** (bounds concurrency)
- No → Continue

**Question 2: Do you need to wait for goroutines to complete?**

- Yes → Use **WaitGroup**
- No → Continue

**Question 3: Do you need to control operation frequency?**

- Yes → Use **Rate Limiting**
- No → Continue

**Question 4: Do you need to share state between goroutines?**

- No → No synchronization needed!
- Yes → Continue

**Question 5: Is the state a simple counter or flag?**

- Yes → Use **Atomic Operations**
- No → Continue

**Question 6: Do you prefer channels over locks?**

- Yes → Use **Stateful Goroutines**
- No → Use **Mutexes**

---

## Real-World Applications {#real-world-applications}

Let's see how these patterns combine in real-world scenarios.

### Application 1: Web Scraper

**Requirements**: Scrape 10,000 URLs. Respect rate limit (100 requests/second). Track progress. Process results.

**Implementation**:

```go
type Scraper struct {
    pool      *WorkerPool
    limiter   *rate.Limiter
    completed int64  // Atomic counter
    failed    int64  // Atomic counter
}

func NewScraper() *Scraper {
    return &Scraper{
        pool:    NewWorkerPool(50),  // 50 concurrent workers
        limiter: rate.NewLimiter(rate.Limit(100), 10),  // 100 req/sec
    }
}

func (s *Scraper) Scrape(urls []string) []Result {
    s.pool.Start()
    results := make(chan Result, len(urls))
    
    var wg sync.WaitGroup
    
    for _, url := range urls {
        wg.Add(1)
        s.pool.Submit(Task{
            Work: func() {
                defer wg.Done()
                
                // Rate limit
                s.limiter.Wait(context.Background())
                
                // Scrape
                result, err := fetch(url)
                
                // Update counters
                if err != nil {
                    atomic.AddInt64(&s.failed, 1)
                } else {
                    atomic.AddInt64(&s.completed, 1)
                }
                
                results <- result
            },
        })
    }
    
    go func() {
        wg.Wait()
        close(results)
    }()
    
    // Collect results
    var allResults []Result
    for result := range results {
        allResults = append(allResults, result)
    }
    
    return allResults
}
```

**Patterns Used**:

- Worker Pool: Bounds concurrency to 50
- Rate Limiting: Ensures 100 req/sec
- WaitGroup: Waits for all scraping to complete
- Atomic Counters: Tracks completed/failed (thread-safe metrics)

### Application 2: Order Processing System

**Requirements**: Process orders. Validate, charge payment, update inventory, send notifications. Each step has its own rate limits and requirements.

**Implementation**:

```go
type OrderProcessor struct {
    validationPool   *WorkerPool       // CPU-bound
    paymentLimiter   *rate.Limiter     // API rate limit
    inventoryMu      sync.Mutex        // Protect shared inventory
    inventory        map[string]int
    notifications    chan Notification // Stateful goroutine
}

func (op *OrderProcessor) Process(order Order) error {
    // Step 1: Validate (CPU-bound, use worker pool)
    validResult := make(chan error)
    op.validationPool.Submit(Task{
        Work: func() {
            validResult <- validate(order)
        },
    })
    
    if err := <-validResult; err != nil {
        return err
    }
    
    // Step 2: Charge payment (rate limited API)
    if err := op.paymentLimiter.Wait(context.Background()); err != nil {
        return err
    }
    
    if err := chargePayment(order); err != nil {
        return err
    }
    
    // Step 3: Update inventory (mutex-protected)
    op.inventoryMu.Lock()
    for item, qty := range order.Items {
        op.inventory[item] -= qty
    }
    op.inventoryMu.Unlock()
    
    // Step 4: Send notification (stateful goroutine)
    op.notifications <- Notification{
        OrderID: order.ID,
        Message: "Order processed",
    }
    
    return nil
}
```

**Patterns Used**:

- Worker Pool: CPU-bound validation
- Rate Limiting: Payment API calls
- Mutex: Shared inventory access
- Stateful Goroutine: Notification processing (assuming complex logic)

This comprehensive guide covers all six concurrency patterns with deep explanations, real-world examples, and practical guidance on when to use each!