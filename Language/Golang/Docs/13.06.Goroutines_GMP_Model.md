# The Complete Guide to Go's GMP Concurrency Model: Deep Dive into CPU and Memory

## Table of Contents

1. [Introduction - The GMP Revolution](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#introduction)
2. [The Three Core Components](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#core-components)
3. [G - Goroutine (The Worker)](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#g-goroutine)
4. [M - Machine (The OS Thread)](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#m-machine)
5. [P - Processor (The Context)](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#p-processor)
6. [Complete GMP Architecture](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#gmp-architecture)
7. [Goroutine Lifecycle](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#goroutine-lifecycle)
8. [CPU-Level Execution](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#cpu-execution)
9. [Memory Layout and Management](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#memory-layout)
10. [Work Stealing Algorithm](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#work-stealing)
11. [System Calls and Blocking](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#system-calls)
12. [Real-World Examples](https://claude.ai/chat/405a99a2-36ec-4ced-a866-5f5f881932a6#examples)

---

## Introduction - The GMP Revolution {#introduction}

Imagine managing a restaurant with thousands of orders coming in every second. You have two choices:

**Traditional Approach (OS Threads)**:

```
Hire 10,000 chefs (OS threads)
Each chef:
  - Has their own massive kitchen (8MB stack)
  - Handles ONE order at a time
  - Takes 5ms to switch between chefs (context switch)
  - Total memory: 10,000 × 8MB = 80GB!
  - OS manages scheduling (expensive)
```

**Go's GMP Approach**:

```
Hire 8 master chefs (M = OS threads, matching CPU cores)
Have 8 cooking stations (P = Logical processors)
Accept 10,000 orders (G = Goroutines)

Each order (goroutine):
  - Tiny ticket (2KB initial stack)
  - Efficiently scheduled by Go runtime (not OS)
  - Switches in microseconds (not milliseconds)
  - Total memory: 10,000 × 2KB = 20MB!
  - 4000x less memory!
```

**Why GMP is Revolutionary**:

```
┌───────────────────────────────────────────────────────────────────────────┐
│                  OS THREADS vs GO GMP MODEL                               │
└───────────────────────────────────────────────────────────────────────────┘

OS Threads Model (Java, C++, etc.):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Application
    ↓
┌─────────────────────────────────────────────────────────────────┐
│  Thread 1   Thread 2   Thread 3  ...  Thread 10000              │
│  (8MB)      (8MB)      (8MB)          (8MB)                     │
└──────────────────────────────┬──────────────────────────────────┘
                               ↓
                    OS Kernel Scheduler
                    (Expensive scheduling)
                               ↓
┌─────────────────────────────────────────────────────────────────┐
│  CPU Core 1   CPU Core 2   CPU Core 3  ...  CPU Core 8          │
└─────────────────────────────────────────────────────────────────┘

Problems:
  × Memory: 10,000 threads × 8MB = 80GB
  × Context switches: 1-5ms (kernel involvement)
  × Max threads: ~few thousand (OS limits)
  × Scheduling overhead: OS does all the work


Go GMP Model:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Application
    ↓
┌─────────────────────────────────────────────────────────────────┐
│  G1   G2   G3   G4   G5  ...  G10000                            │  Goroutines
│ (2KB)(2KB)(2KB)(2KB)(2KB)    (2KB)                              │  (10,000)
└──────────────────────────────┬──────────────────────────────────┘
                               ↓
            Go Runtime Scheduler (Userspace)
            (Fast, intelligent scheduling)
                               ↓
┌────────────────────────────────────────────────────────────────┐
│  P0      P1      P2      P3      P4      P5      P6      P7    │  Processors
│  (ctx)   (ctx)   (ctx)   (ctx)   (ctx)   (ctx)   (ctx)   (ctx) │  (8)
└────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────────┘
     ↓      ↓      ↓      ↓      ↓      ↓      ↓      ↓
┌─────────────────────────────────────────────────────────────────┐
│  M0      M1      M2      M3      M4      M5      M6      M7     │  Machines
│ (Thread)(Thread)(Thread)(Thread)(Thread)(Thread)(Thread)(Thread)│  (8 OS threads)
└────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────────-┘
     ↓      ↓      ↓      ↓      ↓      ↓      ↓      ↓
┌─────────────────────────────────────────────────────────────────┐
│  CPU 0   CPU 1   CPU 2   CPU 3   CPU 4   CPU 5   CPU 6   CPU 7  │  Physical
└─────────────────────────────────────────────────────────────────┘  Cores

Advantages:
  ✓ Memory: 10,000 goroutines × 2KB = 20MB (4000x less!)
  ✓ Context switches: 200ns-2µs (no kernel, userspace only)
  ✓ Max goroutines: Millions (only limited by memory)
  ✓ Scheduling overhead: Go runtime (optimized, work-stealing)
  ✓ CPU efficiency: 8 threads for 8 cores (perfect match)
```

---

## The Three Core Components {#core-components}

The GMP model has three key components working together:

### Overview Diagram

```
┌───────────────────────────────────────────────────────────────────────────┐
│                      GMP MODEL - THE THREE PILLARS                        │
└───────────────────────────────────────────────────────────────────────────┘

G (Goroutine) - The Task
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌────────────────────────────────────────────────────────────────┐
│  What:    Lightweight thread (user-level)                      │
│  Size:    2KB initial stack (grows dynamically)                │
│  Count:   Can have millions                                    │
│  Purpose: Represents a function execution                      │
│  State:   Running, Runnable, Waiting, Dead                     │
│                                                                │
│  struct g {                                                    │
│      stack       stack    // 2KB-1GB stack space               │
│      stackguard0 uintptr  // Stack overflow detection          │
│      m           *m       // Current M (if running)            │
│      sched       gobuf    // Scheduling context (PC, SP)       │
│      atomicstatus uint32  // G state                           │
│      goid        int64    // Unique goroutine ID               │
│      waitsince   int64    // Time when G started waiting       │
│      ... 50+ more fields                                       │
│  }                                                             │
└────────────────────────────────────────────────────────────────┘

M (Machine) - The Worker
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌───────────────────────────────────────────────────────────────┐
│  What:    OS thread (kernel-level)                            │
│  Size:    ~8KB control structure + OS thread overhead         │
│  Count:   Typically equals GOMAXPROCS (CPU cores)             │
│  Purpose: Executes goroutines                                 │
│  Bound:   1:1 with OS threads                                 │
│                                                               │
│  struct m {                                                   │
│      g0           *g      // Scheduling goroutine             │
│      curg         *g      // Current running G                │
│      p            puintptr // Attached P (processor)          │
│      nextp        puintptr // Next P to run                   │
│      spinning     bool    // Looking for work?                │
│      blocked      bool    // In blocking syscall?             │
│      park         note    // Park/unpark mechanism            │
│      ... 40+ more fields                                      │
│  }                                                            │
└───────────────────────────────────────────────────────────────┘

P (Processor) - The Context
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌───────────────────────────────────────────────────────────────┐
│  What:    Logical processor (scheduling context)              │
│  Size:    ~0.5KB control structure                            │
│  Count:   Set by GOMAXPROCS (default = CPU cores)             │
│  Purpose: Provides resources for M to execute G               │
│  Contains: Local run queue, memory cache, timers              │
│                                                               │
│  struct p {                                                   │
│      status       uint32  // P state (idle, running, etc.)    │
│      m            muintptr // Attached M                      │
│      runqhead     uint32   // Local run queue head            │
│      runqtail     uint32   // Local run queue tail            │
│      runq         [256]guintptr // Local queue (256 Gs max)   │
│      runnext      guintptr // Next G to run (priority)        │
│      mcache       *mcache  // Memory allocator cache          │
│      ... 30+ more fields                                      │
│  }                                                            │
└───────────────────────────────────────────────────────────────┘
```

### How They Work Together

```
┌───────────────────────────────────────────────────────────────────────────┐
│                   GMP RELATIONSHIP AND FLOW                               │
└───────────────────────────────────────────────────────────────────────────┘

The Execution Chain:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

      G (Goroutine)                "I'm a task that needs to run"
      │
      │ Scheduled onto
      ↓
      P (Processor)                "I'm the context/resources"
      │
      │ Needs worker to execute
      ↓
      M (Machine / Thread)           "I'm the actual worker"
      │
      │ Runs on
      ↓
      CPU Core                     "I'm the physical hardware"


Visual Representation:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

                        Goroutines (10,000+)
                    ┌────────────────────────┐
                    │ G1  G2  G3  G4  G5 ... │
                    │ G6  G7  G8  G9  G10... │
                    │ G11 G12 G13 G14 G15... │
                    │  ...                   │
                    │  G9998 G9999 G10000    │
                    └───────────┬────────────┘
                                │
                   Distributed across Processors
                                │
            ┌───────────────────┼───────────────────┐
            ↓                   ↓                   ↓
    ┌──────────────┐    ┌──────────────┐    ┌──────────────┐
    │ P0           │    │ P1           │ .. │ P7           │
    │ ┌──────────┐ │    │ ┌──────────┐ │    │ ┌──────────┐ │
    │ │ RunQueue │ │    │ │ RunQueue │ │    │ │ RunQueue │ │
    │ │ G1  G2   │ │    │ │ G5  G6   │ │    │ │ G50 G51  │ │
    │ │ G3  G4   │ │    │ │ G7  G8   │ │    │ │ G52 G53  │ │
    │ └──────────┘ │    │ └──────────┘ │    │ └──────────┘ │
    │              │    │              │    │              │
    │ Attached to  │    │ Attached to  │    │ Attached to  │
    │      ↓       │    │      ↓       │    │      ↓       │
    │    ┌───┐     │    │    ┌───┐     │    │    ┌───┐     │
    │    │ M0│     │    │    │ M1│     │    │    │ M7│     │
    │    └─┬─┘     │    │    └─┬─┘     │    │    └─┬─┘     │
    └──────┼─────-─┘    └──────┼──────-┘    └──────┼────-──┘
           │                   │                   │
           ↓                   ↓                   ↓
    ┌──────────────────────────────────────────────────┐
    │ CPU0    CPU1    CPU2  ...           CPU7         │
    │ [EXEC]  [EXEC]  [EXEC]              [EXEC]       │
    └──────────────────────────────────────────────────┘

Legend:
  G  = Goroutine (task to execute)
  P  = Processor (scheduling context, has local run queue)
  M  = Machine (OS thread, does the actual work)
  CPU = Physical processor core


Key Relationships:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

G:P Relationship (Many-to-One)
  - Thousands of Gs can be queued on one P
  - P's local queue can hold 256 Gs
  - Overflow goes to global queue

P:M Relationship (One-to-One when running)
  - Each M needs a P to execute Gs
  - P can be handed off between Ms
  - Idle Ps wait for Ms to attach

M:CPU Relationship (Many-to-Few)
  - More Ms than CPUs (typically ~few more)
  - Only GOMAXPROCS Ms run simultaneously
  - Extra Ms used for blocking calls

GOMAXPROCS = P count = CPU count (by default)
  - GOMAXPROCS = 8 means:
    * 8 Ps created
    * 8 Gs can run simultaneously
    * Potentially thousands of Ms (most parked)
    * Thousands/millions of Gs (queued)
```

---

## G - Goroutine (The Worker) {#g-goroutine}

Deep dive into the Goroutine structure and lifecycle.

### Goroutine Structure

```
┌───────────────────────────────────────────────────────────────────────────┐
│                    GOROUTINE (G) - DETAILED STRUCTURE                     │
└───────────────────────────────────────────────────────────────────────────┘

Complete G Struct (simplified from runtime/runtime2.go):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

type g struct {
    // Stack fields
    stack       stack   // Stack bounds: [lo, hi)
    stackguard0 uintptr // Stack overflow check value
    stackguard1 uintptr // C stack overflow check
    
    // Scheduling fields
    m           *m      // Current M executing this G
    sched       gobuf   // Saved context (PC, SP, G)
    
    // State and identity
    atomicstatus uint32 // G state (see states below)
    goid         int64  // Unique goroutine ID
    
    // Preemption
    preempt       bool   // Preemption signal
    preemptStop   bool   // Transition to _Gpreempted
    preemptShrink bool   // Shrink stack at sync safe point
    
    // Waiting information
    waitsince     int64     // When G started waiting
    waitreason    waitReason // Why G is waiting
    
    // Panic/defer tracking
    _panic     *_panic // Active panic
    _defer     *_defer // Active defer calls
    
    // Locking
    locks      int32  // Lock depth (prevents preemption)
    
    // GC and memory
    gcscandone   bool   // GC scan completed?
    gcAssistBytes int64 // GC assist bytes allocated
    
    ... 50+ more fields for profiling, tracing, etc.
}

type stack struct {
    lo uintptr // Low bound (bottom of stack)
    hi uintptr // High bound (top of stack)
}

type gobuf struct {
    sp   uintptr // Stack pointer
    pc   uintptr // Program counter
    g    guintptr // Goroutine pointer
    ctxt unsafe.Pointer // Context
    ret  uintptr // Return value
    lr   uintptr // Link register (ARM)
    bp   uintptr // Base pointer (x86)
}

Memory Layout of a Goroutine:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

┌────────────────────────────────────────────────────────────────┐
│  G Struct (~376 bytes)                                         │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  stack.lo:  0x00c000100000   ◄─┐                         │  │
│  │  stack.hi:  0x00c000102000   ◄─┼─ Stack boundaries       │  │
│  │  stackguard0: 0x00c0001001f0   │  (8KB = 8192 bytes)     │  │
│  │                                │                         │  │
│  │  m: *m (pointer)               │                         │  │
│  │  sched: gobuf {                │                         │  │
│  │    sp: 0x00c000101f80  ◄───────┼─ Current stack pointer  │  │
│  │    pc: 0x00456789       ◄──────┼─ Next instruction       │  │
│  │  }                             │                         │  │
│  │                                │                         │  │
│  │  atomicstatus: 2 (_Grunning)   │                         │  │
│  │  goid: 42                      │                         │  │
│  │  ...                           │                         │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘
                        │
                        ↓ Points to stack
┌────────────────────────────────────────────────────────────────┐
│  Goroutine Stack (2KB-1GB, grows dynamically)                  │
│                                                                │
│  High Address (stack.hi)  0x00c000102000                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  [Guard Page - 4KB]       ◄─── Stack overflow protection │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  [Available Space - grows downward]                      │  │
│  │                                                          │  │
│  │         ↓ SP (Stack Pointer)                             │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  Local variables (current function)                      │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  Return address                                          │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  Previous function's stack frame                         │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  Arguments                                               │  │
│  └──────────────────────────────────────────────────────────┘  │
│  Low Address (stack.lo)   0x00c000100000                       │
└────────────────────────────────────────────────────────────────┘

Size Breakdown:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
G struct:          376 bytes (fixed size)
Initial stack:     2,048 bytes (2KB)
Stack guard:       4,096 bytes (4KB protection)
────────────────────────────────
Minimum per G:     ~6.5KB
```

### Goroutine States

```
┌───────────────────────────────────────────────────────────────────────────┐
│                    GOROUTINE STATE MACHINE                                │
└───────────────────────────────────────────────────────────────────────────┘

States (atomicstatus values):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

_Gidle        = 0  // Just allocated, not initialized
_Grunnable    = 1  // On run queue, ready to run
_Grunning     = 2  // Executing on M
_Gsyscall     = 3  // In system call, not executing
_Gwaiting     = 4  // Blocked (channel, lock, I/O, etc.)
_Gdead        = 6  // Finished execution, can be reused
_Gcopystack   = 8  // Stack is being copied (growth)
_Gpreempted   = 9  // Preempted, waiting to run again


State Transition Diagram:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

                    ┌──────────────┐
         Creation → │   _Gidle     │ ← New goroutine allocated
                    └──────┬───────┘
                           │ Initialized
                           ↓
                    ┌──────────────┐
                    │  _Grunnable  │ ← Waiting in run queue
                    └───┬──────────┘
                        │ Scheduled by P
                        │
         Preempted ←────┤
         Timer expired  │
                        ↓
                    ┌──────────────┐
                    │  _Grunning   │ ← Executing on M + P
                    └───┬──────────┘
                        │
        ┌───────────────┼───────────────┬─────────────┐
        │               │               │             │
        ↓               ↓               ↓             ↓
  ┌──────────┐    ┌──────────┐   ┌──────────┐  ┌──────────┐
  │_Gwaiting │    │_Gsyscall │   │_Gcopystack│ │  _Gdead  │
  │          │    │          │   │          │  │          │
  │ Blocked  │    │ System   │   │ Stack    │  │ Finished │
  │ on:      │    │ call     │   │ growing  │  │          │
  │ - Chan   │    │          │   │          │  │          │
  │ - Lock   │    │          │   │          │  │          │
  │ - I/O    │    │          │   │          │  │          │
  │ - Timer  │    │          │   │          │  │          │
  └────┬─────┘    └────┬─────┘   └────┬─────┘  └──────────┘
       │               │              │             │
       │ Unblocked     │ Returns      │ Complete    │ GC reuse
       ↓               ↓              ↓             ↓
  ┌──────────────────────────────────────────────────────┐
  │              _Grunnable (back to queue)              │
  └──────────────────────────────────────────────────────┘


Example Transitions with Code:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Transition 1: Creation → Runnable
┌────────────────────────────────────────────────────────┐
│  go func() {                // Creates new goroutine   │
│      fmt.Println("Hello")                              │
│  }()                                                   │
│                                                        │
│  State: _Gidle → _Grunnable                            │
│  Action: G added to P's local run queue                │
└────────────────────────────────────────────────────────┘

Transition 2: Runnable → Running
┌────────────────────────────────────────────────────────┐
│  Scheduler picks G from queue                          │
│  Attaches G to M                                       │
│  Sets G's M pointer                                    │
│                                                        │
│  State: _Grunnable → _Grunning                         │
│  Action: G starts executing on CPU                     │
└────────────────────────────────────────────────────────┘

Transition 3: Running → Waiting (Channel)
┌────────────────────────────────────────────────────────┐
│  ch := make(chan int)                                  │
│  val := <-ch            // Blocks here                 │
│                                                        │
│  State: _Grunning → _Gwaiting                          │
│  Action: G removed from M, added to channel wait queue │
│  waitreason: waitReasonChanReceive                     │
└────────────────────────────────────────────────────────┘

Transition 4: Waiting → Runnable (Channel send)
┌────────────────────────────────────────────────────────┐
│  ch <- 42               // Another goroutine sends     │
│                                                        │
│  State: _Gwaiting → _Grunnable                         │
│  Action: G woken up, added back to run queue           │
└────────────────────────────────────────────────────────┘

Transition 5: Running → Syscall
┌────────────────────────────────────────────────────────┐
│  file.Read(buffer)      // System call                 │
│                                                        │
│  State: _Grunning → _Gsyscall                          │
│  Action: P detached from M, can run other Gs           │
└────────────────────────────────────────────────────────┘

Transition 6: Running → Dead
┌────────────────────────────────────────────────────────┐
│  func worker() {                                       │
│      // ... work done                                  │
│  } // Function returns                                 │
│                                                        │
│  State: _Grunning → _Gdead                             │
│  Action: G put in dead list, can be reused             │
└────────────────────────────────────────────────────────┘
```

### Stack Growth

```
┌───────────────────────────────────────────────────────────────────────────┐
│                    DYNAMIC STACK GROWTH                                   │
└───────────────────────────────────────────────────────────────────────────┘

Initial State (2KB stack):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

┌────────────────────────────────────────────────────────────────┐
│  Stack (2048 bytes)                                            │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Used: 1024 bytes    (50% full)                          │  │
│  │  Free: 1024 bytes                                        │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                │
│  stackguard0 = stack.lo + StackGuard (usually 928 bytes)       │
│  When SP < stackguard0 → Stack overflow detected!              │
└────────────────────────────────────────────────────────────────┘


Stack Growth Trigger:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

func deepRecursion(n int) {
    var buffer [512]byte  // Need more stack space!
    
    // Prologue checks: if SP - 512 < stackguard0 → grow
    
    if n > 0 {
        deepRecursion(n - 1)
    }
}

Step 1: Stack Nearly Full
┌────────────────────────────────────────────────────────────────┐
│  Current Stack (2KB)                                           │
│  ├─────────────────────────────────────────┐                   │
│  │  Used: 1920 bytes                       │                   │
│  │  Free: 128 bytes ◄─── Not enough!       │                   │
│  └─────────────────────────────────────────┘                   │
│  SP is approaching stackguard0                                 │
└────────────────────────────────────────────────────────────────┘

Step 2: Runtime Detects Need to Grow
┌────────────────────────────────────────────────────────────────┐
│  Function needs 512 bytes                                      │
│  Available: 128 bytes                                          │
│  → Trigger morestack()                                         │
└────────────────────────────────────────────────────────────────┘

Step 3: Allocate New Larger Stack (Double Size = 4KB)
┌────────────────────────────────────────────────────────────────┐
│  New Stack (4096 bytes)                                        │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  [Empty - will copy old stack here]                      │  │
│  │                                                          │  │
│  │                                                          │  │
│  │                                                          │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘

Step 4: Copy Stack Contents
┌────────────────────────────────────────────────────────────────┐
│  New Stack (4096 bytes)                                        │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  [Copied from old stack: 1920 bytes]                     │  │
│  ├──────────────────────────────────────────────────────────┤  │
│  │  Free: 2176 bytes  ◄─── Plenty of space now!             │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘

Step 5: Update Pointers
┌────────────────────────────────────────────────────────────────┐
│  g.stack.lo = new_stack_base                                   │
│  g.stack.hi = new_stack_base + 4096                            │
│  g.stackguard0 = new_stack_base + StackGuard                   │
│  Adjust all pointers in copied stack frames                    │
└────────────────────────────────────────────────────────────────┘

Step 6: Free Old Stack
┌────────────────────────────────────────────────────────────────┐
│  Old 2KB stack → returned to memory pool                       │
│  G continues execution on new 4KB stack                        │
└────────────────────────────────────────────────────────────────┘


Growth Pattern:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

2KB → 4KB → 8KB → 16KB → 32KB → ... → 1GB (max)

Each growth typically doubles the size
Growth is relatively rare (only when needed)
Growth cost: ~50-200µs depending on stack size

Example Timeline:
┌────────────────────────────────────────────────────────────────┐
│  T=0µs:    G created with 2KB stack                            │
│  T=100µs:  Stack 90% full → grow to 4KB (50µs)                 │
│  T=150µs:  Continue execution                                  │
│  T=500µs:  Stack 90% full → grow to 8KB (75µs)                 │
│  T=575µs:  Continue execution                                  │
│  ...                                                           │
│  Final:    Stack settled at 16KB (function recursion depth)    │
└────────────────────────────────────────────────────────────────┘

Most goroutines never grow beyond 2-8KB!
```

This covers the fundamentals and G (Goroutine) in extreme detail. Should I continue with M (Machine), P (Processor), and the complete GMP execution flow?

## M - Machine (The OS Thread) {#m-machine}

The M represents an actual OS thread that does the work.

### Machine Structure

```
┌───────────────────────────────────────────────────────────────────────────┐
│                    MACHINE (M) - DETAILED STRUCTURE                       │
└───────────────────────────────────────────────────────────────────────────┘

Complete M Struct (simplified from runtime/runtime2.go):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

type m struct {
    // Special goroutines
    g0       *g    // Scheduling goroutine (for this M)
    morebuf  gobuf // Signal handler's stack

    // Current execution
    curg     *g    // Currently running G
    
    // Associated processor
    p        puintptr // Attached P (only when running)
    nextp    puintptr // Next P to attach to
    oldp     puintptr // Previous P (for preemption)
    
    // Thread identity
    id       int64  // Unique M ID
    
    // Scheduling state
    spinning bool   // M is looking for work
    blocked  bool   // M is blocked in syscall
    
    // Locking and parking
    locks    int32  // Number of locks held
    park     note   // For parking/unparking M
    
    // Memory allocation
    mcache   *mcache  // Per-M memory cache
    
    // System call state
    syscalltick uint32 // Incremented on each syscall
    
    ... 40+ more fields
}


Memory Layout:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

┌────────────────────────────────────────────────────────────────┐
│  M Struct (~8KB)                                               │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  g0:      *g (points to scheduling goroutine)            │  │
│  │  curg:    *g (points to currently running G)             │  │
│  │  p:       *p (points to attached P)                      │  │
│  │  nextp:   *p (next P to run)                             │  │
│  │                                                          │  │
│  │  id:      42 (M identifier)                              │  │
│  │  spinning: false                                         │  │
│  │  blocked:  false                                         │  │
│  │  ...                                                     │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘
         ↓ Creates and manages
┌────────────────────────────────────────────────────────────────┐
│  OS Thread (Kernel-level)                                      │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Thread ID (TID): 1234                                   │  │
│  │  Kernel Stack: 8MB                                       │  │
│  │  Priority: Normal                                        │  │
│  │  CPU Affinity: Any CPU                                   │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘
         ↓ Runs on
┌────────────────────────────────────────────────────────────────┐
│  Physical CPU Core                                             │
└────────────────────────────────────────────────────────────────┘
```

### M's Special g0 Goroutine

```
┌───────────────────────────────────────────────────────────────────────────┐
│                    M's g0 - THE SCHEDULING GOROUTINE                      │
└───────────────────────────────────────────────────────────────────────────┘

Every M has a special g0 goroutine used for scheduling:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

┌────────────────────────────────────────────────────────────────┐
│  M (OS Thread)                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                                                          │  │
│  │  Regular Execution:                                      │  │
│  │  ┌────────────────────────────────────────────────────┐  │  │
│  │  │  m.curg = User Goroutine G1                        │  │  │
│  │  │  Running user code on G1's stack                   │  │  │
│  │  │  (Your application logic)                          │  │  │
│  │  └────────────────────────────────────────────────────┘  │  │
│  │                                                          │  │
│  │  Switch to g0 when:                                      │  │
│  │  ┌────────────────────────────────────────────────────┐  │  │
│  │  │  m.curg = m.g0 (Scheduling goroutine)              │  │  │
│  │  │  Running on g0's stack (system stack)              │  │  │
│  │  │                                                    │  │  │
│  │  │  Tasks performed on g0:                            │  │  │
│  │  │  - Schedule next goroutine                         │  │  │
│  │  │  - Handle system calls                             │  │  │
│  │  │  - Grow goroutine stacks                           │  │  │
│  │  │  - Run GC work                                     │  │  │
│  │  │  - Handle signals                                  │  │  │
│  │  └────────────────────────────────────────────────────┘  │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘


Why g0?
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. System Stack (larger, more predictable)
   ┌────────────────────────────────────────────────────────┐
   │  g0 has a large fixed stack (8MB on Linux)             │
   │  User G has small dynamic stack (2KB-1GB)              │
   │  Scheduling operations need predictable stack space    │
   └────────────────────────────────────────────────────────┘

2. No Preemption
   ┌────────────────────────────────────────────────────────┐
   │  g0 cannot be preempted                                │
   │  Critical runtime operations must complete atomically  │
   └────────────────────────────────────────────────────────┘

3. Safe Point for Operations
   ┌────────────────────────────────────────────────────────┐
   │  Stack growth: Can't grow stack while examining stack  │
   │  GC operations: Need stable state                      │
   │  Scheduler: Need to save/restore context               │
   └────────────────────────────────────────────────────────┘


Context Switch (User G ↔ g0):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

From User G to g0:
┌────────────────────────────────────────────────────────────────┐
│  Step 1: Save User G's context                                 │
│    g.sched.sp = current_sp                                     │
│    g.sched.pc = return_address                                 │
│                                                                │
│  Step 2: Switch to g0's stack                                  │
│    sp = m.g0.sched.sp                                          │
│    m.curg = m.g0                                               │
│                                                                │
│  Step 3: Execute scheduling code                               │
│    schedule()  // Find next G to run                           │
└────────────────────────────────────────────────────────────────┘

From g0 to User G:
┌────────────────────────────────────────────────────────────────┐
│  Step 1: Select next G to run                                  │
│    gp = findrunnable()                                         │
│                                                                │
│  Step 2: Restore G's context                                   │
│    sp = gp.sched.sp                                            │
│    pc = gp.sched.pc                                            │
│                                                                │
│  Step 3: Jump to G's code                                      │
│    m.curg = gp                                                 │
│    execute gp  // Resume user code                             │
└────────────────────────────────────────────────────────────────┘
```

---

## P - Processor (The Context) {#p-processor}

The P provides the execution context and resources.

### Processor Structure

```
┌───────────────────────────────────────────────────────────────────────────┐
│                    PROCESSOR (P) - DETAILED STRUCTURE                     │
└───────────────────────────────────────────────────────────────────────────┘

Complete P Struct (simplified from runtime/runtime2.go):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

type p struct {
    // Identity
    id        int32    // P identifier (0 to GOMAXPROCS-1)
    status    uint32   // P state (_Pidle, _Prunning, etc.)
    
    // Associated M
    m         muintptr // Attached M (only when running)
    
    // Local run queue (256 Gs max)
    runqhead  uint32              // Head index
    runqtail  uint32              // Tail index
    runq      [256]guintptr       // Circular queue of Gs
    runnext   guintptr            // Next G to run (priority)
    
    // Memory allocation cache
    mcache    *mcache  // Per-P memory cache
    
    // GC state
    gcAssistTime int64  // Time in GC assist
    gcBgMarkWorker guintptr // Background mark worker
    
    // Timer management
    timersLock mutex
    timers     []*timer // Heap of timers
    
    // Statistics
    schedtick  uint32 // Incremented on each schedule()
    syscalltick uint32 // Incremented on each syscall
    
    ... 30+ more fields
}


Memory Layout:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

┌────────────────────────────────────────────────────────────────┐
│  P Struct (~4KB)                                               │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  id:        0 (P0)                                       │  │
│  │  status:    _Prunning                                    │  │
│  │  m:         *m (pointer to attached M)                   │  │
│  │                                                          │  │
│  │  Local Run Queue (256 slots):                            │  │
│  │  ┌────────────────────────────────────────────────────┐  │  │
│  │  │  runqhead: 0                                       │  │  │
│  │  │  runqtail: 5                                       │  │  │
│  │  │  runq[0]: *G1  ◄─┐                                 │  │  │
│  │  │  runq[1]: *G2    │ Currently queued                │  │  │
│  │  │  runq[2]: *G3    │ goroutines                      │  │  │
│  │  │  runq[3]: *G4    │                                 │  │  │
│  │  │  runq[4]: *G5  ◄─┘                                 │  │  │
│  │  │  runq[5-255]: empty                                │  │  │
│  │  └────────────────────────────────────────────────────┘  │  │
│  │                                                          │  │
│  │  runnext: *G10 (priority, runs next)                     │  │
│  │                                                          │  │
│  │  mcache: *mcache (memory allocation cache)               │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘
```

### P States

```
┌───────────────────────────────────────────────────────────────────────────┐
│                    PROCESSOR STATE MACHINE                                │
└───────────────────────────────────────────────────────────────────────────┘

P States:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

_Pidle     = 0  // Not attached to M, in idle list
_Prunning  = 1  // Attached to M, executing Gs
_Psyscall  = 2  // In syscall, may be taken by another M
_Pgcstop   = 3  // Stopped for GC
_Pdead     = 4  // No longer used (GOMAXPROCS decreased)


State Transitions:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

                    ┌──────────────┐
         Initial →  │   _Pidle     │ ◄─┐
                    └──────┬───────┘   │
                           │           │
                  M picks P│           │P detached
                           │           │from M
                           ↓           │
                    ┌──────────────┐   │
              ┌─────│  _Prunning   │───┘
              │     └──────┬───────┘
              │            │
    GC Stop   │            │Syscall
              │            ↓
              │     ┌──────────────┐
              │     │  _Psyscall   │ ← P may be stolen by
              │     └──────┬───────┘    another M during syscall
              │            │
              │            │Syscall returns
              │            ↓
              │     ┌──────────────┐
              └────→│  _Pgcstop    │ ← Stopped for GC
                    └──────────────┘


Example Scenarios:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Scenario 1: Normal Execution
┌────────────────────────────────────────────────────────────────┐
│  P0 in idle list                                               │
│  M0 needs work                                                 │
│  M0 acquires P0                                                │
│  P0: _Pidle → _Prunning                                        │
│  M0 executes Gs from P0's queue                                │
└────────────────────────────────────────────────────────────────┘

Scenario 2: System Call
┌────────────────────────────────────────────────────────────────┐
│  M0 running with P0                                            │
│  G makes syscall (e.g., file.Read())                           │
│  P0: _Prunning → _Psyscall                                     │
│  M0 blocks in kernel                                           │
│  P0 detached, can be picked by another M                       │
│  M1 acquires P0                                                │
│  P0: _Psyscall → _Prunning (on M1)                             │
│  M1 runs other Gs while M0 blocked                             │
└────────────────────────────────────────────────────────────────┘

Scenario 3: Garbage Collection
┌────────────────────────────────────────────────────────────────┐
│  GC needs to stop the world                                    │
│  All Ps: _Prunning → _Pgcstop                                  │
│  All Ms wait                                                   │
│  GC work performed                                             │
│  All Ps: _Pgcstop → _Prunning                                  │
│  Normal execution resumes                                      │
└────────────────────────────────────────────────────────────────┘
```

### Local Run Queue

```
┌───────────────────────────────────────────────────────────────────────────┐
│                    P's LOCAL RUN QUEUE                                    │
└───────────────────────────────────────────────────────────────────────────┘

Structure: Circular Queue (256 slots)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

┌────────────────────────────────────────────────────────────────┐
│  P's Local Run Queue                                           │
│                                                                │
│  runq[256]: Circular buffer                                    │
│  runqhead:  Index of next G to dequeue                         │
│  runqtail:  Index where next G will be enqueued                │
│  runnext:   Special slot for high-priority G                   │
│                                                                │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Index    0    1    2    3    4    5   ...   255         │  │
│  │          ┌────┬────┬────┬────┬────┬────┬────┬────┐       │  │
│  │  runq:   │ G1 │ G2 │ G3 │ G4 │ G5 │    │    │    │       │  │
│  │          └────┴────┴────┴────┴────┴────┴────┴────┘       │  │
│  │           ▲                   ▲                          │  │
│  │           │                   │                          │  │
│  │       runqhead=0          runqtail=5                     │  │
│  │                                                          │  │
│  │  Count = (runqtail - runqhead) % 256 = 5 Gs              │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                │
│  Special Priority Slot:                                        │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  runnext: G10 ◄─── Runs before queue (priority)          │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘


Operations:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Enqueue (Add G to tail):
┌────────────────────────────────────────────────────────────────┐
│  Before: head=0, tail=5, count=5                               │
│  [G1][G2][G3][G4][G5][  ][  ][  ]                              │
│   ▲                   ▲                                        │
│  head                tail                                      │
│                                                                │
│  Add G6:                                                       │
│  runq[tail] = G6                                               │
│  tail = (tail + 1) % 256 = 6                                   │
│                                                                │
│  After: head=0, tail=6, count=6                                │
│  [G1][G2][G3][G4][G5][G6][  ][  ]                              │
│   ▲                       ▲                                    │
│  head                    tail                                  │
└────────────────────────────────────────────────────────────────┘

Dequeue (Get G from head):
┌────────────────────────────────────────────────────────────────┐
│  Before: head=0, tail=6, count=6                               │
│  [G1][G2][G3][G4][G5][G6][  ][  ]                              │
│   ▲                       ▲                                    │
│  head                    tail                                  │
│                                                                │
│  Get next G:                                                   │
│  g = runq[head]  // g = G1                                     │
│  head = (head + 1) % 256 = 1                                   │
│                                                                │
│  After: head=1, tail=6, count=5                                │
│  [  ][G2][G3][G4][G5][G6][  ][  ]                              │
│       ▲                   ▲                                    │
│      head                tail                                  │
└────────────────────────────────────────────────────────────────┘

Queue Full (256 Gs):
┌────────────────────────────────────────────────────────────────┐
│  When queue full, next G goes to global queue                  │
│                                                                │
│  if count == 256 {                                             │
│      globrunqput(g)  // Add to global run queue                │
│  }                                                             │
└────────────────────────────────────────────────────────────────┘

runnext Priority:
┌────────────────────────────────────────────────────────────────┐
│  When scheduler picks next G:                                  │
│                                                                │
│  if p.runnext != nil {                                         │
│      g = p.runnext      // Run this first                      │
│      p.runnext = nil                                           │
│      return g                                                  │
│  }                                                             │
│  g = runq[head]          // Otherwise from queue               │
└────────────────────────────────────────────────────────────────┘
```

---

## Complete GMP Architecture {#gmp-architecture}

Now let's see how G, M, and P work together in the complete system.

### Full System View

```
┌───────────────────────────────────────────────────────────────────────────┐
│              COMPLETE GMP ARCHITECTURE (8-core system)                    │
└───────────────────────────────────────────────────────────────────────────┘

System Configuration:
  GOMAXPROCS = 8 (number of Ps)
  CPUs = 8 (physical cores)
  Goroutines = 10,000 (active)
  
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

LAYER 1: GOROUTINES (User Level)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌────────────────────────────────────────────────────────────────┐
│  Active Goroutines: 10,000                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Running:    8 (one per P)                               │  │
│  │  Runnable:   2,000 (in run queues)                       │  │
│  │  Waiting:    7,992 (blocked on I/O, channels, etc.)      │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘
                        │
                  Distributed across
                        │
                        ↓
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

LAYER 2: LOCAL RUN QUEUES (Per-P Distribution)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌──────────────────────────────────────────────────────────────────────────┐
│  P0            P1            P2            P3                            │
│  ┌────────┐    ┌────────┐    ┌────────┐    ┌────────┐                    │
│  │250 Gs  │    │250 Gs  │    │250 Gs  │    │250 Gs  │                    │
│  └────────┘    └────────┘    └────────┘    └────────┘                    │
│                                                                          │
│  P4            P5            P6            P7                            │
│  ┌────────┐    ┌────────┐    ┌────────┐    ┌────────┐                    │
│  │250 Gs  │    │250 Gs  │    │250 Gs  │    │250 Gs  │                    │
│  └────────┘    └────────┘    └────────┘    └────────┘                    │
│                                                                          │
│  Total in local queues: 2,000 Gs (250 per P × 8 Ps)                      │
└──────────────────────────────────────────────────────────────────────────┘

Global Run Queue (Overflow):
┌────────────────────────────────────────────────────────────────┐
│  When local queues full (256 limit), overflow here             │
│  Occasionally checked by idle Ps                               │
└────────────────────────────────────────────────────────────────┘
                        │
                 Scheduled onto
                        │
                        ↓
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

LAYER 3: PROCESSORS (Execution Contexts)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌──────────────────────────────────────────────────────────────────────────┐
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐                      │
│  │   P0    │  │   P1    │  │   P2    │  │   P3    │                      │
│  │ Status: │  │ Status: │  │ Status: │  │ Status: │                      │
│  │ Running │  │ Running │  │ Running │  │ Running │                      │
│  │         │  │         │  │         │  │         │                      │
│  │ Queue:  │  │ Queue:  │  │ Queue:  │  │ Queue:  │                      │
│  │ 250 Gs  │  │ 250 Gs  │  │ 250 Gs  │  │ 250 Gs  │                      │
│  │         │  │         │  │         │  │         │                      │
│  │ Current:│  │ Current:│  │ Current:│  │ Current:│                      │
│  │ G1      │  │ G2      │  │ G3      │  │ G4      │                      │
│  └────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘                      │
│       │            │            │            │                           │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐                      │
│  │   P4    │  │   P5    │  │   P6    │  │   P7    │                      │
│  │ Status: │  │ Status: │  │ Status: │  │ Status: │                      │
│  │ Running │  │ Running │  │ Running │  │ Running │                      │
│  │         │  │         │  │         │  │         │                      │
│  │ Queue:  │  │ Queue:  │  │ Queue:  │  │ Queue:  │                      │
│  │ 250 Gs  │  │ 250 Gs  │  │ 250 Gs  │  │ 250 Gs  │                      │
│  │         │  │         │  │         │  │         │                      │
│  │ Current:│  │ Current:│  │ Current:│  │ Current:│                      │
│  │ G5      │  │ G6      │  │ G7      │  │ G8      │                      │
│  └────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘                      │
└───────┼─────────────┼─────────────┼─────────────┼────────────────────────┘
        │             │             │             │
    Attached to   Attached to   Attached to   Attached to
        │             │             │             │
        ↓             ↓             ↓             ↓
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

LAYER 4: MACHINES (OS Threads)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌──────────────────────────────────────────────────────────────────────────┐
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐                      │
│  │   M0    │  │   M1    │  │   M2    │  │   M3    │                      │
│  │ TID:1001│  │ TID:1002│  │ TID:1003│  │ TID:1004│                      │
│  │ P: P0   │  │ P: P1   │  │ P: P2   │  │ P: P3   │                      │
│  │ G: G1   │  │ G: G2   │  │ G: G3   │  │ G: G4   │                      │
│  └────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘                      │
│       │            │            │            │                           │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐                      │
│  │   M4    │  │   M5    │  │   M6    │  │   M7    │                      │
│  │ TID:1005│  │ TID:1006│  │ TID:1007│  │ TID:1008│                      │
│  │ P: P4   │  │ P: P5   │  │ P: P6   │  │ P: P7   │                      │
│  │ G: G5   │  │ G: G6   │  │ G: G7   │  │ G: G8   │                      │
│  └────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘                      │
└───────┼────────────┼─────────────┼─────────────┼─────────────────────────┘
        │            │             │             │
    Runs on       Runs on       Runs on       Runs on
        │             │             │             │
        ↓             ↓             ↓             ↓
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

LAYER 5: PHYSICAL CPUs
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌──────────────────────────────────────────────────────────────────────────┐
│  ┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐                          │
│  │  CPU 0 │  │  CPU 1 │  │  CPU 2 │  │  CPU 3 │                          │
│  │ [EXEC] │  │ [EXEC] │  │ [EXEC] │  │ [EXEC] │                          │
│  │  M0→G1 │  │  M1→G2 │  │  M2→G3 │  │  M3→G4 │                          │
│  └────────┘  └────────┘  └────────┘  └────────┘                          │
│                                                                          │
│  ┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐                          │
│  │  CPU 4 │  │  CPU 5 │  │  CPU 6 │  │  CPU 7 │                          │
│  │ [EXEC] │  │ [EXEC] │  │ [EXEC] │  │ [EXEC] │                          │
│  │  M4→G5 │  │  M5→G6 │  │  M6→G7 │  │  M7→G8 │                          │
│  └────────┘  └────────┘  └────────┘  └────────┘                          │
└──────────────────────────────────────────────────────────────────────────┘

Summary:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
10,000 Goroutines
    ↓ Distributed across
8 Processors (P0-P7)
    ↓ Attached to
8 Machines (M0-M7) (OS threads)
    ↓ Running on
8 CPU Cores

Only 8 goroutines execute simultaneously (one per CPU)
The rest wait in queues or are blocked
Scheduler switches between them rapidly (every ~10ms or on block)
```

This covers M, P, and the complete GMP architecture. Should I continue with CPU-level execution, work stealing, and real-world examples?

## CPU-Level Execution {#cpu-execution}

How goroutines actually execute on physical CPUs.

### Single Goroutine Execution Timeline

```
┌───────────────────────────────────────────────────────────────────────────┐
│         SINGLE GOROUTINE EXECUTION ON CPU (10ms window)                   │
└───────────────────────────────────────────────────────────────────────────┘

Time    CPU State           G State      Stack Pointer    Instruction
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

0.0ms   Context Switch      _Grunnable   0xc000100000    -
        ↓ M loads G's context
0.1ms   Executing           _Grunning    0xc000100100    0x456789
        ├─ ADD instruction
0.2ms   Executing           _Grunning    0xc000100100    0x45678a
        ├─ MOV instruction
0.3ms   Executing           _Grunning    0xc000100110    0x45678b
        ├─ CALL function
1.0ms   Executing           _Grunning    0xc000100200    0x456800
        ├─ Inside function
5.0ms   Channel Receive     _Grunning    0xc000100200    0x456850
        ├─ <-ch blocks
5.1ms   Park Goroutine      _Gwaiting    0xc000100200    0x456850
        ↓ Save context, yield CPU
5.2ms   Scheduler           g0           0xc000200000    schedule()
        ├─ Find next G
5.3ms   Context Switch      _Grunnable   0xc000300000    -
        ↓ Load new G's context
5.4ms   Executing G2        _Grunning    0xc000300100    0x457000
        ... (G1 blocked, not using CPU)

Physical CPU Register State:
┌────────────────────────────────────────────────────────────────┐
│  During G1 execution (0.0-5.0ms):                              │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  RIP (instruction ptr): 0x456789 (G1's code)             │  │
│  │  RSP (stack ptr):       0xc000100100 (G1's stack)        │  │
│  │  RBP (base ptr):        0xc000100050 (G1's frame)        │  │
│  │  RAX, RBX, etc.:        G1's local variables             │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                │
│  During context switch (5.1-5.3ms):                            │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Save G1's registers to g.sched                          │  │
│  │  Load g0's registers (scheduler)                         │  │
│  │  RIP: schedule() function                                │  │
│  │  RSP: g0's stack                                         │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                │
│  During G2 execution (5.4ms+):                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Load G2's registers from g.sched                        │  │
│  │  RIP: 0x457000 (G2's code)                               │  │
│  │  RSP: 0xc000300100 (G2's stack)                          │  │
│  └──────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────┘
```

## Work Stealing Algorithm {#work-stealing}

How idle processors steal work to stay busy.

```
┌───────────────────────────────────────────────────────────────────────────┐
│                    WORK STEALING ALGORITHM                                │
└───────────────────────────────────────────────────────────────────────────┘

Scenario: P0 is overloaded, P7 is idle

Initial State:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

P0: [G1][G2][G3]...[G200]  ← 200 goroutines queued
P1: [G201][G202]...[G250]   ← 50 goroutines
P2-P6: Similar loads
P7: [Empty]                 ← No work!

Step 1: P7's M becomes idle
┌────────────────────────────────────────────────────────────────┐
│  M7 finishes last goroutine                                    │
│  P7's queue is empty                                           │
│  M7 calls findrunnable()                                       │
└────────────────────────────────────────────────────────────────┘

Step 2: Check local queue (empty)
┌────────────────────────────────────────────────────────────────┐
│  P7.runq: empty ✗                                              │
│  P7.runnext: nil ✗                                             │
└────────────────────────────────────────────────────────────────┘

Step 3: Check global queue
┌────────────────────────────────────────────────────────────────┐
│  globrunq: empty (or take one if available)                    │
└────────────────────────────────────────────────────────────────┘

Step 4: Random victim selection
┌────────────────────────────────────────────────────────────────┐
│  Pick random P: P0 (victim)                                    │
│  P0 has 200 Gs in queue                                        │
└────────────────────────────────────────────────────────────────┘

Step 5: Steal half of victim's queue
┌────────────────────────────────────────────────────────────────┐
│  Before:                                                       │
│  P0: [G1][G2][G3]...[G200]     (200 Gs)                        │
│  P7: [Empty]                   (0 Gs)                          │
│                                                                │
│  Steal: 200 / 2 = 100 Gs from tail                             │
│                                                                │
│  After:                                                        │
│  P0: [G1][G2]...[G100]         (100 Gs) ← Kept half            │
│  P7: [G101][G102]...[G200]     (100 Gs) ← Stole half           │
└────────────────────────────────────────────────────────────────┘

Step 6: Execute stolen work
┌────────────────────────────────────────────────────────────────┐
│  P7 now has work!                                              │
│  M7 starts executing G101                                      │
│  System balanced                                               │
└────────────────────────────────────────────────────────────────┘

Complete Algorithm (simplified):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

func findrunnable(p *P) *G {
    // 1. Check local queue (fast path)
    if gp := p.runq.pop(); gp != nil {
        return gp
    }
    
    // 2. Check global queue (1/61 of the time)
    if sched.runqsize > 0 {
        if gp := globrunqget(p, 0); gp != nil {
            return gp
        }
    }
    
    // 3. Try to steal from other Ps
    for i := 0; i < 4; i++ {  // Try 4 times
        victim := random_P()
        if victim == p {
            continue
        }
        
        if gp := runqsteal(p, victim); gp != nil {
            return gp  // Successfully stole work!
        }
    }
    
    // 4. Park M (no work available)
    return nil
}

Visualization of 4 Ps with work stealing:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

T=0ms:  Initial state
┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐
│ P0: 100│  │ P1: 100│  │ P2: 100│  │ P3:   0│ ← Idle
└────────┘  └────────┘  └────────┘  └────────┘

T=1ms:  P3 steals from P0
┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐
│ P0:  50│  │ P1: 100│  │ P2: 100│  │ P3:  50│
└────────┘  └────────┘  └────────┘  └────────┘
                ↑                           ↑
                └───── Stolen 50 ─────────┘

T=10ms: Balanced state
┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐
│ P0:  25│  │ P1:  25│  │ P2:  25│  │ P3:  25│
└────────┘  └────────┘  └────────┘  └────────┘
         All Ps have work, balanced!
```

## Real-World Examples {#examples}

### Example 1: HTTP Server with 1000 Concurrent Requests

```go
func main() {
    http.HandleFunc("/", handler)
    http.ListenAndServe(":8080", nil)
}

func handler(w http.ResponseWriter, r *http.Request) {
    // Each request runs in its own goroutine
    result := queryDatabase() // Blocks on I/O
    w.Write([]byte(result))
}
```

**What happens at GMP level:**

```
System: GOMAXPROCS=8 (8 CPUs)

T=0ms: 1000 requests arrive
┌────────────────────────────────────────────────────────────────┐
│  Go runtime creates 1000 goroutines                            │
│  Each goroutine: ~2KB = 2MB total                              │
│  (Compare: 1000 OS threads = 8GB!)                             │
└────────────────────────────────────────────────────────────────┘

T=1ms: Goroutines distributed
┌────────────────────────────────────────────────────────────────┐
│  P0-P7 each get ~125 goroutines in local queues                │
│  8 goroutines running (one per P)                              │
│  992 goroutines waiting in queues                              │
└────────────────────────────────────────────────────────────────┘

T=5ms: Database queries start
┌────────────────────────────────────────────────────────────────┐
│  G1 calls queryDatabase()                                      │
│  Blocks on network I/O                                         │
│  State: _Grunning → _Gwaiting                                  │
│  M0 immediately switches to G2 from P0's queue                 │
│  CPU never idle!                                               │
└────────────────────────────────────────────────────────────────┘

T=10ms: High concurrency
┌────────────────────────────────────────────────────────────────┐
│  Running: 8 (on CPUs)                                          │
│  Runnable: 200 (waiting for CPU)                               │
│  Waiting: 792 (blocked on DB I/O)                              │
│                                                                │
│  CPU usage: ~90% (processing + context switching)              │
│  Throughput: ~100 requests/sec                                 │
└────────────────────────────────────────────────────────────────┘

T=100ms: DB responses return
┌────────────────────────────────────────────────────────────────┐
│  As DB calls complete, Gs wake up:                             │
│  _Gwaiting → _Grunnable                                        │
│  Added back to run queues                                      │
│  Processed in turn                                             │
└────────────────────────────────────────────────────────────────┘

Result: All 1000 requests handled efficiently with just 8 OS threads!
```

### Example 2: Channel Communication

```go
func main() {
    ch := make(chan int)
    
    // Producer
    go func() {
        ch <- 42
    }()
    
    // Consumer
    go func() {
        val := <-ch
        fmt.Println(val)
    }()
    
    time.Sleep(time.Second)
}
```

**GMP execution:**

```
T=0ms: Create channel and goroutines
┌────────────────────────────────────────────────────────────────┐
│  Channel created: make(chan int)                               │
│  G1 (producer): go func() { ch <- 42 }                         │
│  G2 (consumer): go func() { val := <-ch }                      │
│                                                                │
│  Both added to P0's run queue                                  │
└────────────────────────────────────────────────────────────────┘

T=1ms: Consumer starts first (arbitrary)
┌────────────────────────────────────────────────────────────────┐
│  G2 scheduled on M0                                            │
│  G2 executes: val := <-ch                                      │
│  Channel empty! G2 blocks                                      │
│                                                                │
│  Actions:                                                      │
│  1. G2 added to channel's wait queue                           │
│  2. G2 state: _Grunning → _Gwaiting                            │
│  3. M0 switches to next runnable G                             │
└────────────────────────────────────────────────────────────────┘

T=2ms: Producer runs
┌────────────────────────────────────────────────────────────────┐
│  G1 scheduled on M0                                            │
│  G1 executes: ch <- 42                                         │
│                                                                │
│  Actions:                                                      │
│  1. Check channel's wait queue                                 │
│  2. Find G2 waiting!                                           │
│  3. Transfer value directly: G1 → G2 (no buffer copy)          │
│  4. Wake G2: _Gwaiting → _Grunnable                            │
│  5. Add G2 to run queue                                        │
│  6. G1 continues                                               │
└────────────────────────────────────────────────────────────────┘

T=3ms: Consumer resumes
┌────────────────────────────────────────────────────────────────┐
│  G2 scheduled again                                            │
│  G2 has value 42                                               │
│  G2 executes: fmt.Println(42)                                  │
│  G2 completes                                                  │
└────────────────────────────────────────────────────────────────┘

Memory synchronization happens automatically!
```

### Example 3: CPU-Bound vs I/O-Bound

```go
// CPU-bound
func cpuBound() {
    for i := 0; i < 1000000000; i++ {
        _ = math.Sqrt(float64(i))
    }
}

// I/O-bound
func ioBound() {
    time.Sleep(100 * time.Millisecond)
}
```

**Execution comparison:**

```
Scenario: 100 goroutines on 8-core system

CPU-Bound (cpuBound):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌────────────────────────────────────────────────────────────────┐
│  Running: 8 (saturating all CPUs)                              │
│  Runnable: 92 (waiting for CPU)                                │
│  Waiting: 0 (no blocking)                                      │
│                                                                │
│  CPU Usage: 100%                                               │
│  Time per goroutine: ~125 seconds (1000s / 8 CPUs)             │
│  Total time: ~125 seconds                                      │
│                                                                │
│  Limited by CPU availability                                   │
└────────────────────────────────────────────────────────────────┘

I/O-Bound (ioBound):
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
┌────────────────────────────────────────────────────────────────┐
│  Running: 0 (all sleeping)                                     │
│  Runnable: 0 (none ready)                                      │
│  Waiting: 100 (all in time.Sleep)                              │
│                                                                │
│  CPU Usage: 0%                                                 │
│  Time per goroutine: 100ms                                     │
│  Total time: ~100ms (all sleep concurrently!)                  │
│                                                                │
│  Not limited by CPU - limited by I/O wait                      │
└────────────────────────────────────────────────────────────────┘

This is why Go can handle 10,000s of I/O-bound goroutines easily!
```

This completes the comprehensive GMP model guide with deep dives into CPU and memory level execution, extensive visualizations, and real-world examples!