# Rust Concurrency: A Comprehensive Guide

## Table of Contents

1. Introduction to Concurrency in Rust
2. Threads and Thread Safety
3. Send and Sync Traits
4. Mutex and RwLock
5. Channels for Message Passing
6. Async/Await Fundamentals
7. Runtime Systems: Tokio and async-std
8. Real-World Backend Development
9. Best Practices and Patterns

---

## Chapter 1: Introduction to Concurrency in Rust

Concurrency is the ability of a program to execute multiple tasks simultaneously or in overlapping time periods. Rust approaches concurrency with a unique philosophy: **fearless concurrency**. This means the compiler catches many concurrency bugs at compile time, preventing data races and undefined behavior that plague other languages.

Rust achieves this through its ownership system, type system, and two fundamental traits: `Send` and `Sync`. Before diving into specific mechanisms, it's essential to understand that Rust's concurrency guarantees are enforced at compile time, not runtime. This is revolutionary—you cannot compile code with data races.

---

## Chapter 2: Threads and Thread Safety

### Understanding Threads

A thread is an independent execution context within a process. Threads share the same memory space but execute independently. In Rust, threads are created using `std::thread::spawn`.

### Basic Thread Creation

```rust
use std::thread;
use std::time::Duration;

fn main() {
    // Spawning a new thread
    let handle = thread::spawn(|| {
        for i in 1..10 {
            println!("Number {} from spawned thread", i);
            thread::sleep(Duration::from_millis(1));
        }
    });

    // Main thread continues executing
    for i in 1..5 {
        println!("Number {} from main thread", i);
        thread::sleep(Duration::from_millis(1));
    }

    // Wait for the spawned thread to finish
    handle.join().unwrap();
}
```

**Deep Explanation:**

- `thread::spawn` takes a closure (an anonymous function) and executes it in a new thread
- The function returns a `JoinHandle`, which represents the spawned thread
- `handle.join()` blocks the current thread until the spawned thread completes
- `.unwrap()` handles the `Result` that `join()` returns (it could fail if the thread panicked)
- Without `join()`, the main thread might terminate before the spawned thread completes, killing the spawned thread prematurely

### Moving Data into Threads

```rust
use std::thread;

fn main() {
    let data = vec![1, 2, 3, 4, 5];

    // The 'move' keyword transfers ownership to the thread
    let handle = thread::spawn(move || {
        println!("Vector from thread: {:?}", data);
        let sum: i32 = data.iter().sum();
        sum // Return value from thread
    });

    // data is no longer accessible here because ownership was moved
    // println!("{:?}", data); // This would cause a compile error

    // Retrieve the result from the thread
    let result = handle.join().unwrap();
    println!("Sum calculated in thread: {}", result);
}
```

**Deep Explanation:**

- The `move` keyword forces the closure to take ownership of captured variables
- Without `move`, the closure would try to borrow `data`, but the thread might outlive the main function
- Rust prevents this potential dangling reference at compile time
- The thread can return a value, which is retrieved via `join()`
- This ensures memory safety: the thread owns its data, preventing race conditions

### Thread Safety Concerns

```rust
use std::thread;
use std::rc::Rc;

fn main() {
    let data = Rc::new(vec![1, 2, 3]);

    // This will NOT compile!
    // let handle = thread::spawn(move || {
    //     println!("{:?}", data);
    // });

    // Rc is not thread-safe because it uses non-atomic reference counting
}
```

**Deep Explanation:**

- `Rc` (Reference Counted) is a single-threaded reference counting pointer
- It cannot be sent between threads because incrementing/decrementing its counter isn't atomic
- If multiple threads modified the count simultaneously, corruption would occur
- Rust's type system prevents this at compile time through the `Send` trait (covered next)
- For thread-safe reference counting, use `Arc` (Atomic Reference Counted)

---

## Chapter 3: Send and Sync Traits

These are marker traits that form the foundation of Rust's thread safety guarantees.

### The Send Trait

`Send` indicates that ownership of a type can be transferred between threads.

```rust
use std::thread;
use std::sync::Arc;

fn main() {
    // String is Send - can be moved between threads
    let text = String::from("Hello from thread");

    let handle = thread::spawn(move || {
        println!("{}", text);
    });

    handle.join().unwrap();

    // Arc is Send - can be shared between threads
    let shared_data = Arc::new(vec![1, 2, 3, 4, 5]);
    let shared_clone = Arc::clone(&shared_data);

    let handle2 = thread::spawn(move || {
        println!("Shared data: {:?}", shared_clone);
    });

    println!("Original data: {:?}", shared_data);
    handle2.join().unwrap();
}
```

**Deep Explanation:**

- Most types in Rust are `Send` by default
- Types that aren't `Send` include `Rc`, raw pointers, and types containing non-`Send` fields
- `Arc` (Atomic Reference Counted) is `Send` because it uses atomic operations for thread-safe reference counting
- When you `Arc::clone()`, you create a new reference to the same data, incrementing the atomic counter
- Each thread can own an `Arc` pointing to the same underlying data

### The Sync Trait

`Sync` indicates that a type is safe to reference from multiple threads. A type `T` is `Sync` if `&T` (a shared reference) is `Send`.

```rust
use std::sync::Arc;
use std::thread;

fn main() {
    // i32 is Sync - immutable references can be shared between threads
    let number = Arc::new(42);
    let mut handles = vec![];

    for _ in 0..5 {
        let number_clone = Arc::clone(&number);
        let handle = thread::spawn(move || {
            // Multiple threads reading the same data safely
            println!("Thread sees number: {}", number_clone);
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.join().unwrap();
    }
}
```

**Deep Explanation:**

- `Sync` means multiple threads can hold shared references (`&T`) safely
- Most immutable types are `Sync`
- `Cell` and `RefCell` are not `Sync` because they provide interior mutability without thread safety
- The compiler automatically derives `Send` and `Sync` for your types based on their fields
- You rarely implement these traits manually; the compiler does it for you

### Types That Are Neither Send Nor Sync

```rust
use std::rc::Rc;
use std::cell::RefCell;

fn demonstrate_non_send_sync() {
    // Rc is not Send (can't move between threads)
    let rc_data = Rc::new(5);

    // RefCell is not Sync (can't share references between threads)
    let ref_cell_data = RefCell::new(10);

    // These cannot be used across threads:
    // thread::spawn(move || println!("{}", rc_data)); // Won't compile
    // thread::spawn(|| println!("{}", ref_cell_data.borrow())); // Won't compile
}
```

**Deep Explanation:**

- `Rc` uses non-atomic reference counting, so it's not `Send`
- `RefCell` provides runtime-checked borrowing but isn't thread-safe, so it's not `Sync`
- These types are optimized for single-threaded use
- For multi-threaded equivalents, use `Arc` and `Mutex`/`RwLock`

---

## Chapter 4: Mutex and RwLock

### Mutex: Mutual Exclusion

A `Mutex` (mutual exclusion) ensures only one thread can access data at a time.

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn main() {
    // Create a counter protected by a Mutex
    let counter = Arc::new(Mutex::new(0));
    let mut handles = vec![];

    for _ in 0..10 {
        let counter_clone = Arc::clone(&counter);

        let handle = thread::spawn(move || {
            // Lock the mutex to get access to the data
            let mut num = counter_clone.lock().unwrap();

            // Increment the counter
            *num += 1;

            // The lock is automatically released when 'num' goes out of scope
        });

        handles.push(handle);
    }

    // Wait for all threads to complete
    for handle in handles {
        handle.join().unwrap();
    }

    // Print the final result
    println!("Final count: {}", *counter.lock().unwrap());
}
```

**Deep Explanation:**

- `Mutex::new(0)` creates a mutex protecting the value `0`
- `Arc::new()` wraps the mutex so multiple threads can own references to it
- `lock()` acquires the mutex, blocking if another thread holds it
- `lock()` returns a `LockResult<MutexGuard>`, which we unwrap
- `MutexGuard` is a smart pointer that dereferences to the protected data
- When `MutexGuard` goes out of scope, it automatically releases the lock (RAII pattern)
- This prevents deadlocks from forgetting to unlock

### Handling Mutex Poisoning

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn main() {
    let data = Arc::new(Mutex::new(0));
    let data_clone = Arc::clone(&data);

    let handle = thread::spawn(move || {
        let mut num = data_clone.lock().unwrap();
        *num += 1;

        // Simulate a panic while holding the lock
        panic!("Thread panicked!");

        // The lock would be released here, but we panicked
    });

    // Thread panics, so join returns an Err
    let _ = handle.join();

    // The mutex is now "poisoned"
    match data.lock() {
        Ok(guard) => println!("Got lock: {}", *guard),
        Err(poisoned) => {
            println!("Mutex was poisoned!");
            // We can still access the data if needed
            let guard = poisoned.into_inner();
            println!("Value despite poison: {}", *guard);
        }
    }
}
```

**Deep Explanation:**

- When a thread panics while holding a mutex lock, the mutex becomes "poisoned"
- Poisoning indicates that the protected data might be in an inconsistent state
- Subsequent `lock()` calls return `Err(PoisonError)`
- You can choose to handle the poison or call `into_inner()` to access the data anyway
- This is a safety mechanism to alert you of potential corruption

### RwLock: Read-Write Lock

`RwLock` allows multiple concurrent readers OR one writer, but not both simultaneously.

```rust
use std::sync::{Arc, RwLock};
use std::thread;
use std::time::Duration;

fn main() {
    let data = Arc::new(RwLock::new(vec![1, 2, 3, 4, 5]));
    let mut handles = vec![];

    // Spawn 5 reader threads
    for i in 0..5 {
        let data_clone = Arc::clone(&data);
        let handle = thread::spawn(move || {
            // Multiple threads can hold read locks simultaneously
            let read_guard = data_clone.read().unwrap();
            println!("Reader {} sees: {:?}", i, *read_guard);
            thread::sleep(Duration::from_millis(100));
            // Read lock released here
        });
        handles.push(handle);
    }

    // Spawn 1 writer thread
    let data_clone = Arc::clone(&data);
    let writer = thread::spawn(move || {
        thread::sleep(Duration::from_millis(50));

        // Write lock waits for all read locks to be released
        let mut write_guard = data_clone.write().unwrap();
        write_guard.push(6);
        println!("Writer added element");
        // Write lock released here
    });
    handles.push(writer);

    for handle in handles {
        handle.join().unwrap();
    }

    println!("Final data: {:?}", data.read().unwrap());
}
```

**Deep Explanation:**

- `read()` acquires a read lock, which can be held by multiple threads simultaneously
- `write()` acquires a write lock, which is exclusive (no other readers or writers)
- `RwLock` is more efficient than `Mutex` when you have many readers and few writers
- The lock acquisition follows priority rules (implementation-dependent, may favor readers or writers)
- Like `Mutex`, locks are automatically released when guards go out of scope
- `RwLock` can also be poisoned like `Mutex`

### Choosing Between Mutex and RwLock

```rust
use std::sync::{Arc, Mutex, RwLock};
use std::thread;
use std::time::Instant;

fn main() {
    // Benchmark: Read-heavy workload
    let mutex_data = Arc::new(Mutex::new(0));
    let rwlock_data = Arc::new(RwLock::new(0));

    // Test Mutex
    let start = Instant::now();
    let mut handles = vec![];

    for _ in 0..100 {
        let data = Arc::clone(&mutex_data);
        handles.push(thread::spawn(move || {
            for _ in 0..1000 {
                let _val = *data.lock().unwrap(); // Just reading
            }
        }));
    }

    for handle in handles {
        handle.join().unwrap();
    }
    println!("Mutex time: {:?}", start.elapsed());

    // Test RwLock
    let start = Instant::now();
    let mut handles = vec![];

    for _ in 0..100 {
        let data = Arc::clone(&rwlock_data);
        handles.push(thread::spawn(move || {
            for _ in 0..1000 {
                let _val = *data.read().unwrap(); // Just reading
            }
        }));
    }

    for handle in handles {
        handle.join().unwrap();
    }
    println!("RwLock time: {:?}", start.elapsed());
}
```

**Deep Explanation:**

- Use `Mutex` for write-heavy workloads or when the critical section is very short
- Use `RwLock` for read-heavy workloads where reads significantly outnumber writes
- `RwLock` has more overhead than `Mutex` due to tracking multiple readers
- For very short critical sections, `Mutex` might be faster even with mostly reads
- Always benchmark your specific use case

---

## Chapter 5: Channels for Message Passing

Channels enable communication between threads by sending messages. This follows the principle: "Don't communicate by sharing memory; share memory by communicating."

### MPSC: Multiple Producer, Single Consumer

```rust
use std::sync::mpsc;
use std::thread;
use std::time::Duration;

fn main() {
    // Create a channel
    let (tx, rx) = mpsc::channel();

    // Spawn a thread that sends messages
    thread::spawn(move || {
        let messages = vec![
            String::from("hello"),
            String::from("from"),
            String::from("the"),
            String::from("thread"),
        ];

        for msg in messages {
            // Send message through the channel
            tx.send(msg).unwrap();
            thread::sleep(Duration::from_millis(500));
        }

        // tx is dropped here, closing the channel
    });

    // Receive messages in the main thread
    for received in rx {
        println!("Got: {}", received);
    }

    println!("All messages received");
}
```

**Deep Explanation:**

- `mpsc::channel()` creates a transmitter (`tx`) and receiver (`rx`)
- The transmitter is moved into the spawned thread
- `send()` sends a message, transferring ownership to the channel
- The receiver acts as an iterator; it blocks waiting for messages
- When all transmitters are dropped, the channel closes and the iteration ends
- This pattern prevents deadlocks: you can't forget to send or receive

### Multiple Producers

```rust
use std::sync::mpsc;
use std::thread;

fn main() {
    let (tx, rx) = mpsc::channel();

    // Clone the transmitter for multiple producers
    for i in 0..5 {
        let tx_clone = tx.clone();

        thread::spawn(move || {
            let msg = format!("Message from thread {}", i);
            tx_clone.send(msg).unwrap();
        });
    }

    // Drop the original transmitter
    drop(tx);

    // Receive messages from all threads
    for received in rx {
        println!("Got: {}", received);
    }
}
```

**Deep Explanation:**

- `tx.clone()` creates multiple transmitters for the same channel
- Each thread gets its own transmitter
- We must drop the original `tx` in the main thread; otherwise, the channel never closes
- The channel closes only when ALL transmitters are dropped
- Messages arrive in the order threads send them (non-deterministic)

### Bounded Channels (Sync Channel)

```rust
use std::sync::mpsc;
use std::thread;
use std::time::Duration;

fn main() {
    // Create a channel with a buffer size of 2
    let (tx, rx) = mpsc::sync_channel(2);

    thread::spawn(move || {
        for i in 0..5 {
            println!("Sending message {}", i);
            tx.send(i).unwrap();
            println!("Message {} sent", i);
        }
    });

    thread::sleep(Duration::from_secs(1));
    println!("Starting to receive...");

    for msg in rx {
        println!("Received: {}", msg);
        thread::sleep(Duration::from_millis(500));
    }
}
```

**Deep Explanation:**

- `sync_channel(2)` creates a bounded channel with capacity 2
- `send()` blocks when the buffer is full, applying backpressure
- This prevents unbounded memory growth when the producer is faster than the consumer
- Unbounded channels (`mpsc::channel()`) buffer messages indefinitely
- Use bounded channels when you need flow control or predictable memory usage

### Using Channels for Results

```rust
use std::sync::mpsc;
use std::thread;

#[derive(Debug)]
struct ComputationResult {
    thread_id: usize,
    result: i32,
}

fn main() {
    let (tx, rx) = mpsc::channel();
    let numbers = vec![1, 2, 3, 4, 5, 6, 7, 8];

    // Spawn threads to compute squares
    for (id, num) in numbers.into_iter().enumerate() {
        let tx_clone = tx.clone();

        thread::spawn(move || {
            let square = num * num;

            let result = ComputationResult {
                thread_id: id,
                result: square,
            };

            tx_clone.send(result).unwrap();
        });
    }

    drop(tx);

    // Collect results
    let mut results: Vec<ComputationResult> = rx.iter().collect();
    results.sort_by_key(|r| r.thread_id);

    for result in results {
        println!("Thread {} computed: {}", result.thread_id, result.result);
    }
}
```

**Deep Explanation:**

- Channels are excellent for collecting results from multiple threads
- We send structured data (a struct) through the channel
- Results arrive in non-deterministic order, so we sort them afterward
- This pattern is common in parallel computations: map-reduce style
- The channel handles synchronization; we don't need explicit locks

---

## Chapter 6: Async/Await Fundamentals

Async programming enables concurrent I/O operations without threads, using cooperative multitasking.

### Understanding Async vs Threads

**Threads:**

- True parallelism (on multi-core systems)
- Pre-emptive multitasking (OS schedules threads)
- Heavy weight (~2MB stack per thread)
- Good for CPU-bound tasks

**Async:**

- Concurrency without parallelism (unless combined with threads)
- Cooperative multitasking (tasks yield control)
- Lightweight (futures are small)
- Excellent for I/O-bound tasks

### Basic Async Function

```rust
use std::time::Duration;
use tokio::time::sleep;

// An async function returns a Future
async fn say_hello() {
    println!("Hello");

    // Await pauses this function and yields control
    sleep(Duration::from_secs(1)).await;

    println!("World");
}

#[tokio::main]
async fn main() {
    // Calling an async function returns a Future but doesn't execute it
    let future = say_hello();

    // .await actually executes the Future
    future.await;
}
```

**Deep Explanation:**

- `async fn` declares an asynchronous function that returns a `Future`
- A `Future` is a value that will be available in the future
- Calling `say_hello()` doesn't execute the function; it returns a `Future`
- `.await` drives the `Future` to completion, blocking only this async task
- `#[tokio::main]` creates a Tokio runtime to execute async code
- When `.await` is called on `sleep()`, the task yields control to the runtime

### Futures and Poll

```rust
use std::future::Future;
use std::pin::Pin;
use std::task::{Context, Poll};

// A simple custom Future
struct TimerFuture {
    completed: bool,
}

impl Future for TimerFuture {
    type Output = String;

    fn poll(mut self: Pin<&mut Self>, _cx: &mut Context<'_>) -> Poll<Self::Output> {
        if self.completed {
            // Future is ready
            Poll::Ready(String::from("Timer completed!"))
        } else {
            // Simulate work
            self.completed = true;
            // Not ready yet, will be polled again
            Poll::Pending
        }
    }
}

#[tokio::main]
async fn main() {
    let future = TimerFuture { completed: false };
    let result = future.await;
    println!("{}", result);
}
```

**Deep Explanation:**

- Futures are state machines that implement the `Future` trait
- The runtime repeatedly calls `poll()` to check if the future is ready
- `Poll::Pending` means the future isn't complete; the runtime will poll again
- `Poll::Ready(value)` means the future completed and returns its value
- This is how `.await` works under the hood: it polls the future until ready
- The `Context` contains a `Waker` that tells the runtime when to poll again

### Concurrent Async Operations

```rust
use tokio::time::{sleep, Duration};

async fn task_one() -> String {
    sleep(Duration::from_secs(2)).await;
    String::from("Task One Complete")
}

async fn task_two() -> String {
    sleep(Duration::from_secs(1)).await;
    String::from("Task Two Complete")
}

async fn task_three() -> String {
    sleep(Duration::from_millis(500)).await;
    String::from("Task Three Complete")
}

#[tokio::main]
async fn main() {
    let start = std::time::Instant::now();

    // Sequential execution - slow
    let result1 = task_one().await;
    let result2 = task_two().await;
    let result3 = task_three().await;

    println!("Sequential: {} {} {}", result1, result2, result3);
    println!("Time: {:?}", start.elapsed());

    let start = std::time::Instant::now();

    // Concurrent execution - fast
    let (result1, result2, result3) = tokio::join!(
        task_one(),
        task_two(),
        task_three()
    );

    println!("Concurrent: {} {} {}", result1, result2, result3);
    println!("Time: {:?}", start.elapsed());
}
```

**Deep Explanation:**

- Sequential `.await` calls block each other: total time is the sum of all operations
- `tokio::join!` runs multiple futures concurrently
- While one future is waiting (e.g., for network I/O), others can progress
- All futures complete when the slowest one finishes
- This is cooperative concurrency: futures yield control when they `.await`
- No additional threads are created; one thread handles all futures

### Selecting Between Futures

```rust
use tokio::time::{sleep, Duration};
use tokio::select;

async fn operation_one() -> &'static str {
    sleep(Duration::from_secs(2)).await;
    "Operation One"
}

async fn operation_two() -> &'static str {
    sleep(Duration::from_secs(1)).await;
    "Operation Two"
}

#[tokio::main]
async fn main() {
    // Run both operations, return when the first completes
    select! {
        result = operation_one() => {
            println!("First to complete: {}", result);
        }
        result = operation_two() => {
            println!("First to complete: {}", result);
        }
    }

    // The other operation is cancelled
}
```

**Deep Explanation:**

- `select!` races multiple futures and proceeds with the first to complete
- Other futures are cancelled (dropped) when one completes
- This is useful for timeouts, cancellation, or racing multiple strategies
- Each branch can have different types; the macro handles the complexity
- Select is like a non-deterministic choice: whichever completes first wins

---

## Chapter 7: Runtime Systems - Tokio and async-std

### Understanding the Runtime

Async Rust requires a runtime—an executor that polls futures and manages I/O.

### Tokio: The Most Popular Runtime

```rust
use tokio::runtime::Runtime;

fn main() {
    // Create a runtime manually
    let runtime = Runtime::new().unwrap();

    // Execute an async block
    runtime.block_on(async {
        println!("Running in Tokio runtime");

        let result = async_computation().await;
        println!("Result: {}", result);
    });
}

async fn async_computation() -> i32 {
    // Simulate async work
    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
    42
}
```

**Deep Explanation:**

- The runtime manages a thread pool and an event loop
- `block_on()` blocks the current thread until the future completes
- Inside the runtime, async operations can progress concurrently
- The runtime handles scheduling: which future to poll and when
- Tokio uses epoll (Linux), kqueue (BSD), or IOCP (Windows) for efficient I/O

### Multi-threaded Runtime

```rust
use tokio::task;
use std::time::Duration;

#[tokio::main(flavor = "multi_thread", worker_threads = 4)]
async fn main() {
    let mut handles = vec![];

    for i in 0..10 {
        // Spawn async tasks onto the runtime's thread pool
        let handle = task::spawn(async move {
            println!("Task {} starting on thread {:?}", i, std::thread::current().id());

            tokio::time::sleep(Duration::from_millis(100)).await;

            println!("Task {} finished", i);
            i * 2
        });

        handles.push(handle);
    }

    // Wait for all tasks to complete
    for handle in handles {
        let result = handle.await.unwrap();
        println!("Got result: {}", result);
    }
}
```

**Deep Explanation:**

- `#[tokio::main(flavor = "multi_thread")]` creates a work-stealing thread pool
- `task::spawn()` schedules an async task on the runtime
- Tasks can run on any thread in the pool (work-stealing for load balancing)
- Each task is lightweight—thousands can run on a few threads
- Unlike OS threads, task switching has minimal overhead
- This combines the benefits of parallelism (multiple cores) and concurrency (many tasks)

### Current Thread Runtime

```rust
#[tokio::main(flavor = "current_thread")]
async fn main() {
    println!("Running on single thread: {:?}", std::thread::current().id());

    let mut handles = vec![];

    for i in 0..5 {
        let handle = tokio::task::spawn(async move {
            println!("Task {} on thread {:?}", i, std::thread::current().id());
            tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.await.unwrap();
    }
}
```

**Deep Explanation:**

- `current_thread` runtime uses only one thread
- All async tasks run on that single thread
- This is more efficient for I/O-bound workloads with low CPU usage
- Less overhead than multi-threaded runtime (no work-stealing complexity)
- Good for embedded systems or when you want predictable execution
- Still achieves concurrency through cooperative multitasking

### Async-std: The Alternative

```rust
use async_std::task;
use async_std::io::timeout;
use std::time::Duration;

fn main() {
    task::block_on(async {
        let result = timeout(Duration::from_secs(2), long_operation()).await;

        match result {
            Ok(value) => println!("Completed: {}", value),
            Err(_) => println!("Operation timed out"),
        }
    });
}

async fn long_operation() -> String {
    async_std::task::sleep(Duration::from_secs(1)).await;
    String::from("Done!")
}
```

**Deep Explanation:**

- `async-std` aims to mirror the std library API but with async versions
- `task::block_on()` is the equivalent of Tokio's `block_on()`
- `async-std` has a more minimalist philosophy than Tokio
- It focuses on ergonomics: making async feel like sync code
- Choose async-std if you want simpler API or prefer its design philosophy
- Tokio has a larger ecosystem and more features

### Spawning Blocking Operations

```rust
use tokio::task;
use std::time::Duration;

#[tokio::main]
async fn main() {
    let handle = task::spawn_blocking(|| {
        // This is CPU-intensive or blocking work
        println!("Blocking operation on thread {:?}", std::thread::current().id());

        // Simulate heavy computation
        std::thread::sleep(Duration::from_secs(2));

        "Blocking result"
    });

    println!("Main async context continues...");

    // Await the blocking operation
    let result = handle.await.unwrap();
    println!("Got: {}", result);
}
```

**Deep Explanation:**

- `spawn_blocking()` runs blocking code on a dedicated thread pool
- This prevents blocking operations from starving the async runtime
- Tokio maintains separate thread pools: one for async tasks, one for blocking
- Use this for file I/O, synchronous database queries, or CPU-bound work
- The blocking pool dynamically grows/shrinks based on demand
- This lets you mix sync and async code without blocking async tasks

---

## Chapter 8: Real-World Backend Development

### Building a Concurrent Web Server

```rust
use tokio::net::{TcpListener, TcpStream};
use tokio::io::{AsyncReadExt, AsyncWriteExt};
use std::error::Error;

#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    // Bind to a TCP port
    let listener = TcpListener::bind("127.0.0.1:8080").await?;
    println!("Server listening on port 8080");

    loop {
        // Accept incoming connections
        let (socket, addr) = listener.accept().await?;
        println!("New connection from: {}", addr);

        // Spawn a new task for each connection
        tokio::spawn(async move {
            if let Err(e) = handle_connection(socket).await {
                eprintln!("Error handling connection: {}", e);
            }
        });
    }
}

async fn handle_connection(mut socket: TcpStream) -> Result<(), Box<dyn Error>> {
    let mut buffer = [0; 1024];

    // Read data from the socket
    let n = socket.read(&mut buffer).await?;

    if n == 0 {
        return Ok(()); // Connection closed
    }

    let request = String::from_utf8_lossy(&buffer[..n]);
    println!("Received: {}", request);

    // Send response
    let response = "HTTP/1.1 200 OK\r\n\r\nHello, World!";
    socket.write_all(response.as_bytes()).await?;
    socket.flush().await?;

    Ok(())
}
```

**Deep Explanation:**

- `TcpListener::bind()` creates an async TCP listener
- `accept()` asynchronously waits for incoming connections
- Each connection is handled in a separate spawned task
- Thousands of connections can be handled concurrently on few threads
- `AsyncReadExt` and `AsyncWriteExt` provide async I/O operations
- Each spawned task runs independently; slow clients don't block fast ones
- The runtime schedules tasks efficiently across available threads

### Database Connection Pooling

```rust
use sqlx::{PgPool, postgres::PgPoolOptions};
use tokio::time::{sleep, Duration};

async fn run_queries(pool: PgPool) -> Result<(), sqlx::Error> {
    let mut handles = vec![];

    // Spawn 10 concurrent queries
    for i in 0..10 {
        let pool_clone = pool.clone();

        let handle = tokio::spawn(async move {
            // Acquire a connection from the pool
            let mut conn = pool_clone.acquire().await?;

            // Execute query
            let result: (i32,) = sqlx::query_as("SELECT $1::int")
                .bind(i)
                .fetch_one(&mut *conn)
                .await?;

            println!("Query {} returned: {}", i, result.0);

            Ok::<_, sqlx::Error>(result.0)
            // Connection automatically returned to pool when dropped
        });

        handles.push(handle);
    }

    // Wait for all queries
    for handle in handles {
        handle.await.unwrap()?;
    }

    Ok(())
}

#[tokio::main]
async fn main() -> Result<(), sqlx::Error> {
    // Create a connection pool
    let pool = PgPoolOptions::new()
        .max_connections(5)  // Maximum 5 database connections
        .acquire_timeout(Duration::from_secs(3))
        .connect("postgres://user:pass@localhost/db")
        .await?;

    run_queries(pool).await?;

    Ok(())
}
```

**Deep Explanation:**

- Connection pools reuse database connections, avoiding expensive connection setup
- `max_connections(5)` limits concurrent database connections to 5
- When more than 5 tasks need connections, they wait in a queue
- `acquire()` gets a connection from the pool, blocking asynchronously if none available
- The connection guard automatically returns the connection when dropped
- This prevents overwhelming the database with connections
- Pool cloning is cheap: it just clones a reference to the shared pool
- sqlx provides compile-time checked queries for safety

### Shared State with Arc and RwLock

```rust
use tokio::sync::RwLock;
use std::sync::Arc;
use std::collections::HashMap;
use tokio::time::{sleep, Duration};

#[derive(Debug, Clone)]
struct User {
    id: u64,
    name: String,
    email: String,
}

type UserDb = Arc<RwLock<HashMap<u64, User>>>;

async fn add_user(db: UserDb, user: User) {
    let mut users = db.write().await;
    users.insert(user.id, user);
    println!("User added, total users: {}", users.len());
}

async fn get_user(db: UserDb, id: u64) -> Option<User> {
    let users = db.read().await;
    users.get(&id).cloned()
}

async fn list_users(db: UserDb) {
    let users = db.read().await;
    println!("Current users: {}", users.len());
    for user in users.values() {
        println!("  - {} ({})", user.name, user.email);
    }
}

#[tokio::main]
async fn main() {
    let db: UserDb = Arc::new(RwLock::new(HashMap::new()));

    // Spawn multiple tasks that modify the database
    let mut handles = vec![];

    // Writers
    for i in 0..5 {
        let db_clone = Arc::clone(&db);
        let handle = tokio::spawn(async move {
            let user = User {
                id: i,
                name: format!("User {}", i),
                email: format!("user{}@example.com", i),
            };
            add_user(db_clone, user).await;
        });
        handles.push(handle);
    }

    // Readers
    for i in 0..10 {
        let db_clone = Arc::clone(&db);
        let handle = tokio::spawn(async move {
            sleep(Duration::from_millis(50)).await;
            if let Some(user) = get_user(db_clone.clone(), i % 5).await {
                println!("Reader {} found: {:?}", i, user);
            }
            list_users(db_clone).await;
        });
        handles.push(handle);
    }

    // Wait for all tasks
    for handle in handles {
        handle.await.unwrap();
    }

    println!("\nFinal database state:");
    list_users(db).await;
}
```

**Deep Explanation:**

- `tokio::sync::RwLock` is the async version of `std::sync::RwLock`
- `.read().await` and `.write().await` are async operations that yield to the runtime
- This prevents blocking the runtime thread while waiting for locks
- Multiple readers can hold locks concurrently for efficient read-heavy workloads
- Writers get exclusive access, waiting for all readers to finish
- `Arc` allows multiple tasks to share ownership of the RwLock
- This pattern is common for in-memory caches, configuration, or session stores

### Rate Limiting with Tokio

```rust
use tokio::sync::Semaphore;
use tokio::time::{sleep, Duration, Instant};
use std::sync::Arc;

async fn expensive_operation(id: usize, semaphore: Arc<Semaphore>) {
    // Acquire a permit (blocks if none available)
    let _permit = semaphore.acquire().await.unwrap();

    println!("Task {} starting at {:?}", id, Instant::now());

    // Simulate work
    sleep(Duration::from_secs(1)).await;

    println!("Task {} finished at {:?}", id, Instant::now());

    // Permit is automatically released when dropped
}

#[tokio::main]
async fn main() {
    // Allow only 3 concurrent operations
    let semaphore = Arc::new(Semaphore::new(3));
    let mut handles = vec![];

    // Spawn 10 tasks
    for i in 0..10 {
        let sem_clone = Arc::clone(&semaphore);
        let handle = tokio::spawn(async move {
            expensive_operation(i, sem_clone).await;
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.await.unwrap();
    }
}
```

**Deep Explanation:**

- `Semaphore` limits the number of concurrent operations
- `acquire()` gets a permit, blocking asynchronously if the limit is reached
- This implements rate limiting: max 3 operations run simultaneously
- When a permit is dropped, waiting tasks can acquire it
- This is useful for limiting concurrent API calls, database queries, or file operations
- Prevents overwhelming external services or exhausting resources
- The permit guard ensures the semaphore is released even if the task panics

### Building a REST API with Shared State

```rust
use axum::{
    Router,
    routing::{get, post},
    extract::{State, Path, Json},
    http::StatusCode,
};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tokio::sync::RwLock;
use std::collections::HashMap;

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Item {
    id: u64,
    name: String,
    quantity: u32,
}

#[derive(Clone)]
struct AppState {
    items: Arc<RwLock<HashMap<u64, Item>>>,
}

// Handler: List all items
async fn list_items(State(state): State<AppState>) -> Json<Vec<Item>> {
    let items = state.items.read().await;
    let item_list: Vec<Item> = items.values().cloned().collect();
    Json(item_list)
}

// Handler: Get item by ID
async fn get_item(
    State(state): State<AppState>,
    Path(id): Path<u64>,
) -> Result<Json<Item>, StatusCode> {
    let items = state.items.read().await;

    items.get(&id)
        .cloned()
        .map(Json)
        .ok_or(StatusCode::NOT_FOUND)
}

// Handler: Create new item
async fn create_item(
    State(state): State<AppState>,
    Json(payload): Json<Item>,
) -> (StatusCode, Json<Item>) {
    let mut items = state.items.write().await;
    items.insert(payload.id, payload.clone());

    (StatusCode::CREATED, Json(payload))
}

// Handler: Delete item
async fn delete_item(
    State(state): State<AppState>,
    Path(id): Path<u64>,
) -> StatusCode {
    let mut items = state.items.write().await;

    if items.remove(&id).is_some() {
        StatusCode::NO_CONTENT
    } else {
        StatusCode::NOT_FOUND
    }
}

#[tokio::main]
async fn main() {
    // Initialize shared state
    let state = AppState {
        items: Arc::new(RwLock::new(HashMap::new())),
    };

    // Build router
    let app = Router::new()
        .route("/items", get(list_items).post(create_item))
        .route("/items/:id", get(get_item).delete(delete_item))
        .with_state(state);

    // Run server
    let listener = tokio::net::TcpListener::bind("127.0.0.1:3000")
        .await
        .unwrap();

    println!("Server running on http://127.0.0.1:3000");

    axum::serve(listener, app).await.unwrap();
}
```

**Deep Explanation:**

- Axum is a web framework built on Tokio for building async REST APIs
- `State` extractor provides access to shared application state
- Each handler is an async function that runs concurrently
- Multiple requests are handled simultaneously on the same thread pool
- RwLock allows many concurrent reads but exclusive writes
- `Path` extractor parses URL parameters
- `Json` extractor/responder handles JSON serialization/deserialization
- The router dispatches requests to appropriate handlers
- This pattern scales to thousands of concurrent requests

### Background Task Processing

```rust
use tokio::sync::mpsc;
use tokio::time::{sleep, Duration};

#[derive(Debug)]
enum Task {
    SendEmail { to: String, subject: String },
    ProcessImage { path: String },
    GenerateReport { user_id: u64 },
}

async fn task_worker(mut receiver: mpsc::Receiver<Task>) {
    while let Some(task) = receiver.recv().await {
        match task {
            Task::SendEmail { to, subject } => {
                println!("Sending email to {} with subject: {}", to, subject);
                sleep(Duration::from_secs(1)).await;
                println!("Email sent!");
            }
            Task::ProcessImage { path } => {
                println!("Processing image: {}", path);
                sleep(Duration::from_secs(2)).await;
                println!("Image processed!");
            }
            Task::GenerateReport { user_id } => {
                println!("Generating report for user {}", user_id);
                sleep(Duration::from_millis(500)).await;
                println!("Report generated!");
            }
        }
    }

    println!("Worker shutting down");
}

#[tokio::main]
async fn main() {
    let (sender, receiver) = mpsc::channel(100);

    // Spawn background worker
    tokio::spawn(task_worker(receiver));

    // Simulate enqueueing tasks
    let tasks = vec![
        Task::SendEmail {
            to: "user@example.com".to_string(),
            subject: "Welcome!".to_string(),
        },
        Task::ProcessImage {
            path: "/images/photo.jpg".to_string(),
        },
        Task::GenerateReport { user_id: 123 },
        Task::SendEmail {
            to: "admin@example.com".to_string(),
            subject: "Daily Report".to_string(),
        },
    ];

    for task in tasks {
        sender.send(task).await.unwrap();
        println!("Task enqueued");
    }

    // Give worker time to process
    sleep(Duration::from_secs(6)).await;

    drop(sender); // Close channel
    sleep(Duration::from_millis(100)).await; // Wait for worker to finish
}
```

**Deep Explanation:**

- This implements a task queue pattern for background job processing
- The channel decouples task production (API handlers) from consumption (worker)
- Worker processes tasks sequentially in the order received
- Multiple workers can share the same receiver (mpsc allows multiple consumers with recv)
- This prevents blocking API responses while long-running tasks execute
- Tasks are buffered in the channel if workers are busy
- When the sender is dropped, the channel closes and the worker exits gracefully

### Timeout and Cancellation

```rust
use tokio::time::{timeout, Duration};
use tokio::select;
use tokio_util::sync::CancellationToken;

async fn long_running_operation() -> Result<String, &'static str> {
    for i in 0..10 {
        println!("Working... step {}", i);
        tokio::time::sleep(Duration::from_secs(1)).await;
    }
    Ok("Operation completed".to_string())
}

async fn cancellable_operation(cancel_token: CancellationToken) -> Result<String, &'static str> {
    for i in 0..10 {
        // Check if cancelled
        if cancel_token.is_cancelled() {
            return Err("Operation was cancelled");
        }

        println!("Step {} of 10", i + 1);
        tokio::time::sleep(Duration::from_millis(500)).await;
    }
    Ok("Completed successfully".to_string())
}

#[tokio::main]
async fn main() {
    // Example 1: Simple timeout
    println!("=== Example 1: Timeout ===");
    match timeout(Duration::from_secs(3), long_running_operation()).await {
        Ok(Ok(result)) => println!("Success: {}", result),
        Ok(Err(e)) => println!("Operation failed: {}", e),
        Err(_) => println!("Operation timed out!"),
    }

    // Example 2: Cancellation token
    println!("\n=== Example 2: Cancellation ===");
    let cancel_token = CancellationToken::new();
    let token_clone = cancel_token.clone();

    let operation_handle = tokio::spawn(async move {
        cancellable_operation(token_clone).await
    });

    // Simulate cancelling after 2 seconds
    tokio::spawn(async move {
        tokio::time::sleep(Duration::from_secs(2)).await;
        println!("Sending cancellation signal...");
        cancel_token.cancel();
    });

    match operation_handle.await.unwrap() {
        Ok(result) => println!("Result: {}", result),
        Err(e) => println!("Error: {}", e),
    }

    // Example 3: Racing operations
    println!("\n=== Example 3: Select ===");
    select! {
        result = long_running_operation() => {
            println!("Long operation finished first: {:?}", result);
        }
        _ = tokio::time::sleep(Duration::from_secs(2)) => {
            println!("Timeout reached, giving up on operation");
        }
    }
}
```

**Deep Explanation:**

- `timeout()` wraps a future and returns `Err` if it doesn't complete in time
- Timeouts are critical for preventing hung requests in production systems
- `CancellationToken` provides cooperative cancellation: tasks check and respond to it
- Unlike thread cancellation, async cancellation is safe and explicit
- `select!` races multiple futures; the first to complete wins
- Cancelled futures are dropped, running their cleanup code (Drop implementations)
- This pattern is essential for graceful shutdown and resource management

---

## Chapter 9: Best Practices and Patterns

### Best Practice 1: Minimize Lock Contention

**Bad:**

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn bad_example() {
    let counter = Arc::new(Mutex::new(0));
    let mut handles = vec![];

    for _ in 0..100 {
        let counter = Arc::clone(&counter);
        let handle = thread::spawn(move || {
            for _ in 0..1000 {
                // Lock acquired and released 1000 times per thread
                let mut num = counter.lock().unwrap();
                *num += 1;
            }
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.join().unwrap();
    }
}
```

**Good:**

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn good_example() {
    let counter = Arc::new(Mutex::new(0));
    let mut handles = vec![];

    for _ in 0..100 {
        let counter = Arc::clone(&counter);
        let handle = thread::spawn(move || {
            // Calculate locally first
            let local_sum = (0..1000).sum::<i32>();

            // Lock only once to add the result
            let mut num = counter.lock().unwrap();
            *num += local_sum;
        });
        handles.push(handle);
    }

    for handle in handles {
        handle.join().unwrap();
    }
}
```

**Explanation:**

- Minimize the duration and frequency of lock acquisitions
- Batch operations to reduce lock contention
- Calculate results locally, then lock once to update shared state
- This reduces contention by 1000x in this example

### Best Practice 2: Prefer Message Passing Over Shared State

**Shared State Approach:**

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn shared_state_approach() {
    let data = Arc::new(Mutex::new(Vec::new()));
    let mut handles = vec![];

    for i in 0..5 {
        let data = Arc::clone(&data);
        handles.push(thread::spawn(move || {
            let mut d = data.lock().unwrap();
            d.push(i);
        }));
    }

    for handle in handles {
        handle.join().unwrap();
    }
}
```

**Message Passing Approach (Better):**

```rust
use std::sync::mpsc;
use std::thread;

fn message_passing_approach() {
    let (tx, rx) = mpsc::channel();

    for i in 0..5 {
        let tx = tx.clone();
        thread::spawn(move || {
            tx.send(i).unwrap();
        });
    }

    drop(tx); // Close the channel

    let data: Vec<i32> = rx.iter().collect();
    println!("{:?}", data);
}
```

**Explanation:**

- Message passing eliminates the need for explicit locks
- No risk of deadlocks or lock contention
- Clearer ownership semantics
- Easier to reason about data flow
- Use channels unless you absolutely need shared mutable state

### Best Practice 3: Use Appropriate Async Primitives

```rust
use tokio::sync::{Mutex, RwLock};
use std::sync::{Mutex as StdMutex, Arc};

// ❌ BAD: Using std::sync::Mutex in async code
async fn bad_async() {
    let data = Arc::new(StdMutex::new(0));

    let d = Arc::clone(&data);
    tokio::spawn(async move {
        let mut num = d.lock().unwrap(); // Blocks the thread!
        *num += 1;
        tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
    });
}

// ✅ GOOD: Using tokio::sync::Mutex
async fn good_async() {
    let data = Arc::new(Mutex::new(0));

    let d = Arc::clone(&data);
    tokio::spawn(async move {
        let mut num = d.lock().await; // Yields to runtime
        *num += 1;
        tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
    });
}
```

**Explanation:**

- Never use `std::sync::Mutex` across `.await` points
- `std::sync::Mutex::lock()` blocks the thread, preventing other tasks from running
- Use `tokio::sync::Mutex` which yields cooperatively
- Exception: `std::sync::Mutex` is fine if you don't hold it across `.await`

### Best Practice 4: Structured Concurrency with JoinSets

```rust
use tokio::task::JoinSet;
use std::time::Duration;

async fn process_item(id: usize) -> Result<String, String> {
    tokio::time::sleep(Duration::from_millis(100)).await;

    if id % 5 == 0 {
        Err(format!("Failed to process item {}", id))
    } else {
        Ok(format!("Processed item {}", id))
    }
}

#[tokio::main]
async fn main() {
    let mut join_set = JoinSet::new();

    // Spawn multiple tasks
    for i in 0..10 {
        join_set.spawn(async move {
            process_item(i).await
        });
    }

    // Process results as they complete
    while let Some(result) = join_set.join_next().await {
        match result {
            Ok(Ok(msg)) => println!("✓ {}", msg),
            Ok(Err(err)) => eprintln!("✗ {}", err),
            Err(join_err) => eprintln!("Task panicked: {}", join_err),
        }
    }

    println!("All tasks completed");
}
```

**Explanation:**

- `JoinSet` manages a collection of tasks
- Tasks can be dynamically added and removed
- Results are processed as tasks complete (not in spawn order)
- Automatically handles task cancellation when dropped
- Better than manually managing `Vec<JoinHandle>` for dynamic workloads

### Best Practice 5: Graceful Shutdown

```rust
use tokio::signal;
use tokio::sync::broadcast;
use tokio::time::{sleep, Duration};

async fn worker(id: usize, mut shutdown: broadcast::Receiver<()>) {
    loop {
        tokio::select! {
            _ = shutdown.recv() => {
                println!("Worker {} received shutdown signal", id);
                // Perform cleanup
                sleep(Duration::from_millis(100)).await;
                println!("Worker {} cleaned up", id);
                break;
            }
            _ = sleep(Duration::from_secs(1)) => {
                println!("Worker {} working...", id);
            }
        }
    }
}

#[tokio::main]
async fn main() {
    let (shutdown_tx, _) = broadcast::channel(1);
    let mut handles = vec![];

    // Spawn workers
    for i in 0..5 {
        let shutdown_rx = shutdown_tx.subscribe();
        let handle = tokio::spawn(worker(i, shutdown_rx));
        handles.push(handle);
    }

    // Wait for Ctrl+C
    signal::ctrl_c().await.expect("Failed to listen for Ctrl+C");
    println!("\nShutdown signal received, gracefully shutting down...");

    // Broadcast shutdown
    let _ = shutdown_tx.send(());

    // Wait for all workers to finish
    for handle in handles {
        handle.await.unwrap();
    }

    println!("All workers shut down successfully");
}
```

**Explanation:**

- Graceful shutdown prevents data loss and corruption
- `broadcast::channel` sends shutdown signals to all workers
- Workers complete their current work before exiting
- `select!` allows workers to respond to shutdown while working
- Critical for production systems: allows clean connection closure, data flushing, etc.

### Best Practice 6: Error Handling in Concurrent Code

```rust
use tokio::task::JoinSet;
use std::time::Duration;

#[derive(Debug)]
enum ProcessError {
    Timeout,
    InvalidData(String),
    NetworkError,
}

async fn process_with_retry(
    id: usize,
    max_retries: usize,
) -> Result<String, ProcessError> {
    for attempt in 0..max_retries {
        match tokio::time::timeout(
            Duration::from_secs(2),
            simulate_processing(id, attempt)
        ).await {
            Ok(Ok(result)) => return Ok(result),
            Ok(Err(e)) => {
                eprintln!("Attempt {} failed for {}: {:?}", attempt + 1, id, e);
                if attempt == max_retries - 1 {
                    return Err(e);
                }
                tokio::time::sleep(Duration::from_millis(100 * (attempt as u64 + 1))).await;
            }
            Err(_) => {
                eprintln!("Timeout on attempt {} for {}", attempt + 1, id);
                if attempt == max_retries - 1 {
                    return Err(ProcessError::Timeout);
                }
            }
        }
    }

    Err(ProcessError::Timeout)
}

async fn simulate_processing(id: usize, attempt: usize) -> Result<String, ProcessError> {
    tokio::time::sleep(Duration::from_millis(100)).await;

    // Simulate different failure modes
    if id % 3 == 0 && attempt == 0 {
        Err(ProcessError::NetworkError)
    } else if id % 5 == 0 && attempt < 2 {
        Err(ProcessError::InvalidData(format!("Bad data for {}", id)))
    } else {
        Ok(format!("Success for {}", id))
    }
}

#[tokio::main]
async fn main() {
    let mut join_set = JoinSet::new();

    for i in 0..10 {
        join_set.spawn(async move {
            process_with_retry(i, 3).await
        });
    }

    let mut successes = 0;
    let mut failures = 0;

    while let Some(result) = join_set.join_next().await {
        match result {
            Ok(Ok(msg)) => {
                println!("✓ {}", msg);
                successes += 1;
            }
            Ok(Err(e)) => {
                eprintln!("✗ Failed: {:?}", e);
                failures += 1;
            }
            Err(e) => {
                eprintln!("✗ Task panicked: {}", e);
                failures += 1;
            }
        }
    }

    println!("\nResults: {} successes, {} failures", successes, failures);
}
```

**Explanation:**

- Implement retry logic with exponential backoff
- Use timeouts to prevent indefinite waiting
- Aggregate errors for reporting
- Handle both task errors and panics
- Critical for resilient distributed systems

### Best Practice 7: Avoiding Common Pitfalls

#### Pitfall: Holding Locks Across Await Points

```rust
use tokio::sync::Mutex;
use std::sync::Arc;

// ❌ BAD: Lock held across await
async fn bad_lock_usage(data: Arc<Mutex<Vec<String>>>) {
    let mut d = data.lock().await;
    d.push("item".to_string());

    // BAD: lock is held while awaiting
    tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;

    d.push("another".to_string());
}

// ✅ GOOD: Minimize lock duration
async fn good_lock_usage(data: Arc<Mutex<Vec<String>>>) {
    // Do async work first
    tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;

    // Then acquire lock briefly
    {
        let mut d = data.lock().await;
        d.push("item".to_string());
        d.push("another".to_string());
    } // Lock released here
}
```

#### Pitfall: Spawning Without Error Handling

```rust
// ❌ BAD: Ignoring task results
async fn bad_spawn() {
    tokio::spawn(async {
        panic!("This panic is silently ignored!");
    });
}

// ✅ GOOD: Handle task results
async fn good_spawn() {
    let handle = tokio::spawn(async {
        // Might panic or return error
        risky_operation().await
    });

    match handle.await {
        Ok(Ok(result)) => println!("Success: {:?}", result),
        Ok(Err(e)) => eprintln!("Task error: {:?}", e),
        Err(e) => eprintln!("Task panicked: {:?}", e),
    }
}

async fn risky_operation() -> Result<String, &'static str> {
    Ok("result".to_string())
}
```

#### Pitfall: Blocking the Runtime

```rust
// ❌ BAD: Blocking operation in async context
async fn bad_blocking() {
    // This blocks the entire runtime thread!
    std::thread::sleep(std::time::Duration::from_secs(5));
}

// ✅ GOOD: Use spawn_blocking
async fn good_blocking() {
    tokio::task::spawn_blocking(|| {
        std::thread::sleep(std::time::Duration::from_secs(5));
    }).await.unwrap();
}
```

---

## Chapter 10: Real-World Backend Architecture

### Complete Example: Microservice with Rust Concurrency

```rust
use axum::{
    Router,
    routing::{get, post},
    extract::{State, Path, Json},
    http::StatusCode,
    response::IntoResponse,
};
use serde::{Deserialize, Serialize};
use sqlx::postgres::{PgPool, PgPoolOptions};
use tokio::sync::{RwLock, mpsc};
use std::sync::Arc;
use std::collections::HashMap;
use std::time::Duration;

// ============= Domain Models =============

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Order {
    id: i64,
    user_id: i64,
    product_id: i64,
quantity: i32,
    status: OrderStatus,
}

#[derive(Debug, Clone, Serialize, Deserialize, sqlx::Type)]
#[sqlx(type_name = "order_status", rename_all = "lowercase")]
enum OrderStatus {
    Pending,
    Processing,
    Completed,
    Failed,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct CreateOrderRequest {
    user_id: i64,
    product_id: i64,
    quantity: i32,
}

// ============= Background Jobs =============

#[derive(Debug)]
enum BackgroundJob {
    ProcessOrder { order_id: i64 },
    SendNotification { user_id: i64, message: String },
    UpdateInventory { product_id: i64, quantity: i32 },
}

async fn background_worker(
    mut receiver: mpsc::Receiver<BackgroundJob>,
    db_pool: PgPool,
) {
    println!("Background worker started");

    while let Some(job) = receiver.recv().await {
        match job {
            BackgroundJob::ProcessOrder { order_id } => {
                println!("Processing order {}", order_id);

                // Simulate order processing
                tokio::time::sleep(Duration::from_secs(2)).await;

                // Update order status in database
                let result = sqlx::query(
                    "UPDATE orders SET status = $1 WHERE id = $2"
                )
                .bind(OrderStatus::Completed)
                .bind(order_id)
                .execute(&db_pool)
                .await;

                match result {
                    Ok(_) => println!("Order {} marked as completed", order_id),
                    Err(e) => eprintln!("Failed to update order {}: {}", order_id, e),
                }
            }

            BackgroundJob::SendNotification { user_id, message } => {
                println!("Sending notification to user {}: {}", user_id, message);
                tokio::time::sleep(Duration::from_millis(500)).await;
                println!("Notification sent");
            }

            BackgroundJob::UpdateInventory { product_id, quantity } => {
                println!("Updating inventory for product {}", product_id);
                tokio::time::sleep(Duration::from_millis(300)).await;
                println!("Inventory updated");
            }
        }
    }

    println!("Background worker shutting down");
}

// ============= Cache Layer =============

type Cache = Arc<RwLock<HashMap<i64, Order>>>;

async fn get_from_cache(cache: &Cache, order_id: i64) -> Option<Order> {
    let cache_read = cache.read().await;
    cache_read.get(&order_id).cloned()
}

async fn set_in_cache(cache: &Cache, order: Order) {
    let mut cache_write = cache.write().await;
    cache_write.insert(order.id, order);
}

async fn invalidate_cache(cache: &Cache, order_id: i64) {
    let mut cache_write = cache.write().await;
    cache_write.remove(&order_id);
}

// ============= Application State =============

#[derive(Clone)]
struct AppState {
    db_pool: PgPool,
    cache: Cache,
    job_sender: mpsc::Sender<BackgroundJob>,
}

// ============= API Handlers =============

async fn health_check() -> impl IntoResponse {
    (StatusCode::OK, Json(serde_json::json!({
        "status": "healthy",
        "timestamp": chrono::Utc::now().to_rfc3339()
    })))
}

async fn create_order(
    State(state): State<AppState>,
    Json(payload): Json<CreateOrderRequest>,
) -> Result<(StatusCode, Json<Order>), StatusCode> {
    println!("Creating order for user {}", payload.user_id);

    // Insert order into database
    let order = sqlx::query_as::<_, (i64, i64, i64, i32, OrderStatus)>(
        "INSERT INTO orders (user_id, product_id, quantity, status)
         VALUES ($1, $2, $3, $4)
         RETURNING id, user_id, product_id, quantity, status"
    )
    .bind(payload.user_id)
    .bind(payload.product_id)
    .bind(payload.quantity)
    .bind(OrderStatus::Pending)
    .fetch_one(&state.db_pool)
    .await
    .map_err(|e| {
        eprintln!("Database error: {}", e);
        StatusCode::INTERNAL_SERVER_ERROR
    })?;

    let order = Order {
        id: order.0,
        user_id: order.1,
        product_id: order.2,
        quantity: order.3,
        status: order.4,
    };

    // Cache the order
    set_in_cache(&state.cache, order.clone()).await;

    // Send background jobs (non-blocking)
    let _ = state.job_sender.send(BackgroundJob::ProcessOrder {
        order_id: order.id
    }).await;

    let _ = state.job_sender.send(BackgroundJob::SendNotification {
        user_id: order.user_id,
        message: format!("Your order #{} has been placed", order.id),
    }).await;

    let _ = state.job_sender.send(BackgroundJob::UpdateInventory {
        product_id: order.product_id,
        quantity: -order.quantity,
    }).await;

    Ok((StatusCode::CREATED, Json(order)))
}

async fn get_order(
    State(state): State<AppState>,
    Path(order_id): Path<i64>,
) -> Result<Json<Order>, StatusCode> {
    // Try cache first
    if let Some(order) = get_from_cache(&state.cache, order_id).await {
        println!("Cache hit for order {}", order_id);
        return Ok(Json(order));
    }

    println!("Cache miss for order {}, fetching from DB", order_id);

    // Fetch from database
    let order = sqlx::query_as::<_, (i64, i64, i64, i32, OrderStatus)>(
        "SELECT id, user_id, product_id, quantity, status FROM orders WHERE id = $1"
    )
    .bind(order_id)
    .fetch_optional(&state.db_pool)
    .await
    .map_err(|e| {
        eprintln!("Database error: {}", e);
        StatusCode::INTERNAL_SERVER_ERROR
    })?
    .ok_or(StatusCode::NOT_FOUND)?;

    let order = Order {
        id: order.0,
        user_id: order.1,
        product_id: order.2,
        quantity: order.3,
        status: order.4,
    };

    // Update cache
    set_in_cache(&state.cache, order.clone()).await;

    Ok(Json(order))
}

async fn list_orders(
    State(state): State<AppState>,
) -> Result<Json<Vec<Order>>, StatusCode> {
    let orders = sqlx::query_as::<_, (i64, i64, i64, i32, OrderStatus)>(
        "SELECT id, user_id, product_id, quantity, status FROM orders ORDER BY id DESC LIMIT 100"
    )
    .fetch_all(&state.db_pool)
    .await
    .map_err(|e| {
        eprintln!("Database error: {}", e);
        StatusCode::INTERNAL_SERVER_ERROR
    })?;

    let orders: Vec<Order> = orders.into_iter().map(|row| Order {
        id: row.0,
        user_id: row.1,
        product_id: row.2,
        quantity: row.3,
        status: row.4,
    }).collect();

    Ok(Json(orders))
}

async fn update_order_status(
    State(state): State<AppState>,
    Path(order_id): Path<i64>,
    Json(new_status): Json<OrderStatus>,
) -> Result<Json<Order>, StatusCode> {
    let order = sqlx::query_as::<_, (i64, i64, i64, i32, OrderStatus)>(
        "UPDATE orders SET status = $1 WHERE id = $2
         RETURNING id, user_id, product_id, quantity, status"
    )
    .bind(&new_status)
    .bind(order_id)
    .fetch_optional(&state.db_pool)
    .await
    .map_err(|e| {
        eprintln!("Database error: {}", e);
        StatusCode::INTERNAL_SERVER_ERROR
    })?
    .ok_or(StatusCode::NOT_FOUND)?;

    let order = Order {
        id: order.0,
        user_id: order.1,
        product_id: order.2,
        quantity: order.3,
        status: order.4,
    };

    // Invalidate cache
    invalidate_cache(&state.cache, order_id).await;

    Ok(Json(order))
}

// ============= Metrics and Monitoring =============

async fn metrics(State(state): State<AppState>) -> impl IntoResponse {
    let pool_connections = state.db_pool.size();
    let cache_size = state.cache.read().await.len();

    (StatusCode::OK, Json(serde_json::json!({
        "database_pool_size": pool_connections,
        "cache_entries": cache_size,
        "timestamp": chrono::Utc::now().to_rfc3339()
    })))
}

// ============= Main Application =============

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize tracing
    tracing_subscriber::fmt::init();

    println!("Starting application...");

    // Database connection pool
    let database_url = std::env::var("DATABASE_URL")
        .unwrap_or_else(|_| "postgres://user:pass@localhost/orders".to_string());

    let db_pool = PgPoolOptions::new()
        .max_connections(20)
        .acquire_timeout(Duration::from_secs(3))
        .connect(&database_url)
        .await?;

    println!("Database connection pool established");

    // Run migrations
    sqlx::migrate!("./migrations")
        .run(&db_pool)
        .await?;

    println!("Database migrations applied");

    // Initialize cache
    let cache: Cache = Arc::new(RwLock::new(HashMap::new()));

    // Create background job channel
    let (job_sender, job_receiver) = mpsc::channel::<BackgroundJob>(1000);

    // Spawn background workers (multiple for parallelism)
    for i in 0..4 {
        let receiver = if i == 0 { job_receiver } else {
            // For multiple workers, we'd need a different pattern
            // This is simplified for demonstration
            continue;
        };
        let pool = db_pool.clone();

        tokio::spawn(async move {
            background_worker(receiver, pool).await;
        });

        println!("Background worker {} started", i);
    }

    // Application state
    let state = AppState {
        db_pool: db_pool.clone(),
        cache: cache.clone(),
        job_sender: job_sender.clone(),
    };

    // Build router
    let app = Router::new()
        .route("/health", get(health_check))
        .route("/metrics", get(metrics))
        .route("/orders", get(list_orders).post(create_order))
        .route("/orders/:id", get(get_order))
        .route("/orders/:id/status", post(update_order_status))
        .with_state(state);

    // Start server
    let listener = tokio::net::TcpListener::bind("0.0.0.0:3000")
        .await?;

    println!("Server listening on http://0.0.0.0:3000");

    axum::serve(listener, app).await?;

    Ok(())
}
```

**Deep Explanation of Architecture:**

1. **Layer Separation:**

   - Domain models define business entities
   - Handlers process HTTP requests
   - Background workers handle async jobs
   - Cache layer optimizes database access

2. **Concurrency Patterns:**

   - Database connection pooling handles concurrent queries
   - RwLock cache allows many concurrent reads
   - Message passing (mpsc) decouples request handling from job processing
   - Each request handler runs as an independent async task

3. **Scalability:**

   - Connection pool prevents database overload
   - Cache reduces database load for read-heavy workloads
   - Background jobs don't block API responses
   - Multiple workers can process jobs in parallel

4. **Reliability:**
   - Graceful error handling at each layer
   - Database transactions ensure consistency
   - Job queue buffers work during traffic spikes
   - Health checks and metrics for monitoring

### Performance Patterns

```rust
use tokio::time::{sleep, Duration, Instant};
use std::sync::Arc;
use tokio::sync::Semaphore;

// Pattern: Rate Limiting
async fn rate_limited_api_call(
    semaphore: Arc<Semaphore>,
    id: usize,
) -> Result<String, &'static str> {
    // Acquire permit (limits concurrent calls)
    let _permit = semaphore.acquire().await.unwrap();

    println!("Making API call {}", id);
    sleep(Duration::from_millis(100)).await;

    Ok(format!("Response {}", id))
}

// Pattern: Circuit Breaker
struct CircuitBreaker {
    failure_threshold: usize,
    failures: tokio::sync::Mutex<usize>,
    last_failure: tokio::sync::Mutex<Option<Instant>>,
    timeout: Duration,
}

impl CircuitBreaker {
    fn new(failure_threshold: usize, timeout: Duration) -> Self {
        Self {
            failure_threshold,
            failures: tokio::sync::Mutex::new(0),
            last_failure: tokio::sync::Mutex::new(None),
            timeout,
        }
    }

    async fn is_open(&self) -> bool {
        let failures = *self.failures.lock().await;
        let last_failure = *self.last_failure.lock().await;

        if failures >= self.failure_threshold {
            if let Some(last) = last_failure {
                if last.elapsed() < self.timeout {
                    return true; // Circuit is open
                }
            }
        }

        false
    }

    async fn record_success(&self) {
        let mut failures = self.failures.lock().await;
        *failures = 0;
    }

    async fn record_failure(&self) {
        let mut failures = self.failures.lock().await;
        *failures += 1;

        let mut last_failure = self.last_failure.lock().await;
        *last_failure = Some(Instant::now());
    }

    async fn call<F, Fut, T, E>(&self, f: F) -> Result<T, &'static str>
    where
        F: FnOnce() -> Fut,
        Fut: std::future::Future<Output = Result<T, E>>,
    {
        if self.is_open().await {
            return Err("Circuit breaker is open");
        }

        match f().await {
            Ok(result) => {
                self.record_success().await;
                Ok(result)
            }
            Err(_) => {
                self.record_failure().await;
                Err("Request failed")
            }
        }
    }
}

// Pattern: Bulkhead (Resource Isolation)
struct Bulkhead {
    high_priority: Arc<Semaphore>,
    low_priority: Arc<Semaphore>,
}

impl Bulkhead {
    fn new(high_priority_limit: usize, low_priority_limit: usize) -> Self {
        Self {
            high_priority: Arc::new(Semaphore::new(high_priority_limit)),
            low_priority: Arc::new(Semaphore::new(low_priority_limit)),
        }
    }

    async fn execute_high_priority<F, Fut, T>(&self, f: F) -> T
    where
        F: FnOnce() -> Fut,
        Fut: std::future::Future<Output = T>,
    {
        let _permit = self.high_priority.acquire().await.unwrap();
        f().await
    }

    async fn execute_low_priority<F, Fut, T>(&self, f: F) -> T
    where
        F: FnOnce() -> Fut,
        Fut: std::future::Future<Output = T>,
    {
        let _permit = self.low_priority.acquire().await.unwrap();
        f().await
    }
}

#[tokio::main]
async fn main() {
    // Demonstrate rate limiting
    println!("=== Rate Limiting ===");
    let semaphore = Arc::new(Semaphore::new(3));
    let mut handles = vec![];

    for i in 0..10 {
        let sem = Arc::clone(&semaphore);
        handles.push(tokio::spawn(async move {
            rate_limited_api_call(sem, i).await
        }));
    }

    for handle in handles {
        let _ = handle.await;
    }

    // Demonstrate circuit breaker
    println!("\n=== Circuit Breaker ===");
    let breaker = Arc::new(CircuitBreaker::new(3, Duration::from_secs(5)));

    for i in 0..10 {
        let cb = Arc::clone(&breaker);
        let result = cb.call(|| async {
            if i < 5 {
                Err::<(), _>("Simulated failure")
            } else {
                Ok(())
            }
        }).await;

        println!("Request {}: {:?}", i, result);
        sleep(Duration::from_millis(500)).await;
    }

    // Demonstrate bulkhead
    println!("\n=== Bulkhead ===");
    let bulkhead = Arc::new(Bulkhead::new(2, 1));

    // High priority requests
    for i in 0..3 {
        let bh = Arc::clone(&bulkhead);
        tokio::spawn(async move {
            bh.execute_high_priority(|| async move {
                println!("High priority {} executing", i);
                sleep(Duration::from_secs(1)).await;
            }).await;
        });
    }

    // Low priority requests (limited)
    for i in 0..3 {
        let bh = Arc::clone(&bulkhead);
        tokio::spawn(async move {
            bh.execute_low_priority(|| async move {
                println!("Low priority {} executing", i);
                sleep(Duration::from_secs(1)).await;
            }).await;
        });
    }

    sleep(Duration::from_secs(5)).await;
}
```

**Explanation:**

- **Rate Limiting:** Controls the number of concurrent operations to prevent overwhelming services
- **Circuit Breaker:** Fails fast when a service is unavailable, preventing cascading failures
- **Bulkhead:** Isolates resources for critical vs. non-critical operations

---

## Chapter 11: Testing Concurrent Code

### Testing Async Functions

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio::time::{sleep, Duration};

    #[tokio::test]
    async fn test_async_operation() {
        let result = async_computation().await;
        assert_eq!(result, 42);
    }

    async fn async_computation() -> i32 {
        sleep(Duration::from_millis(10)).await;
        42
    }

    #[tokio::test]
    async fn test_concurrent_operations() {
        let (result1, result2, result3) = tokio::join!(
            async { 1 + 1 },
            async { 2 + 2 },
            async { 3 + 3 }
        );

        assert_eq!(result1, 2);
        assert_eq!(result2, 4);
        assert_eq!(result3, 6);
    }

    #[tokio::test]
    async fn test_timeout() {
        let result = tokio::time::timeout(
            Duration::from_millis(100),
            slow_operation()
        ).await;

        assert!(result.is_err()); // Should timeout
    }

    async fn slow_operation() {
        sleep(Duration::from_secs(10)).await;
    }
}
```

### Testing Thread Safety

```rust
#[cfg(test)]
mod thread_tests {
    use std::sync::{Arc, Mutex};
    use std::thread;

    #[test]
    fn test_concurrent_counter() {
        let counter = Arc::new(Mutex::new(0));
        let mut handles = vec![];

        for _ in 0..10 {
            let counter = Arc::clone(&counter);
            let handle = thread::spawn(move || {
                for _ in 0..1000 {
                    let mut num = counter.lock().unwrap();
                    *num += 1;
                }
            });
            handles.push(handle);
        }

        for handle in handles {
            handle.join().unwrap();
        }

        assert_eq!(*counter.lock().unwrap(), 10000);
    }
}
```

### Property-Based Testing

```rust
#[cfg(test)]
mod property_tests {
    use proptest::prelude::*;
    use tokio::runtime::Runtime;

    proptest! {
        #[test]
        fn test_concurrent_map_operations(operations in prop::collection::vec(0..100i32, 1..100)) {
            let rt = Runtime::new().unwrap();

            rt.block_on(async {
                let map = Arc::new(tokio::sync::RwLock::new(std::collections::HashMap::new()));
                let mut handles = vec![];

                for op in operations.clone() {
                    let map_clone = Arc::clone(&map);
                    handles.push(tokio::spawn(async move {
                        let mut m = map_clone.write().await;
                        m.insert(op, op * 2);
                    }));
                }

                for handle in handles {
                    handle.await.unwrap();
                }

                let m = map.read().await;
                for op in operations {
                    assert_eq!(m.get(&op), Some(&(op * 2)));
                }
            });
        }
    }
}
```

---

## Summary: Key Takeaways

### When to Use What

**Threads:**

- CPU-bound computations
- True parallelism needed
- Blocking operations (with spawn_blocking)
- Simple isolation between tasks

**Async/Await:**

- I/O-bound operations (HTTP, databases, files)
- High concurrency (thousands of connections)
- Efficient resource usage
- Modern web services

**Mutex:**

- Short critical sections
- Write-heavy workloads
- Simple mutual exclusion

**RwLock:**

- Read-heavy workloads
- Long read operations
- Infrequent writes

**Channels:**

- Message passing between tasks
- Background job queues
- Decoupling producers and consumers
- When you want to avoid shared state

### Production Checklist

1. ✅ Use appropriate sync primitives (tokio:: for async)
2. ✅ Implement graceful shutdown
3. ✅ Add timeouts to all external calls
4. ✅ Monitor and limit resource usage (connection pools, semaphores)
5. ✅ Handle errors at every async boundary
6. ✅ Use structured concurrency (JoinSet)
7. ✅ Test concurrent code thoroughly
8. ✅ Add metrics and observability
9. ✅ Implement retry logic with backoff
10. ✅ Use circuit breakers for external services

Rust's concurrency model gives you performance and safety. The compiler prevents data races, making concurrent programming less error-prone. By understanding these patterns and following best practices, you can build highly concurrent, reliable backend systems that scale efficiently.
