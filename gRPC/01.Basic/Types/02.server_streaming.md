# **Chapter: Server Streaming RPC – One Request, Many Responses**

Think about visiting a library and asking:
👉 “Can you give me all the books by Rabindranath Tagore?”

If the librarian were following the **Unary RPC style**, they’d:

* Fetch **all the books at once**, bundle them into a giant box, and give you that single response.

But in real life, the librarian doesn’t work like that. They bring books one by one, **streaming results** until your request is fully satisfied.

That’s exactly what a **Server Streaming RPC** is in gRPC:

* **One request goes in.**
* **Multiple responses come out, one after another.**

---

## **1. What is Server Streaming RPC?**

In **Server Streaming**, the client sends a single request, but instead of waiting for one answer, it opens a channel where the server can send **a stream of responses**.

* Client: “Give me all results for X.”
* Server: “Here’s item 1… here’s item 2… here’s item 3… done.”

So:

* **Unary RPC:** request → single response.
* **Server Streaming RPC:** request → sequence of responses.

---

## **2. Why Not Just Use Unary RPC?**

You might ask: *“Can’t I just return a list (array) in a Unary RPC?”*
Yes, but that breaks down in some situations:

1. **Large datasets**

   * Imagine fetching **1 million log entries**.
   * Unary RPC → server must gather everything, hold it in memory, send it in one massive response.
   * Server Streaming → server sends results in **chunks** (like pagination, but live).

2. **Progressive delivery**

   * Unary RPC → client waits until everything is ready.
   * Server Streaming → client can start **processing partial data** immediately.

3. **Resource efficiency**

   * Unary RPC → big memory footprint, risk of timeout.
   * Server Streaming → smaller packets, smoother transmission.

4. **Real-time or near real-time flows**

   * Unary RPC → too rigid.
   * Server Streaming → perfect for pushing updates/events without requiring client polling.

👉 In short: **Server Streaming is needed when results are too big, too progressive, or too continuous for one-shot delivery.**

---

## **3. Defining Server Streaming in Protobuf**

Let’s define a service where the client requests a product category, and the server streams back all products one by one.

```proto
syntax = "proto3";

package products;

message ProductRequest {
  string category = 1;
}

message ProductResponse {
  int32 id = 1;
  string name = 2;
  double price = 3;
}

service ProductService {
  // Server streaming: one request, multiple responses
  rpc GetProducts (ProductRequest) returns (stream ProductResponse);
}
```

Notice the **`stream`** keyword before `ProductResponse`.
That’s the magic word telling gRPC: *“This method will stream responses.”*

---

## **4. Server-Side Implementation**

Example in Node.js:

```js
function GetProducts(call) {
  const category = call.request.category;
  
  // Mock database
  const products = [
    { id: 1, name: "Laptop", price: 1200 },
    { id: 2, name: "Keyboard", price: 50 },
    { id: 3, name: "Mouse", price: 25 }
  ];

  // Stream products one by one
  products.forEach(product => {
    call.write(product);
  });

  // End the stream
  call.end();
}
```

* The server **writes multiple responses** using `call.write()`.
* When done, it signals completion with `call.end()`.

---

## **5. Client-Side Consumption**

Client subscribes to the stream:

```js
const request = { category: "electronics" };

const call = client.GetProducts(request);

call.on("data", (product) => {
  console.log("Received product:", product);
});

call.on("end", () => {
  console.log("All products received.");
});
```

* `on("data")`: triggered for every streamed response.
* `on("end")`: triggered when the server finishes sending.

---

## **6. Lifecycle of a Server Streaming RPC**

1. **Client sends one request.**
2. **Server starts processing.**
3. **Server streams multiple responses** back → `call.write()`.
4. **Client receives each response immediately** as data arrives.
5. **Server ends the stream.**
6. **Client knows the stream is complete.**

---

## **7. Example Use Cases**

* **Fetching large datasets progressively** → logs, transactions, product lists.
* **Live feeds** → stock prices, sports scores, live auction bids.
* **File transfers** → stream file chunks instead of sending all at once.
* **Database query results** → deliver rows as they’re read.

---

## **8. Pros and Cons**

**Pros:**

* Efficient for large/multiple results.
* Client can process data as it arrives.
* Reduces latency for the first result.
* More memory-friendly on server side.

**Cons:**

* Slightly more complex than Unary RPC.
* Client must handle stream events (`data`, `end`).
* Not ideal if you truly only need one response.

---

## **Key Takeaways**

* **Server Streaming = single request, multiple responses.**
* It’s essential when data is too large, needs to be delivered progressively, or is continuously produced.
* Defined in `.proto` with `returns (stream Response)`.
* Server sends data with `call.write()`, finishes with `call.end()`.
* Client listens for `data` and `end` events.

---

👉 Unary RPC is like asking for **a single cup of tea**.
👉 Server Streaming RPC is like asking for **endless refills until the pot is empty**.

---